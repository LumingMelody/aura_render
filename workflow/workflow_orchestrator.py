"""
å·¥ä½œæµç¼–æ’å™¨ - ç»Ÿä¸€ç®¡ç†è§†é¢‘ç”Ÿæˆçš„ç«¯åˆ°ç«¯æµç¨‹
"""
from typing import Dict, List, Any, Optional, Set, Callable
import asyncio
import json
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum
import networkx as nx
from collections import defaultdict, deque

from nodes.base_node import BaseNode, NodeResult, ProcessingContext, NodeStatus
from task_queue.task_queue_manager import TaskQueueManager, QueueConfig


class WorkflowStatus(Enum):
    """å·¥ä½œæµçŠ¶æ€"""
    IDLE = "idle"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    PAUSED = "paused"


@dataclass
class NodeDependency:
    """èŠ‚ç‚¹ä¾èµ–å…³ç³»"""
    node_id: str
    dependencies: List[str] = field(default_factory=list)
    conditions: Dict[str, Any] = field(default_factory=dict)
    timeout: Optional[float] = None
    retry_on_failure: bool = True


@dataclass
class WorkflowConfig:
    """å·¥ä½œæµé…ç½®"""
    name: str
    description: str
    max_parallel_nodes: int = 5
    total_timeout: float = 3600.0  # 1å°æ—¶
    auto_retry: bool = True
    save_intermediate_results: bool = True
    enable_monitoring: bool = True
    error_handling_strategy: str = "fail_fast"  # fail_fast, continue, retry


@dataclass
class WorkflowResult:
    """å·¥ä½œæµæ‰§è¡Œç»“æœ"""
    workflow_id: str
    status: WorkflowStatus
    execution_time: float = 0.0
    nodes_executed: int = 0
    nodes_failed: int = 0
    final_output: Dict[str, Any] = field(default_factory=dict)
    intermediate_results: Dict[str, Any] = field(default_factory=dict)
    error_log: List[str] = field(default_factory=list)
    performance_metrics: Dict[str, Any] = field(default_factory=dict)


class WorkflowOrchestrator:
    """å·¥ä½œæµç¼–æ’å™¨"""

    def __init__(self, config: WorkflowConfig, queue_manager: Optional[TaskQueueManager] = None):
        self.config = config
        self.queue_manager = queue_manager

        # å·¥ä½œæµçŠ¶æ€
        self.status = WorkflowStatus.IDLE
        self.workflow_id: Optional[str] = None
        self.start_time: Optional[datetime] = None

        # èŠ‚ç‚¹ç®¡ç†
        self.nodes: Dict[str, BaseNode] = {}
        self.dependency_graph = nx.DiGraph()
        self.node_dependencies: Dict[str, NodeDependency] = {}

        # æ‰§è¡ŒçŠ¶æ€è·Ÿè¸ª
        self.executed_nodes: Set[str] = set()
        self.failed_nodes: Set[str] = set()
        self.running_nodes: Set[str] = set()
        self.node_results: Dict[str, NodeResult] = {}

        # äº‹ä»¶å¤„ç†
        self.event_handlers: Dict[str, List[Callable]] = defaultdict(list)

        # ç›‘æ§æ•°æ®
        self.metrics = {
            'nodes_executed': 0,
            'nodes_failed': 0,
            'total_execution_time': 0.0,
            'average_node_time': 0.0,
            'bottleneck_nodes': [],
            'resource_usage': {}
        }

    def add_node(self, node: BaseNode, dependencies: List[str] = None,
                 conditions: Dict[str, Any] = None, **kwargs) -> bool:
        """æ·»åŠ èŠ‚ç‚¹åˆ°å·¥ä½œæµ"""
        try:
            # ç¡®ä¿ä½¿ç”¨å­—ç¬¦ä¸²node_idä½œä¸ºé”®
            if hasattr(node, 'config') and hasattr(node.config, 'node_id'):
                # ä¼˜å…ˆä» config ä¸­è·å–
                node_id = node.config.node_id
            elif hasattr(node, 'node_id'):
                # å¤‡é€‰ï¼šç›´æ¥ä» node è·å–
                node_id = node.node_id
            else:
                raise ValueError(f"Node must have node_id attribute: {node}")

            # ç¡®ä¿ node_id æ˜¯å­—ç¬¦ä¸²ï¼Œä¸æ˜¯å¯¹è±¡
            if not isinstance(node_id, str):
                if hasattr(node_id, 'node_id'):
                    # å¦‚æœ node_id æ˜¯ NodeConfig å¯¹è±¡ï¼Œæå–å…¶ node_id å­—ç¬¦ä¸²
                    node_id = node_id.node_id
                else:
                    node_id = str(node_id)

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if node_id in self.nodes:
                print(f"âš ï¸  Node {node_id} already exists in self.nodes, skipping...")
                return True

            # æ£€æŸ¥å›¾ä¸­æ˜¯å¦å·²å­˜åœ¨
            if node_id in self.dependency_graph:
                print(f"âš ï¸  Node {node_id} already exists in dependency_graph, removing...")
                self.dependency_graph.remove_node(node_id)

            self.nodes[node_id] = node
            print(f"â• Adding node to workflow: {node_id}")

            # æ·»åŠ ä¾èµ–å…³ç³»
            dependency = NodeDependency(
                node_id=node_id,
                dependencies=dependencies or [],
                conditions=conditions or {},
                **kwargs
            )
            self.node_dependencies[node_id] = dependency

            # æ„å»ºä¾èµ–å›¾
            self.dependency_graph.add_node(node_id)
            for dep in dependency.dependencies:
                self.dependency_graph.add_edge(dep, node_id)

            # æ£€æŸ¥å¾ªç¯ä¾èµ–
            if not nx.is_directed_acyclic_graph(self.dependency_graph):
                self.dependency_graph.remove_node(node_id)
                del self.nodes[node_id]
                del self.node_dependencies[node_id]
                raise ValueError(f"Adding node {node_id} would create a circular dependency")

            # è°ƒè¯•ï¼šæ˜¾ç¤ºå½“å‰çŠ¶æ€
            graph_node_count = len(self.dependency_graph.nodes())
            nodes_dict_count = len(self.nodes)
            print(f"âœ… Node added: {node_id} | self.nodes={nodes_dict_count}, graph={graph_node_count}")

            return True

        except Exception as e:
            print(f"âŒ Failed to add node: {e}")
            return False

    def remove_node(self, node_id: str) -> bool:
        """ä»å·¥ä½œæµä¸­ç§»é™¤èŠ‚ç‚¹"""
        try:
            if node_id in self.nodes:
                del self.nodes[node_id]
                del self.node_dependencies[node_id]
                self.dependency_graph.remove_node(node_id)
                print(f"âœ… Node removed from workflow: {node_id}")
                return True
            return False
        except Exception as e:
            print(f"âŒ Failed to remove node {node_id}: {e}")
            return False

    def get_execution_order(self) -> List[str]:
        """è·å–èŠ‚ç‚¹æ‰§è¡Œé¡ºåºï¼ˆæ‹“æ‰‘æ’åºï¼‰"""
        try:
            return list(nx.topological_sort(self.dependency_graph))
        except nx.NetworkXError as e:
            raise ValueError(f"Cannot determine execution order: {e}")

    def get_ready_nodes(self) -> List[str]:
        """è·å–å‡†å¤‡æ‰§è¡Œçš„èŠ‚ç‚¹ï¼ˆæ‰€æœ‰ä¾èµ–å·²å®Œæˆï¼‰"""
        ready_nodes = []

        for node_id in self.nodes:
            if node_id in self.executed_nodes or node_id in self.running_nodes:
                continue

            # æ£€æŸ¥ä¾èµ–æ˜¯å¦éƒ½å·²å®Œæˆ
            dependency = self.node_dependencies[node_id]
            dependencies_met = True

            for dep_id in dependency.dependencies:
                if dep_id not in self.executed_nodes:
                    dependencies_met = False
                    break

                # æ£€æŸ¥æ¡ä»¶
                if dependency.conditions:
                    dep_result = self.node_results.get(dep_id)
                    if not self._check_conditions(dep_result, dependency.conditions):
                        dependencies_met = False
                        break

            if dependencies_met:
                ready_nodes.append(node_id)

        return ready_nodes

    def _check_conditions(self, node_result: Optional[NodeResult],
                         conditions: Dict[str, Any]) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹æ‰§è¡Œæ¡ä»¶"""
        if not node_result or not conditions:
            return True

        for condition, expected_value in conditions.items():
            if condition == "status":
                if node_result.status.value != expected_value:
                    return False
            elif condition == "data_key":
                if expected_value not in node_result.data:
                    return False
            elif condition == "custom":
                # è‡ªå®šä¹‰æ¡ä»¶æ£€æŸ¥é€»è¾‘
                if not self._evaluate_custom_condition(node_result, expected_value):
                    return False

        return True

    def _evaluate_custom_condition(self, node_result: NodeResult,
                                  condition: Dict[str, Any]) -> bool:
        """è¯„ä¼°è‡ªå®šä¹‰æ¡ä»¶"""
        # è¿™é‡Œå¯ä»¥å®ç°å¤æ‚çš„æ¡ä»¶è¯„ä¼°é€»è¾‘
        # ä¾‹å¦‚ï¼šæ£€æŸ¥æ•°æ®è´¨é‡ã€æ–‡ä»¶å¤§å°ç­‰
        return True

    async def execute(self, context: ProcessingContext) -> WorkflowResult:
        """æ‰§è¡Œå·¥ä½œæµ"""
        self.workflow_id = context.task_id
        self.start_time = datetime.now()
        self.status = WorkflowStatus.RUNNING

        try:
            print(f"ğŸš€ Starting workflow execution: {self.config.name}")

            # è§¦å‘å¼€å§‹äº‹ä»¶
            await self._trigger_event('workflow_started', context)

            # éªŒè¯å·¥ä½œæµ
            if not self._validate_workflow():
                raise ValueError("Workflow validation failed")

            # æ‰§è¡ŒèŠ‚ç‚¹
            result = await self._execute_nodes(context)

            # è®¡ç®—æœ€ç»ˆç»“æœ
            final_result = self._compile_final_result(result, context)

            # è§¦å‘å®Œæˆäº‹ä»¶
            await self._trigger_event('workflow_completed', final_result)

            return final_result

        except Exception as e:
            print(f"âŒ Workflow execution failed: {e}")
            self.status = WorkflowStatus.FAILED

            error_result = WorkflowResult(
                workflow_id=self.workflow_id,
                status=WorkflowStatus.FAILED,
                execution_time=(datetime.now() - self.start_time).total_seconds(),
                nodes_executed=len(self.executed_nodes),
                nodes_failed=len(self.failed_nodes),
                error_log=[str(e)]
            )

            await self._trigger_event('workflow_failed', error_result)
            return error_result

    def _validate_workflow(self) -> bool:
        """éªŒè¯å·¥ä½œæµé…ç½®"""
        if not self.nodes:
            print("âŒ No nodes in workflow")
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰å­¤ç«‹èŠ‚ç‚¹
        try:
            execution_order = self.get_execution_order()
            if len(execution_order) != len(self.nodes):
                print(f"âŒ Workflow contains unreachable nodes")
                print(f"   Total nodes: {len(self.nodes)}")
                print(f"   Reachable nodes: {len(execution_order)}")
                # ç¡®ä¿æ‰“å°çš„æ˜¯é”®ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼Œä¸æ˜¯å€¼ï¼ˆå¯¹è±¡ï¼‰
                all_node_keys = [str(k) for k in self.nodes.keys()]
                print(f"   All node keys: {all_node_keys}")
                execution_order_str = [str(n) for n in execution_order]
                print(f"   Reachable keys: {execution_order_str}")

                # è°ƒè¯•ï¼šæ£€æŸ¥å›¾ä¸­çš„èŠ‚ç‚¹
                graph_nodes = list(self.dependency_graph.nodes())
                print(f"   Graph nodes ({len(graph_nodes)}): {[str(n) for n in graph_nodes]}")

                unreachable = set(self.nodes.keys()) - set(execution_order)
                unreachable_str = [str(n) for n in unreachable]
                print(f"   Unreachable nodes: {unreachable_str}")

                # æ‰“å°æ¯ä¸ªä¸å¯è¾¾èŠ‚ç‚¹çš„ä¾èµ–å…³ç³»
                for node_id in unreachable:
                    deps = self.node_dependencies.get(str(node_id), NodeDependency(node_id=str(node_id))).dependencies
                    print(f"     {node_id} depends on: {deps}")
                return False
        except ValueError as e:
            print(f"âŒ Workflow validation failed: {e}")
            return False

        # æ£€æŸ¥èŠ‚ç‚¹è¾“å…¥è¾“å‡ºåŒ¹é…
        # æ³¨æ„ï¼šVGP èŠ‚ç‚¹é€šè¿‡ context.intermediate_results åŠ¨æ€ä¼ é€’æ•°æ®
        # é™æ€ schema éªŒè¯å¯èƒ½ä¸å‡†ç¡®ï¼Œæš‚æ—¶ç¦ç”¨ä¸¥æ ¼éªŒè¯
        # å¦‚æœçœŸçš„ç¼ºå°‘è¾“å…¥ï¼ŒèŠ‚ç‚¹æ‰§è¡Œæ—¶ä¼šæŠ¥é”™

        # å¯é€‰ï¼šå¯ç”¨å®½æ¾éªŒè¯ï¼ˆä»…è­¦å‘Šï¼Œä¸é˜»æ­¢æ‰§è¡Œï¼‰
        enable_strict_validation = False

        if enable_strict_validation:
            global_context_inputs = [
                'initial_data', 'user_input',
                'user_description_id', 'video_type_id', 'theme_id',
                'keywords_id', 'target_duration_id', 'reference_media',
                'project_data', 'session_id', 'task_id', 'user_id'
            ]

            for node_id, node in self.nodes.items():
                required_inputs = node.get_required_inputs()
                dependency = self.node_dependencies[node_id]

                # å¦‚æœèŠ‚ç‚¹æ²¡æœ‰ä¾èµ–ï¼ˆèµ·å§‹èŠ‚ç‚¹ï¼‰ï¼Œè·³è¿‡è¾“å…¥éªŒè¯
                if not dependency.dependencies:
                    continue

                # ç¡®ä¿ä¾èµ–èŠ‚ç‚¹èƒ½æä¾›æ‰€éœ€è¾“å…¥
                for required_input in required_inputs:
                    # å¦‚æœæ˜¯å…¨å±€ä¸Šä¸‹æ–‡è¾“å…¥ï¼Œè·³è¿‡éªŒè¯
                    if required_input in global_context_inputs:
                        continue

                    input_available = False
                    for dep_id in dependency.dependencies:
                        dep_node = self.nodes[dep_id]
                        output_schema = dep_node.get_output_schema()
                        if required_input in output_schema:
                            input_available = True
                            break

                    if not input_available:
                        print(f"âš ï¸  Warning: Node {node_id} requires input '{required_input}' not in dependency output schemas")
                        print(f"   (This may be provided via context.intermediate_results at runtime)")
                        # ä¸è¿”å› Falseï¼Œåªæ˜¯è­¦å‘Š

        print("âœ… Workflow validation passed")
        return True

    async def _execute_nodes(self, context: ProcessingContext) -> WorkflowResult:
        """æ‰§è¡Œæ‰€æœ‰èŠ‚ç‚¹"""
        semaphore = asyncio.Semaphore(self.config.max_parallel_nodes)

        while len(self.executed_nodes) + len(self.failed_nodes) < len(self.nodes):
            # è·å–å‡†å¤‡æ‰§è¡Œçš„èŠ‚ç‚¹
            ready_nodes = self.get_ready_nodes()

            if not ready_nodes:
                if self.running_nodes:
                    # ç­‰å¾…è¿è¡Œä¸­çš„èŠ‚ç‚¹å®Œæˆ
                    await asyncio.sleep(0.1)
                    continue
                else:
                    # æ²¡æœ‰å¯æ‰§è¡Œçš„èŠ‚ç‚¹ä¸”æ²¡æœ‰è¿è¡Œä¸­çš„èŠ‚ç‚¹ï¼Œå¯èƒ½æ˜¯æ­»é”
                    remaining_nodes = set(self.nodes.keys()) - self.executed_nodes - self.failed_nodes
                    raise RuntimeError(f"Workflow deadlock detected. Remaining nodes: {remaining_nodes}")

            # å¹¶å‘æ‰§è¡Œå‡†å¤‡å°±ç»ªçš„èŠ‚ç‚¹
            tasks = []
            for node_id in ready_nodes:
                if len(self.running_nodes) < self.config.max_parallel_nodes:
                    task = asyncio.create_task(
                        self._execute_single_node(node_id, context, semaphore)
                    )
                    tasks.append(task)
                    self.running_nodes.add(node_id)

            # ç­‰å¾…è‡³å°‘ä¸€ä¸ªèŠ‚ç‚¹å®Œæˆ
            if tasks:
                done, pending = await asyncio.wait(
                    tasks, return_when=asyncio.FIRST_COMPLETED
                )

                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                for task in done:
                    try:
                        await task
                    except Exception as e:
                        print(f"Node execution error: {e}")

        # ç¼–è¯‘æ‰§è¡Œç»“æœ
        execution_time = (datetime.now() - self.start_time).total_seconds()

        return WorkflowResult(
            workflow_id=self.workflow_id,
            status=WorkflowStatus.COMPLETED if not self.failed_nodes else WorkflowStatus.FAILED,
            execution_time=execution_time,
            nodes_executed=len(self.executed_nodes),
            nodes_failed=len(self.failed_nodes),
            intermediate_results={node_id: result.data for node_id, result in self.node_results.items()}
        )

    async def _execute_single_node(self, node_id: str, context: ProcessingContext,
                                  semaphore: asyncio.Semaphore):
        """æ‰§è¡Œå•ä¸ªèŠ‚ç‚¹"""
        async with semaphore:
            try:
                node = self.nodes[node_id]
                print(f"ğŸ”„ Executing node: {node_id}")

                # å‡†å¤‡èŠ‚ç‚¹ä¸Šä¸‹æ–‡
                node_context = self._prepare_node_context(node_id, context)

                # è§¦å‘èŠ‚ç‚¹å¼€å§‹äº‹ä»¶
                await self._trigger_event('node_started', {'node_id': node_id, 'context': node_context})

                # æ‰§è¡ŒèŠ‚ç‚¹
                start_time = datetime.now()
                result = await node.execute(node_context)
                execution_time = (datetime.now() - start_time).total_seconds()

                # æ›´æ–°ç»Ÿè®¡
                self.metrics['nodes_executed'] += 1
                self.metrics['total_execution_time'] += execution_time

                # ä¿å­˜ç»“æœ
                self.node_results[node_id] = result

                if result.is_success():
                    self.executed_nodes.add(node_id)
                    print(f"âœ… Node completed: {node_id} ({execution_time:.2f}s)")

                    # æ›´æ–°ä¸Šä¸‹æ–‡
                    context.intermediate_results.update(result.data)

                    await self._trigger_event('node_completed', {
                        'node_id': node_id,
                        'result': result,
                        'execution_time': execution_time
                    })
                else:
                    self.failed_nodes.add(node_id)
                    print(f"âŒ Node failed: {node_id} - {result.error_message}")

                    await self._trigger_event('node_failed', {
                        'node_id': node_id,
                        'error': result.error_message
                    })

                    # æ ¹æ®é”™è¯¯å¤„ç†ç­–ç•¥å†³å®šæ˜¯å¦ç»§ç»­
                    if self.config.error_handling_strategy == "fail_fast":
                        raise RuntimeError(f"Node {node_id} failed: {result.error_message}")

            except Exception as e:
                self.failed_nodes.add(node_id)
                print(f"âŒ Node execution exception: {node_id} - {e}")

                await self._trigger_event('node_error', {
                    'node_id': node_id,
                    'exception': str(e)
                })

                if self.config.error_handling_strategy == "fail_fast":
                    raise

            finally:
                self.running_nodes.discard(node_id)

    def _prepare_node_context(self, node_id: str, context: ProcessingContext) -> ProcessingContext:
        """ä¸ºèŠ‚ç‚¹å‡†å¤‡æ‰§è¡Œä¸Šä¸‹æ–‡"""
        # åˆ›å»ºèŠ‚ç‚¹ä¸“ç”¨çš„ä¸Šä¸‹æ–‡å‰¯æœ¬
        node_context = ProcessingContext(
            task_id=context.task_id,
            session_id=context.session_id,
            user_id=context.user_id,
            project_data=context.project_data.copy(),
            intermediate_results=context.intermediate_results.copy(),
            metadata=context.metadata.copy()
        )

        # æ·»åŠ èŠ‚ç‚¹ç‰¹å®šçš„å…ƒæ•°æ®
        node_context.metadata['current_node'] = node_id
        node_context.metadata['execution_order'] = len(self.executed_nodes) + 1

        return node_context

    def _compile_final_result(self, execution_result: WorkflowResult,
                            context: ProcessingContext) -> WorkflowResult:
        """ç¼–è¯‘æœ€ç»ˆå·¥ä½œæµç»“æœ"""
        # æå–æœ€ç»ˆè¾“å‡º
        final_output = {}

        # æŸ¥æ‰¾è¾“å‡ºèŠ‚ç‚¹ï¼ˆæ²¡æœ‰åç»­ä¾èµ–çš„èŠ‚ç‚¹ï¼‰
        output_nodes = [
            node_id for node_id in self.nodes
            if not list(self.dependency_graph.successors(node_id))
        ]

        for node_id in output_nodes:
            if node_id in self.node_results:
                result = self.node_results[node_id]
                if result.is_success():
                    final_output[node_id] = result.data

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        self._calculate_performance_metrics()

        execution_result.final_output = final_output
        execution_result.performance_metrics = self.metrics

        return execution_result

    def _calculate_performance_metrics(self):
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        if self.metrics['nodes_executed'] > 0:
            self.metrics['average_node_time'] = (
                self.metrics['total_execution_time'] / self.metrics['nodes_executed']
            )

        # è¯†åˆ«ç“¶é¢ˆèŠ‚ç‚¹
        node_times = []
        for node_id, result in self.node_results.items():
            if result.is_success():
                node_times.append((node_id, result.execution_time))

        # æŒ‰æ‰§è¡Œæ—¶é—´æ’åºï¼Œæ‰¾å‡ºæœ€æ…¢çš„èŠ‚ç‚¹
        node_times.sort(key=lambda x: x[1], reverse=True)
        self.metrics['bottleneck_nodes'] = node_times[:3]  # å‰3ä¸ªæœ€æ…¢çš„èŠ‚ç‚¹

    async def _trigger_event(self, event_type: str, data: Any):
        """è§¦å‘äº‹ä»¶å¤„ç†å™¨"""
        handlers = self.event_handlers.get(event_type, [])
        for handler in handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler(data)
                else:
                    handler(data)
            except Exception as e:
                print(f"Event handler error ({event_type}): {e}")

    def add_event_handler(self, event_type: str, handler: Callable):
        """æ·»åŠ äº‹ä»¶å¤„ç†å™¨"""
        self.event_handlers[event_type].append(handler)

    def get_workflow_status(self) -> Dict[str, Any]:
        """è·å–å·¥ä½œæµçŠ¶æ€"""
        return {
            'workflow_id': self.workflow_id,
            'status': self.status.value,
            'config': {
                'name': self.config.name,
                'description': self.config.description
            },
            'execution': {
                'start_time': self.start_time.isoformat() if self.start_time else None,
                'nodes_total': len(self.nodes),
                'nodes_executed': len(self.executed_nodes),
                'nodes_running': len(self.running_nodes),
                'nodes_failed': len(self.failed_nodes)
            },
            'metrics': self.metrics
        }

    def get_node_status(self, node_id: str) -> Optional[Dict[str, Any]]:
        """è·å–èŠ‚ç‚¹çŠ¶æ€"""
        if node_id not in self.nodes:
            return None

        node = self.nodes[node_id]
        result = self.node_results.get(node_id)

        status_info = {
            'node_id': node_id,
            'node_name': node.node_name,
            'dependencies': self.node_dependencies[node_id].dependencies,
            'executed': node_id in self.executed_nodes,
            'running': node_id in self.running_nodes,
            'failed': node_id in self.failed_nodes
        }

        if result:
            status_info.update({
                'status': result.status.value,
                'execution_time': result.execution_time,
                'error_message': result.error_message
            })

        return status_info

    async def pause(self):
        """æš‚åœå·¥ä½œæµæ‰§è¡Œ"""
        self.status = WorkflowStatus.PAUSED
        print(f"â¸ï¸ Workflow paused: {self.config.name}")

    async def resume(self):
        """æ¢å¤å·¥ä½œæµæ‰§è¡Œ"""
        if self.status == WorkflowStatus.PAUSED:
            self.status = WorkflowStatus.RUNNING
            print(f"â–¶ï¸ Workflow resumed: {self.config.name}")

    async def cancel(self):
        """å–æ¶ˆå·¥ä½œæµæ‰§è¡Œ"""
        self.status = WorkflowStatus.CANCELLED
        print(f"ğŸ›‘ Workflow cancelled: {self.config.name}")

    def reset(self):
        """é‡ç½®å·¥ä½œæµçŠ¶æ€"""
        self.status = WorkflowStatus.IDLE
        self.workflow_id = None
        self.start_time = None
        self.executed_nodes.clear()
        self.failed_nodes.clear()
        self.running_nodes.clear()
        self.node_results.clear()
        self.metrics = {
            'nodes_executed': 0,
            'nodes_failed': 0,
            'total_execution_time': 0.0,
            'average_node_time': 0.0,
            'bottleneck_nodes': [],
            'resource_usage': {}
        }
        print(f"ğŸ”„ Workflow reset: {self.config.name}")