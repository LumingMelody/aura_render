"""
å›¾ç‰‡åˆ°è§†é¢‘ç¼–æ’å™¨ - æ”¯æŒä»å•å¼ å›¾ç‰‡ç”Ÿæˆè§†é¢‘
"""
from typing import Dict, List, Any, Optional
import asyncio
from dataclasses import dataclass
from pathlib import Path
import base64

from enhanced_video_orchestrator import EnhancedVideoOrchestrator
from qwen_integration import QwenVideoGenerator


@dataclass
class ImageToVideoRequest:
    """å›¾ç‰‡åˆ°è§†é¢‘è¯·æ±‚"""

    # å¿…å¡«å­—æ®µ
    image_path: str  # è¾“å…¥å›¾ç‰‡è·¯å¾„
    duration_seconds: int  # è§†é¢‘æ—¶é•¿

    # å¯é€‰å­—æ®µ
    description: Optional[str] = None  # è§†é¢‘æè¿°ï¼ˆå¯é€‰ï¼Œç”¨äºå¼•å¯¼ç”Ÿæˆï¼‰
    motion_intensity: str = "medium"  # è¿åŠ¨å¼ºåº¦: low, medium, high
    style: str = "realistic"  # è§†é¢‘é£æ ¼

    # ç”Ÿæˆå‚æ•°
    generation_mode: str = "single_image_extend"  # ç”Ÿæˆæ¨¡å¼
    fps: int = 30  # å¸§ç‡
    resolution: str = "1920x1080"  # åˆ†è¾¨ç‡

    # è¾“å‡ºé…ç½®
    output_path: Optional[str] = None
    save_intermediate: bool = False  # æ˜¯å¦ä¿å­˜ä¸­é—´å¸§


@dataclass
class ImageToVideoResponse:
    """å›¾ç‰‡åˆ°è§†é¢‘å“åº”"""
    success: bool
    video_path: Optional[str] = None
    duration_seconds: int = 0
    segments_count: int = 0
    generation_mode: str = ""
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = None


class ImageToVideoOrchestrator:
    """å›¾ç‰‡åˆ°è§†é¢‘ç¼–æ’å™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config

        # åƒé—®è§†é¢‘ç”Ÿæˆå™¨
        qwen_key = config.get("qwen_api_key")
        if not qwen_key:
            raise ValueError("qwen_api_key is required")
        self.qwen_generator = QwenVideoGenerator(qwen_key)

        # å¢å¼ºè§†é¢‘ç¼–æ’å™¨ï¼ˆç”¨äºå¤æ‚åœºæ™¯ï¼‰
        self.enhanced_orchestrator = EnhancedVideoOrchestrator(config)

        self.work_dir = Path(config.get("work_dir", "/tmp/image_to_video"))
        self.work_dir.mkdir(parents=True, exist_ok=True)

    async def process_image_to_video(self, request: ImageToVideoRequest) -> ImageToVideoResponse:
        """
        å¤„ç†å›¾ç‰‡åˆ°è§†é¢‘çš„è¯·æ±‚
        """

        print(f"\nğŸ–¼ï¸ å¼€å§‹å›¾ç‰‡åˆ°è§†é¢‘ç”Ÿæˆ")
        print(f"ğŸ“ è¾“å…¥å›¾ç‰‡: {request.image_path}")
        print(f"â±ï¸ ç›®æ ‡æ—¶é•¿: {request.duration_seconds}ç§’")
        print("="*60)

        try:
            # éªŒè¯è¾“å…¥å›¾ç‰‡
            if not Path(request.image_path).exists():
                raise FileNotFoundError(f"è¾“å…¥å›¾ç‰‡ä¸å­˜åœ¨: {request.image_path}")

            # æ ¹æ®æ—¶é•¿é€‰æ‹©ç”Ÿæˆç­–ç•¥
            if request.duration_seconds <= 5:
                # 5ç§’ä»¥å†…ï¼Œç›´æ¥ä½¿ç”¨åƒé—®å•æ¬¡ç”Ÿæˆ
                result = await self._generate_short_video(request)
            else:
                # è¶…è¿‡5ç§’ï¼Œéœ€è¦åˆ†æ®µç”Ÿæˆ
                result = await self._generate_long_video(request)

            return result

        except Exception as e:
            print(f"âŒ å›¾ç‰‡åˆ°è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return ImageToVideoResponse(
                success=False,
                error_message=str(e)
            )

    async def _generate_short_video(self, request: ImageToVideoRequest) -> ImageToVideoResponse:
        """ç”ŸæˆçŸ­è§†é¢‘ï¼ˆâ‰¤5ç§’ï¼‰"""

        print("\n[æ¨¡å¼] ğŸ¬ çŸ­è§†é¢‘ç›´æ¥ç”Ÿæˆ")

        if request.generation_mode == "single_image_extend":
            # å•å›¾æ‰©å±•æ¨¡å¼ï¼šä½¿ç”¨åŒä¸€å¼ å›¾ä½œä¸ºé¦–å°¾å¸§
            video_result = await self.qwen_generator.generate_video_from_frames(
                start_image_path=request.image_path,
                end_image_path=request.image_path,  # ä½¿ç”¨åŒä¸€å¼ å›¾
                duration_seconds=request.duration_seconds
            )

        elif request.generation_mode == "image_to_sequence":
            # å›¾ç‰‡åºåˆ—æ¨¡å¼ï¼šç”Ÿæˆä¸­é—´å¸§ç„¶åç”Ÿæˆè§†é¢‘
            end_frame_path = await self._generate_end_frame(request)

            video_result = await self.qwen_generator.generate_video_from_frames(
                start_image_path=request.image_path,
                end_image_path=end_frame_path,
                duration_seconds=request.duration_seconds
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç”Ÿæˆæ¨¡å¼: {request.generation_mode}")

        if video_result["success"]:
            # ç­‰å¾…ç”Ÿæˆå®Œæˆ
            completion_result = await self.qwen_generator.wait_for_completion(
                video_result["task_id"]
            )

            if completion_result["success"]:
                # ä¸‹è½½è§†é¢‘
                output_path = request.output_path or str(self.work_dir / "output.mp4")
                await self._download_video(completion_result["video_url"], output_path)

                return ImageToVideoResponse(
                    success=True,
                    video_path=output_path,
                    duration_seconds=request.duration_seconds,
                    segments_count=1,
                    generation_mode=request.generation_mode,
                    metadata={
                        "input_image": request.image_path,
                        "motion_intensity": request.motion_intensity,
                        "task_id": video_result["task_id"]
                    }
                )
            else:
                raise Exception(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {completion_result['error']}")
        else:
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {video_result['error']}")

    async def _generate_long_video(self, request: ImageToVideoRequest) -> ImageToVideoResponse:
        """ç”Ÿæˆé•¿è§†é¢‘ï¼ˆ>5ç§’ï¼‰"""

        print(f"\n[æ¨¡å¼] ğŸ¬ é•¿è§†é¢‘åˆ†æ®µç”Ÿæˆ ({request.duration_seconds}ç§’)")

        # è®¡ç®—éœ€è¦çš„æ®µæ•°
        segments_count = (request.duration_seconds + 4) // 5
        print(f"  ğŸ“Š å°†åˆ†æˆ {segments_count} ä¸ª5ç§’ç‰‡æ®µ")

        # ç”Ÿæˆå…³é”®å¸§åºåˆ—
        keyframes = await self._generate_keyframes_from_image(request, segments_count)

        # ç”Ÿæˆè§†é¢‘ç‰‡æ®µ
        video_clips = []
        for i in range(segments_count):
            start_frame = keyframes[i]
            end_frame = keyframes[i + 1] if i + 1 < len(keyframes) else keyframes[i]

            print(f"  ğŸ¥ ç”Ÿæˆç‰‡æ®µ {i+1}/{segments_count}")

            clip_result = await self.qwen_generator.generate_video_from_frames(
                start_image_path=start_frame["path"],
                end_image_path=end_frame["path"],
                duration_seconds=5.0
            )

            if clip_result["success"]:
                completion = await self.qwen_generator.wait_for_completion(clip_result["task_id"])
                if completion["success"]:
                    clip_path = str(self.work_dir / f"clip_{i:03d}.mp4")
                    await self._download_video(completion["video_url"], clip_path)
                    video_clips.append(clip_path)
                    print(f"    âœ… ç‰‡æ®µ {i+1} ç”Ÿæˆå®Œæˆ")
                else:
                    print(f"    âŒ ç‰‡æ®µ {i+1} ç”Ÿæˆå¤±è´¥")
            else:
                print(f"    âŒ ç‰‡æ®µ {i+1} APIè°ƒç”¨å¤±è´¥")

        if not video_clips:
            raise Exception("æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•è§†é¢‘ç‰‡æ®µ")

        # åˆå¹¶è§†é¢‘ç‰‡æ®µ
        output_path = request.output_path or str(self.work_dir / "final_video.mp4")
        final_video = await self._merge_video_clips(video_clips, output_path)

        return ImageToVideoResponse(
            success=True,
            video_path=final_video,
            duration_seconds=request.duration_seconds,
            segments_count=len(video_clips),
            generation_mode="multi_segment",
            metadata={
                "input_image": request.image_path,
                "keyframes_count": len(keyframes),
                "clips_generated": len(video_clips)
            }
        )

    async def _generate_keyframes_from_image(self, request: ImageToVideoRequest, segments_count: int) -> List[Dict]:
        """ä»è¾“å…¥å›¾ç‰‡ç”Ÿæˆå…³é”®å¸§åºåˆ—"""

        print("  ğŸ–¼ï¸ ç”Ÿæˆå…³é”®å¸§åºåˆ—...")

        keyframes = []

        # ç¬¬ä¸€å¸§ä½¿ç”¨åŸå§‹å›¾ç‰‡
        keyframes.append({
            "frame_id": "frame_000",
            "path": request.image_path,
            "is_original": True
        })

        # æ ¹æ®æè¿°ç”Ÿæˆä¸­é—´å¸§å’Œå°¾å¸§
        if request.description:
            # æœ‰æè¿°ï¼Œä½¿ç”¨AIç”Ÿæˆæ¼”å˜å¸§
            for i in range(1, segments_count + 1):
                # ç”Ÿæˆæè¿°è¯¥æ—¶é—´ç‚¹çš„æç¤ºè¯
                time_progress = i / segments_count
                frame_prompt = self._generate_frame_prompt(request, time_progress)

                # ä½¿ç”¨å›¾ç”Ÿå›¾ç”Ÿæˆå¸§
                frame_path = await self._generate_frame_from_prompt(
                    reference_image=request.image_path,
                    prompt=frame_prompt,
                    frame_id=f"frame_{i:03d}"
                )

                keyframes.append({
                    "frame_id": f"frame_{i:03d}",
                    "path": frame_path,
                    "is_original": False,
                    "prompt": frame_prompt
                })
        else:
            # æ— æè¿°ï¼Œç”Ÿæˆè½»å¾®å˜åŒ–çš„å¸§
            for i in range(1, segments_count + 1):
                # ç”Ÿæˆè½»å¾®è¿åŠ¨çš„å¸§
                frame_path = await self._generate_motion_frame(
                    reference_image=request.image_path,
                    motion_intensity=request.motion_intensity,
                    frame_id=f"frame_{i:03d}"
                )

                keyframes.append({
                    "frame_id": f"frame_{i:03d}",
                    "path": frame_path,
                    "is_original": False
                })

        print(f"  âœ… ç”Ÿæˆäº† {len(keyframes)} ä¸ªå…³é”®å¸§")
        return keyframes

    def _generate_frame_prompt(self, request: ImageToVideoRequest, time_progress: float) -> str:
        """æ ¹æ®æ—¶é—´è¿›åº¦ç”Ÿæˆå¸§æç¤ºè¯"""

        base_description = request.description or "natural movement and progression"

        # æ ¹æ®æ—¶é—´è¿›åº¦æ·»åŠ å˜åŒ–æè¿°
        if time_progress < 0.3:
            stage = "beginning"
            description = f"early stage, {base_description}"
        elif time_progress < 0.7:
            stage = "middle"
            description = f"developing, {base_description}"
        else:
            stage = "end"
            description = f"final stage, {base_description}"

        # æ·»åŠ è¿åŠ¨å¼ºåº¦
        motion_desc = {
            "low": "subtle movement",
            "medium": "moderate motion",
            "high": "dynamic action"
        }.get(request.motion_intensity, "moderate motion")

        return f"{description}, {motion_desc}, {request.style} style"

    async def _generate_frame_from_prompt(self, reference_image: str, prompt: str, frame_id: str) -> str:
        """ä½¿ç”¨æç¤ºè¯ä»å‚è€ƒå›¾ç”Ÿæˆæ–°å¸§"""

        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„å›¾ç”Ÿå›¾API
        # æš‚æ—¶è¿”å›åŸå›¾è·¯å¾„ï¼ˆå®é™…åº”è¯¥ç”Ÿæˆæ–°çš„å˜åŒ–å¸§ï¼‰

        print(f"    ç”Ÿæˆå¸§ {frame_id}: {prompt[:50]}...")

        # æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹
        output_path = str(self.work_dir / f"{frame_id}.png")

        # å®é™…åº”è¯¥è°ƒç”¨ img2img API
        # await self.image_generation_api.img2img(
        #     reference_image=reference_image,
        #     prompt=prompt,
        #     output_path=output_path
        # )

        # æš‚æ—¶å¤åˆ¶åŸå›¾ï¼ˆæ¼”ç¤ºç”¨ï¼‰
        import shutil
        shutil.copy2(reference_image, output_path)

        return output_path

    async def _generate_motion_frame(self, reference_image: str, motion_intensity: str, frame_id: str) -> str:
        """ç”Ÿæˆè¿åŠ¨å¸§"""

        motion_prompts = {
            "low": "very subtle movement, minimal change",
            "medium": "gentle motion, natural progression",
            "high": "dynamic movement, noticeable change"
        }

        prompt = motion_prompts.get(motion_intensity, motion_prompts["medium"])

        return await self._generate_frame_from_prompt(reference_image, prompt, frame_id)

    async def _generate_end_frame(self, request: ImageToVideoRequest) -> str:
        """ç”Ÿæˆå°¾å¸§"""

        if request.description:
            end_prompt = f"final result of {request.description}, {request.style} style"
        else:
            end_prompt = f"natural progression, {request.motion_intensity} motion, {request.style} style"

        return await self._generate_frame_from_prompt(
            request.image_path,
            end_prompt,
            "end_frame"
        )

    async def _download_video(self, url: str, output_path: str):
        """ä¸‹è½½è§†é¢‘æ–‡ä»¶"""

        import aiohttp

        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                if response.status == 200:
                    content = await response.read()
                    with open(output_path, "wb") as f:
                        f.write(content)
                else:
                    raise Exception(f"ä¸‹è½½è§†é¢‘å¤±è´¥: {response.status}")

    async def _merge_video_clips(self, clip_paths: List[str], output_path: str) -> str:
        """åˆå¹¶è§†é¢‘ç‰‡æ®µ"""

        print(f"  ğŸ”„ åˆå¹¶ {len(clip_paths)} ä¸ªè§†é¢‘ç‰‡æ®µ...")

        # åˆ›å»ºffmpegè¾“å…¥åˆ—è¡¨
        list_file = self.work_dir / "clips.txt"
        with open(list_file, "w") as f:
            for clip_path in clip_paths:
                f.write(f"file '{clip_path}'\n")

        # ä½¿ç”¨ffmpegåˆå¹¶
        import subprocess

        cmd = [
            "ffmpeg",
            "-f", "concat",
            "-safe", "0",
            "-i", str(list_file),
            "-c", "copy",
            "-y",
            output_path
        ]

        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode != 0:
            raise Exception(f"è§†é¢‘åˆå¹¶å¤±è´¥: {result.stderr}")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        list_file.unlink()

        print(f"  âœ… è§†é¢‘åˆå¹¶å®Œæˆ: {output_path}")
        return output_path


# APIæ¥å£å‡½æ•°
async def generate_video_from_image(
    image_path: str,
    duration_seconds: int,
    description: Optional[str] = None,
    motion_intensity: str = "medium",
    config: Optional[Dict] = None
) -> ImageToVideoResponse:
    """
    ä¾¿æ·APIï¼šä»å›¾ç‰‡ç”Ÿæˆè§†é¢‘

    å‚æ•°:
        image_path: è¾“å…¥å›¾ç‰‡è·¯å¾„
        duration_seconds: è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
        description: è§†é¢‘æè¿°ï¼ˆå¯é€‰ï¼‰
        motion_intensity: è¿åŠ¨å¼ºåº¦ (low/medium/high)
        config: é…ç½®ä¿¡æ¯

    è¿”å›:
        ImageToVideoResponse
    """

    if not config:
        config = {
            "qwen_api_key": "your_qwen_api_key",
            "work_dir": "/tmp/image_to_video"
        }

    orchestrator = ImageToVideoOrchestrator(config)

    request = ImageToVideoRequest(
        image_path=image_path,
        duration_seconds=duration_seconds,
        description=description,
        motion_intensity=motion_intensity
    )

    return await orchestrator.process_image_to_video(request)


# ä½¿ç”¨ç¤ºä¾‹
async def demo_image_to_video():
    """æ¼”ç¤ºå›¾ç‰‡åˆ°è§†é¢‘åŠŸèƒ½"""

    # é…ç½®
    config = {
        "qwen_api_key": "your_actual_qwen_key",
        "work_dir": "/tmp/image_to_video_demo"
    }

    # ç¤ºä¾‹1: çŸ­è§†é¢‘ï¼ˆ5ç§’ä»¥å†…ï¼‰
    print("\nğŸ¬ ç¤ºä¾‹1: çŸ­è§†é¢‘ç”Ÿæˆ")
    short_result = await generate_video_from_image(
        image_path="/path/to/your/image.jpg",
        duration_seconds=3,
        description="äº§å“ç¼“æ…¢æ—‹è½¬å±•ç¤º",
        motion_intensity="low",
        config=config
    )

    if short_result.success:
        print(f"âœ… çŸ­è§†é¢‘ç”ŸæˆæˆåŠŸ: {short_result.video_path}")
    else:
        print(f"âŒ çŸ­è§†é¢‘ç”Ÿæˆå¤±è´¥: {short_result.error_message}")

    # ç¤ºä¾‹2: é•¿è§†é¢‘ï¼ˆè¶…è¿‡5ç§’ï¼‰
    print("\nğŸ¬ ç¤ºä¾‹2: é•¿è§†é¢‘ç”Ÿæˆ")
    long_result = await generate_video_from_image(
        image_path="/path/to/your/image.jpg",
        duration_seconds=15,
        description="ä»é™æ€å±•ç¤ºåˆ°åŠ¨æ€ä½¿ç”¨åœºæ™¯çš„è½¬å˜",
        motion_intensity="medium",
        config=config
    )

    if long_result.success:
        print(f"âœ… é•¿è§†é¢‘ç”ŸæˆæˆåŠŸ: {long_result.video_path}")
        print(f"ğŸ“Š ç‰‡æ®µæ•°: {long_result.segments_count}")
    else:
        print(f"âŒ é•¿è§†é¢‘ç”Ÿæˆå¤±è´¥: {long_result.error_message}")


if __name__ == "__main__":
    asyncio.run(demo_image_to_video())