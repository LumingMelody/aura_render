"""
è§†é¢‘åˆ†é•œç¼–æ’å™¨ - æ•´åˆç°æœ‰èŠ‚ç‚¹æ¶æ„å®ç°åˆ†é•œåˆ°è§†é¢‘çš„å®Œæ•´æµç¨‹
"""
from typing import Dict, List, Any, Optional
import asyncio
from dataclasses import dataclass
from pathlib import Path

# å¯¼å…¥ç°æœ‰èŠ‚ç‚¹
from video_generate_protocol.nodes.image_generation_node import (
    ImageGenerationNode,
    ImageGenerationTask,
    ImageGenerationNodeRequest
)
from vgp_optimization_node import VGPOptimizationNode, VGPOptimizationConfig
from storyboard_sequence_node import StoryboardSequenceNode
from qwen_integration import StoryboardToVideoProcessor


@dataclass
class VideoStoryboardRequest:
    """è§†é¢‘åˆ†é•œè¯·æ±‚"""
    text_description: str
    duration_seconds: int
    product_info: Optional[Dict] = None
    style_config: Optional[Dict] = None
    output_path: Optional[str] = None

    # VGPç³»ç»Ÿå¿…éœ€çš„IDå‚æ•°
    theme_id: Optional[str] = None
    keywords_id: Optional[str] = None
    target_duration_id: Optional[str] = None
    user_description_id: Optional[str] = None


class VideoStoryboardOrchestrator:
    """
    è§†é¢‘åˆ†é•œç¼–æ’å™¨

    æ•´åˆç°æœ‰çš„ video_generate_protocol/nodes ä¸­çš„èŠ‚ç‚¹ï¼š
    - ImageGenerationNode: å›¾åƒç”Ÿæˆ
    - VGPOptimizationNode: åˆ†é•œä¼˜åŒ–
    - StoryboardSequenceNode: åˆ†é•œåºåˆ—
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config

        # åˆå§‹åŒ–ç°æœ‰èŠ‚ç‚¹
        self.image_generation_node = ImageGenerationNode(config)

        # VGPä¼˜åŒ–é…ç½®
        vgp_config = VGPOptimizationConfig(
            product_protection_level="maximum",
            era_preference=config.get("era_preference", "modern"),
            forbidden_elements=config.get("forbidden_elements", []),
            prefer_wide_shots=True,
            enhance_lighting=True
        )
        self.vgp_optimization_node = VGPOptimizationNode(vgp_config)

        # åˆ†é•œåºåˆ—èŠ‚ç‚¹
        self.storyboard_sequence_node = StoryboardSequenceNode(config)

        # åƒé—®è§†é¢‘å¤„ç†å™¨
        qwen_key = config.get("qwen_api_key")
        if not qwen_key:
            raise ValueError("qwen_api_key is required")
        self.video_processor = StoryboardToVideoProcessor(qwen_key)

    async def process_video_request(self, request: VideoStoryboardRequest) -> Dict[str, Any]:
        """
        å¤„ç†è§†é¢‘ç”Ÿæˆè¯·æ±‚ - ä½¿ç”¨ç°æœ‰èŠ‚ç‚¹æ¶æ„
        """

        print(f"\nğŸ¬ å¼€å§‹è§†é¢‘ç”Ÿæˆæµç¨‹ ({request.duration_seconds}ç§’)")
        print("="*60)

        try:
            # ç¬¬1æ­¥ï¼šVGPä¼˜åŒ–åˆ†é•œè§„åˆ’
            storyboard_plan = await self._optimize_storyboard(request)

            # ç¬¬2æ­¥ï¼šä½¿ç”¨ç°æœ‰ImageGenerationNodeç”Ÿæˆå…³é”®å¸§
            keyframes = await self._generate_keyframes_with_existing_node(storyboard_plan)

            # ç¬¬3æ­¥ï¼šä½¿ç”¨åƒé—®APIç”Ÿæˆ5ç§’è§†é¢‘ç‰‡æ®µ
            video_clips = await self._generate_video_clips(keyframes)

            # ç¬¬4æ­¥ï¼šåˆå¹¶æœ€ç»ˆè§†é¢‘
            final_video = await self._merge_final_video(video_clips, request.output_path)

            return {
                "success": True,
                "video_path": final_video,
                "duration_seconds": request.duration_seconds,
                "segments_count": len(video_clips),
                "keyframes_count": len(keyframes),
                "storyboard_plan": storyboard_plan
            }

        except Exception as e:
            print(f"âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def _optimize_storyboard(self, request: VideoStoryboardRequest) -> Dict:
        """ç¬¬1æ­¥ï¼šä½¿ç”¨VGPèŠ‚ç‚¹ä¼˜åŒ–åˆ†é•œ"""

        print("\n[ç¬¬1æ­¥] ğŸ“‹ VGPä¼˜åŒ–åˆ†é•œè§„åˆ’...")

        # è§£ææ–‡æœ¬ä¸ºåŸå§‹æ®µè½
        raw_segments = self._parse_text_to_segments(request.text_description)

        # è°ƒç”¨ç°æœ‰çš„VGPä¼˜åŒ–èŠ‚ç‚¹
        optimization_result = await self.vgp_optimization_node.optimize_storyboard_sequence(
            raw_segments=raw_segments,
            product_info=request.product_info,
            total_duration_ms=request.duration_seconds * 1000
        )

        # æ˜¾ç¤ºä¼˜åŒ–ç»“æœ
        frames = optimization_result['optimized_frames']
        segments_count = (request.duration_seconds + 4) // 5  # 5ç§’ç‰‡æ®µæ•°

        print(f"  âœ… ç”Ÿæˆäº† {len(frames)} ä¸ªå…³é”®å¸§")
        print(f"  âœ… è§„åˆ’äº† {segments_count} ä¸ª5ç§’ç‰‡æ®µ")

        # åˆ†æç”Ÿæˆæ¨¡å¼åˆ†å¸ƒ
        mode_stats = {}
        for frame in frames:
            mode = frame.generation_mode.value
            mode_stats[mode] = mode_stats.get(mode, 0) + 1

        print("  ğŸ“Š ç”Ÿæˆæ¨¡å¼åˆ†å¸ƒ:")
        for mode, count in mode_stats.items():
            print(f"    {mode}: {count}")

        return optimization_result

    async def _generate_keyframes_with_existing_node(self, storyboard_plan: Dict) -> List[Dict]:
        """ç¬¬2æ­¥ï¼šä½¿ç”¨ç°æœ‰ImageGenerationNodeç”Ÿæˆå…³é”®å¸§"""

        print("\n[ç¬¬2æ­¥] ğŸ¨ ç”Ÿæˆå…³é”®å¸§å›¾åƒ...")

        optimized_frames = storyboard_plan['optimized_frames']
        generated_keyframes = []

        # è½¬æ¢ä¸ºImageGenerationTaskæ ¼å¼
        image_tasks = []
        for frame in optimized_frames:
            # è·å–ä¼˜åŒ–åçš„æç¤ºè¯
            prompt = frame.prompt_optimization.get('base_description', frame.description)

            # æ·»åŠ é£æ ¼æ ‡ç­¾
            style_tags = frame.prompt_optimization.get('style_tags', [])
            if style_tags:
                prompt += f", {', '.join(style_tags)}"

            # æ·»åŠ è´¨é‡æ ‡ç­¾
            quality_tags = frame.prompt_optimization.get('quality_tags', [])
            if quality_tags:
                prompt += f", {', '.join(quality_tags)}"

            # åˆ›å»ºå›¾åƒç”Ÿæˆä»»åŠ¡
            task = ImageGenerationTask(
                prompt=prompt,
                negative_prompt="low quality, blurry, distorted",
                style="realistic",
                quality="high",
                aspect_ratio="16:9",  # è§†é¢‘æ¯”ä¾‹
                width=1920,
                height=1080,
                reference_image=frame.reference_product_image if hasattr(frame, 'reference_product_image') else None
            )
            image_tasks.append((frame, task))

        # æ‰¹é‡ç”Ÿæˆå›¾åƒ
        tasks_only = [task for _, task in image_tasks]
        request = ImageGenerationNodeRequest(
            tasks=tasks_only,
            batch_mode=True,
            generation_config={
                "enhance_prompts": True,
                "fallback_enabled": True
            }
        )

        print(f"  ğŸ”„ æ‰¹é‡ç”Ÿæˆ {len(tasks_only)} å¼ å›¾åƒ...")
        response = await self.image_generation_node.process(request)

        if response.generated_images:
            print(f"  âœ… æˆåŠŸç”Ÿæˆ {len(response.generated_images)} å¼ å›¾åƒ")
            print(f"  ğŸ’° æ€»æˆæœ¬: ${response.total_cost:.4f}")
            print(f"  â±ï¸ æ€»æ—¶é—´: {response.total_time_ms/1000:.2f}ç§’")

            # è½¬æ¢ä¸ºkeyframeæ ¼å¼
            for i, (frame, generated_image) in enumerate(zip([f for f, _ in image_tasks], response.generated_images)):
                keyframe = {
                    "frame_id": frame.frame_id,
                    "segment_id": frame.segment_id,
                    "image_path": generated_image.image_path,
                    "generation_mode": frame.generation_mode.value,
                    "is_reused": False,
                    "prompt": generated_image.prompt,
                    "revised_prompt": generated_image.revised_prompt
                }
                generated_keyframes.append(keyframe)
        else:
            raise Exception("å›¾åƒç”Ÿæˆå¤±è´¥")

        # å¤„ç†å¸§å¤ç”¨é€»è¾‘
        processed_keyframes = self._apply_frame_reuse_logic(generated_keyframes)

        return processed_keyframes

    def _apply_frame_reuse_logic(self, keyframes: List[Dict]) -> List[Dict]:
        """
        åº”ç”¨çµæ´»çš„å¸§å¤ç”¨é€»è¾‘
        æ ¹æ®æ®µè½é—´çš„è¿ç»­æ€§åˆ¤æ–­æ˜¯å¦å¤ç”¨é¦–å°¾å¸§
        """

        print("  ğŸ”„ åº”ç”¨çµæ´»å¸§å¤ç”¨é€»è¾‘...")

        # æŒ‰segment_idæ’åº
        keyframes.sort(key=lambda x: (x['segment_id'], x['frame_id']))

        processed = []
        segments = {}

        # æŒ‰æ®µè½åˆ†ç»„
        for kf in keyframes:
            seg_id = kf['segment_id']
            if seg_id not in segments:
                segments[seg_id] = []
            segments[seg_id].append(kf)

        # å¤„ç†æ¯ä¸ªæ®µè½
        for seg_id in sorted(segments.keys()):
            seg_frames = segments[seg_id]

            if len(seg_frames) >= 2:
                start_frame = seg_frames[0]
                end_frame = seg_frames[1]

                # é¦–å¸§ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦å¤ç”¨å‰ä¸€æ®µçš„å°¾å¸§
                if seg_id > 0 and processed:
                    prev_end_frame = processed[-1]

                    # åˆ¤æ–­è¿ç»­æ€§ï¼šæ˜¯å¦åº”è¯¥å¤ç”¨
                    should_reuse = self._should_reuse_frame(
                        prev_segment_id=seg_id-1,
                        curr_segment_id=seg_id,
                        prev_end_frame=prev_end_frame,
                        curr_start_frame=start_frame
                    )

                    if should_reuse:
                        # å¤ç”¨å‰ä¸€æ®µçš„å°¾å¸§ä½œä¸ºé¦–å¸§
                        reused_start_frame = {
                            **prev_end_frame,
                            "frame_id": f"frame_{seg_id:03d}_start_reused",
                            "segment_id": seg_id,
                            "is_reused": True,
                            "source_frame_id": prev_end_frame["frame_id"],
                            "reuse_reason": "scene_continuity"
                        }
                        processed.append(reused_start_frame)
                        print(f"    âœ… æ®µ{seg_id}å¤ç”¨å‰æ®µå°¾å¸§ï¼ˆè¿ç»­åœºæ™¯ï¼‰")
                    else:
                        # ä¸å¤ç”¨ï¼Œä½¿ç”¨ç‹¬ç«‹é¦–å¸§
                        processed.append(start_frame)
                        print(f"    ğŸ¬ æ®µ{seg_id}ä½¿ç”¨ç‹¬ç«‹é¦–å¸§ï¼ˆåœºæ™¯åˆ‡æ¢ï¼‰")
                else:
                    # ç¬¬ä¸€æ®µä½¿ç”¨åŸå§‹é¦–å¸§
                    processed.append(start_frame)

                # å°¾å¸§
                processed.append(end_frame)

        reuse_count = sum(1 for kf in processed if kf.get('is_reused', False))
        independent_count = len(processed) - reuse_count

        print(f"  ğŸ“Š å¸§ç»Ÿè®¡: {independent_count}ä¸ªç‹¬ç«‹å¸§ + {reuse_count}ä¸ªå¤ç”¨å¸§ = {len(processed)}æ€»å¸§")

        return processed

    def _should_reuse_frame(self,
                           prev_segment_id: int,
                           curr_segment_id: int,
                           prev_end_frame: Dict,
                           curr_start_frame: Dict) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥å¤ç”¨å¸§ - æ ¸å¿ƒè¿ç»­æ€§é€»è¾‘

        å†³ç­–ä¾æ®ï¼š
        1. åœºæ™¯è¿ç»­æ€§ï¼ˆåŒåœºæ™¯ã€åŒç‰©ä½“ï¼‰
        2. é•œå¤´åˆ‡æ¢æ£€æµ‹ï¼ˆåœºæ™¯å˜åŒ–ã€è§†è§’å˜åŒ–ï¼‰
        3. äº§å“ä¸€è‡´æ€§è¦æ±‚
        """

        # æå–å¸§çš„æè¿°ä¿¡æ¯
        prev_prompt = prev_end_frame.get('prompt', '').lower()
        curr_prompt = curr_start_frame.get('prompt', '').lower()

        # åˆ¤æ–­å› ç´ 1ï¼šåœºæ™¯å…ƒç´ ç›¸ä¼¼æ€§
        scene_similarity = self._calculate_scene_similarity(prev_prompt, curr_prompt)

        # åˆ¤æ–­å› ç´ 2ï¼šæ˜¯å¦æœ‰æ˜ç¡®çš„åœºæ™¯åˆ‡æ¢è¯æ±‡
        has_scene_change = self._detect_scene_change(prev_prompt, curr_prompt)

        # åˆ¤æ–­å› ç´ 3ï¼šäº§å“è¿ç»­æ€§
        has_product_continuity = self._check_product_continuity(prev_prompt, curr_prompt)

        # å†³ç­–é€»è¾‘
        if has_scene_change:
            # æ˜ç¡®åœºæ™¯åˆ‡æ¢ â†’ ä¸å¤ç”¨
            return False
        elif has_product_continuity and scene_similarity > 0.6:
            # äº§å“è¿ç»­ + åœºæ™¯ç›¸ä¼¼ â†’ å¤ç”¨
            return True
        elif scene_similarity > 0.8:
            # é«˜åº¦ç›¸ä¼¼ â†’ å¤ç”¨
            return True
        else:
            # é»˜è®¤ä¸å¤ç”¨ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
            return False

    def _calculate_scene_similarity(self, prompt1: str, prompt2: str) -> float:
        """è®¡ç®—åœºæ™¯ç›¸ä¼¼åº¦"""

        # å…³é”®åœºæ™¯å…ƒç´ 
        scene_keywords = [
            'office', 'studio', 'outdoor', 'indoor', 'kitchen', 'bedroom',
            'meeting room', 'gym', 'park', 'street', 'home', 'workplace'
        ]

        object_keywords = [
            'watch', 'phone', 'computer', 'desk', 'chair', 'product',
            'table', 'background', 'wall', 'window'
        ]

        person_keywords = [
            'person', 'man', 'woman', 'professional', 'athlete', 'user'
        ]

        all_keywords = scene_keywords + object_keywords + person_keywords

        # ç»Ÿè®¡å…±åŒå…³é”®è¯
        common_keywords = 0
        total_keywords = 0

        for keyword in all_keywords:
            in_prompt1 = keyword in prompt1
            in_prompt2 = keyword in prompt2

            if in_prompt1 or in_prompt2:
                total_keywords += 1
                if in_prompt1 and in_prompt2:
                    common_keywords += 1

        if total_keywords == 0:
            return 0.5  # æ— æ³•åˆ¤æ–­æ—¶è¿”å›ä¸­æ€§å€¼

        return common_keywords / total_keywords

    def _detect_scene_change(self, prompt1: str, prompt2: str) -> bool:
        """æ£€æµ‹æ˜ç¡®çš„åœºæ™¯åˆ‡æ¢"""

        # åœºæ™¯åˆ‡æ¢å…³é”®è¯
        scene_change_indicators = [
            'cut to', 'switch to', 'transition to', 'move to',
            'location change', 'scene change', 'new scene',
            'different location', 'another place', 'elsewhere',
            'åˆ‡æ¢åˆ°', 'è½¬åœºåˆ°', 'åœºæ™¯åˆ‡æ¢', 'ä½ç½®å˜åŒ–', 'æ¢åˆ°'
        ]

        # åœºæ™¯ç±»å‹è¯æ±‡
        location_words = [
            'office', 'home', 'outdoor', 'indoor', 'studio', 'gym',
            'meeting room', 'kitchen', 'bedroom', 'street', 'park',
            'åŠå…¬å®¤', 'å®¶é‡Œ', 'æˆ·å¤–', 'å®¤å†…', 'å·¥ä½œå®¤', 'å¥èº«æˆ¿'
        ]

        # æ£€æŸ¥æ˜ç¡®çš„åˆ‡æ¢æŒ‡ç¤º
        for indicator in scene_change_indicators:
            if indicator in prompt1 or indicator in prompt2:
                return True

        # æ£€æŸ¥å®Œå…¨ä¸åŒçš„åœºæ™¯ç±»å‹
        prompt1_locations = [word for word in location_words if word in prompt1]
        prompt2_locations = [word for word in location_words if word in prompt2]

        if prompt1_locations and prompt2_locations:
            # å¦‚æœä¸¤ä¸ªæç¤ºè¯éƒ½æœ‰åœºæ™¯è¯ä¸”å®Œå…¨ä¸åŒ
            if not set(prompt1_locations).intersection(set(prompt2_locations)):
                return True

        return False

    def _check_product_continuity(self, prompt1: str, prompt2: str) -> bool:
        """æ£€æŸ¥äº§å“è¿ç»­æ€§"""

        product_keywords = [
            'watch', 'smartwatch', 'product', 'device', 'gadget',
            'æ‰‹è¡¨', 'äº§å“', 'è®¾å¤‡'
        ]

        # æ£€æŸ¥ä¸¤ä¸ªæç¤ºè¯æ˜¯å¦éƒ½åŒ…å«äº§å“
        has_product_1 = any(keyword in prompt1 for keyword in product_keywords)
        has_product_2 = any(keyword in prompt2 for keyword in product_keywords)

        return has_product_1 and has_product_2

    async def _generate_video_clips(self, keyframes: List[Dict]) -> List[str]:
        """ç¬¬3æ­¥ï¼šç”Ÿæˆ5ç§’è§†é¢‘ç‰‡æ®µ"""

        print("\n[ç¬¬3æ­¥] ğŸ¥ ç”Ÿæˆ5ç§’è§†é¢‘ç‰‡æ®µ...")

        # ä½¿ç”¨åƒé—®å¤„ç†å™¨ç”Ÿæˆè§†é¢‘
        clips = await self.video_processor.process_storyboard_frames(
            keyframes,
            str(Path(self.config.get("work_dir", "/tmp")) / "video_clips")
        )

        print(f"  âœ… ç”Ÿæˆäº† {len(clips)} ä¸ª5ç§’è§†é¢‘ç‰‡æ®µ")

        return clips

    async def _merge_final_video(self, clips: List[str], output_path: Optional[str]) -> str:
        """ç¬¬4æ­¥ï¼šåˆå¹¶æœ€ç»ˆè§†é¢‘"""

        print("\n[ç¬¬4æ­¥] ğŸ”„ åˆå¹¶æœ€ç»ˆè§†é¢‘...")

        if not output_path:
            output_path = str(Path(self.config.get("work_dir", "/tmp")) / "final_video.mp4")

        final_video = await self.video_processor.merge_clips(clips, output_path)

        print(f"  âœ… æœ€ç»ˆè§†é¢‘: {final_video}")

        return final_video

    def _parse_text_to_segments(self, text: str) -> List[Dict]:
        """è§£ææ–‡æœ¬ä¸ºæ®µè½"""
        lines = [line.strip() for line in text.strip().split('\n') if line.strip()]
        return [{"description": line, "index": i} for i, line in enumerate(lines)]


# ä½¿ç”¨ç¤ºä¾‹å‡½æ•°
async def create_video_from_text(
    text_description: str,
    duration_seconds: int,
    product_info: Optional[Dict] = None,
    config: Optional[Dict] = None
) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä»æ–‡æœ¬ç”Ÿæˆè§†é¢‘

    å‚æ•°:
        text_description: è§†é¢‘æè¿°æ–‡æœ¬
        duration_seconds: è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
        product_info: äº§å“ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        config: é…ç½®ä¿¡æ¯

    è¿”å›:
        ç”Ÿæˆç»“æœ
    """

    if not config:
        config = {
            "qwen_api_key": "your_qwen_api_key",
            "openai_api_key": "your_openai_api_key",
            "work_dir": "/tmp/video_generation"
        }

    # åˆ›å»ºç¼–æ’å™¨
    orchestrator = VideoStoryboardOrchestrator(config)

    # åˆ›å»ºè¯·æ±‚
    request = VideoStoryboardRequest(
        text_description=text_description,
        duration_seconds=duration_seconds,
        product_info=product_info
    )

    # å¤„ç†è¯·æ±‚
    result = await orchestrator.process_video_request(request)

    return result


# æ¼”ç¤ºç”¨æ³•
async def demo():
    """æ¼”ç¤º40ç§’äº§å“è§†é¢‘ç”Ÿæˆ"""

    text = """
    å±•ç¤ºæˆ‘ä»¬çš„æ–°æ¬¾æ™ºèƒ½æ‰‹è¡¨SmartWatch Proã€‚
    å¼€åœºï¼šäº§å“360åº¦æ—‹è½¬å±•ç¤ºï¼Œçªå‡ºè®¾è®¡ç¾æ„Ÿã€‚
    èšç„¦è¡¨ç›˜ï¼šå±•ç¤ºAMOLEDå±å¹•å’Œç²¾ç¾ç•Œé¢ã€‚
    åŠŸèƒ½æ¼”ç¤ºï¼šè¿åŠ¨è¿½è¸ªï¼Œå¿ƒç‡ç›‘æµ‹ï¼ŒGPSå®šä½ã€‚
    ç”Ÿæ´»åœºæ™¯ï¼šå•†åŠ¡ä¼šè®®ä¸­æŸ¥çœ‹æ¶ˆæ¯å’Œæ—¥ç¨‹ã€‚
    è¿åŠ¨åœºæ™¯ï¼šè·‘æ­¥æ—¶ç›‘æµ‹è¿åŠ¨æ•°æ®ã€‚
    é˜²æ°´æµ‹è¯•ï¼šå±•ç¤ºIPX8é˜²æ°´èƒ½åŠ›ã€‚
    å……ç”µå±•ç¤ºï¼šæ— çº¿å……ç”µåº•åº§å’Œå¿«é€Ÿå……ç”µã€‚
    åŒ…è£…å±•ç¤ºï¼šç²¾ç¾åŒ…è£…ç›’å’Œé…ä»¶å…¨è§ˆã€‚
    """

    product_info = {
        "name": "SmartWatch Pro",
        "constraints": [
            "äº§å“é¢œè‰²å¿…é¡»ä¿æŒä¸€è‡´ï¼ˆé»‘è‰²è¡¨å¸¦+é“¶è‰²è¡¨å£³ï¼‰",
            "å¿…é¡»æ¸…æ™°æ˜¾ç¤ºå“ç‰Œlogo",
            "é¿å…å‡ºç°ç«å“æ‰‹è¡¨"
        ],
        "reference_images": ["smartwatch_product.jpg"],
        "attributes": {
            "color": "black/silver",
            "features": ["AMOLED", "heart_rate", "waterproof", "GPS"]
        }
    }

    config = {
        "qwen_api_key": "your_actual_key_here",
        "openai_api_key": "your_actual_key_here",
        "work_dir": "/tmp/smartwatch_video",
        "era_preference": "modern",
        "forbidden_elements": ["competitor", "low quality", "cheap"]
    }

    # ç”Ÿæˆ40ç§’è§†é¢‘ï¼ˆ8ä¸ª5ç§’ç‰‡æ®µï¼‰
    result = await create_video_from_text(
        text_description=text,
        duration_seconds=40,
        product_info=product_info,
        config=config
    )

    if result["success"]:
        print(f"\nğŸ‰ è§†é¢‘ç”ŸæˆæˆåŠŸï¼")
        print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {result['video_path']}")
        print(f"â±ï¸ è§†é¢‘æ—¶é•¿: {result['duration_seconds']}ç§’")
        print(f"ğŸ¬ ç‰‡æ®µæ•°é‡: {result['segments_count']} (æ¯æ®µ5ç§’)")
        print(f"ğŸ–¼ï¸ å…³é”®å¸§æ•°: {result['keyframes_count']}")
    else:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {result['error']}")

    return result


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(demo())