"""
é›†æˆè§†é¢‘ç”Ÿæˆç®¡é“ - å®Œå…¨å—èŠ‚ç‚¹çº¦æŸå’Œæ§åˆ¶
"""
from typing import Dict, List, Optional, Any
import asyncio
from dataclasses import dataclass
from pathlib import Path
import json

from storyboard_sequence_node import StoryboardSequenceNode
from vgp_optimization_node import (
    VGPOptimizationNode,
    VGPOptimizationConfig,
    GenerationMode
)
from qwen_integration import StoryboardToVideoProcessor
from image_generation_node import ImageGenerationNode


@dataclass
class VideoGenerationRequest:
    """è§†é¢‘ç”Ÿæˆè¯·æ±‚"""
    text_description: str  # è‡ªç„¶è¯­è¨€æè¿°
    duration_seconds: int  # è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
    product_info: Optional[Dict] = None  # äº§å“ä¿¡æ¯
    style_preferences: Optional[Dict] = None  # é£æ ¼åå¥½
    output_path: Optional[str] = None  # è¾“å‡ºè·¯å¾„


class IntegratedVideoPipeline:
    """é›†æˆçš„è§†é¢‘ç”Ÿæˆç®¡é“ - å®Œå…¨èŠ‚ç‚¹æ§åˆ¶"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config

        # åˆå§‹åŒ–æ‰€æœ‰èŠ‚ç‚¹
        self._init_nodes()

        # å·¥ä½œç›®å½•
        self.work_dir = Path(config.get("work_dir", "/tmp/video_pipeline"))
        self.work_dir.mkdir(parents=True, exist_ok=True)

    def _init_nodes(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ§åˆ¶èŠ‚ç‚¹"""

        # 1. VGPä¼˜åŒ–èŠ‚ç‚¹ - æ§åˆ¶ç”Ÿæˆç­–ç•¥
        vgp_config = VGPOptimizationConfig(
            product_protection_level="maximum",
            product_consistency_threshold=0.95,
            era_preference=self.config.get("era_preference", "modern"),
            forbidden_elements=self.config.get("forbidden_elements", []),
            prefer_wide_shots=True,
            enhance_lighting=True
        )
        self.vgp_node = VGPOptimizationNode(vgp_config)

        # 2. åˆ†é•œåºåˆ—èŠ‚ç‚¹ - æ§åˆ¶åˆ†é•œé€»è¾‘
        self.storyboard_node = StoryboardSequenceNode(self.config)

        # 3. å›¾åƒç”ŸæˆèŠ‚ç‚¹ - æ§åˆ¶å›¾åƒç”Ÿæˆ
        self.image_node = ImageGenerationNode(self.config)

        # 4. è§†é¢‘å¤„ç†èŠ‚ç‚¹ - æ§åˆ¶è§†é¢‘ç”Ÿæˆ
        qwen_key = self.config.get("qwen_api_key")
        if not qwen_key:
            raise ValueError("Qwen API key is required")
        self.video_processor = StoryboardToVideoProcessor(qwen_key)

    async def generate_video(self, request: VideoGenerationRequest) -> Dict[str, Any]:
        """
        å®Œæ•´çš„è§†é¢‘ç”Ÿæˆæµç¨‹ - å®Œå…¨å—èŠ‚ç‚¹æ§åˆ¶
        """

        print("\n" + "="*60)
        print("ğŸ¬ å¼€å§‹è§†é¢‘ç”Ÿæˆæµç¨‹")
        print("="*60)

        # é˜¶æ®µ1: VGPä¼˜åŒ–å’Œåˆ†é•œè§„åˆ’
        print("\n[é˜¶æ®µ1] ğŸ“‹ VGPä¼˜åŒ–å’Œåˆ†é•œè§„åˆ’...")
        storyboard_plan = await self._plan_storyboard(request)

        # é˜¶æ®µ2: ç”Ÿæˆå…³é”®å¸§å›¾åƒ
        print("\n[é˜¶æ®µ2] ğŸ¨ ç”Ÿæˆå…³é”®å¸§å›¾åƒ...")
        keyframes = await self._generate_keyframes(storyboard_plan)

        # é˜¶æ®µ3: ç”Ÿæˆè§†é¢‘ç‰‡æ®µ
        print("\n[é˜¶æ®µ3] ğŸ¥ ç”Ÿæˆè§†é¢‘ç‰‡æ®µ...")
        video_clips = await self._generate_video_clips(keyframes)

        # é˜¶æ®µ4: åˆå¹¶æœ€ç»ˆè§†é¢‘
        print("\n[é˜¶æ®µ4] ğŸ”„ åˆå¹¶æœ€ç»ˆè§†é¢‘...")
        final_video = await self._merge_final_video(video_clips, request.output_path)

        # é˜¶æ®µ5: è´¨é‡éªŒè¯
        print("\n[é˜¶æ®µ5] âœ… è´¨é‡éªŒè¯...")
        validation_result = await self._validate_output(final_video, storyboard_plan)

        print("\n" + "="*60)
        print("ğŸ‰ è§†é¢‘ç”Ÿæˆå®Œæˆï¼")
        print("="*60)

        return {
            "success": True,
            "video_path": final_video,
            "duration_seconds": request.duration_seconds,
            "segments_generated": len(video_clips),
            "keyframes_generated": len(keyframes),
            "validation": validation_result,
            "storyboard_plan": storyboard_plan
        }

    async def _plan_storyboard(self, request: VideoGenerationRequest) -> Dict:
        """é˜¶æ®µ1: åˆ†é•œè§„åˆ’ï¼ˆå—VGPèŠ‚ç‚¹æ§åˆ¶ï¼‰"""

        # è§£æç”¨æˆ·è¾“å…¥
        raw_segments = self._parse_text_to_segments(request.text_description)

        # VGPä¼˜åŒ– - è¿™é‡Œæ˜¯æ ¸å¿ƒæ§åˆ¶ç‚¹
        optimization_result = await self.vgp_node.optimize_storyboard_sequence(
            raw_segments=raw_segments,
            product_info=request.product_info,
            total_duration_ms=request.duration_seconds * 1000
        )

        # éªŒè¯ä¼˜åŒ–ç»“æœ
        self._validate_optimization(optimization_result)

        return optimization_result

    async def _generate_keyframes(self, storyboard_plan: Dict) -> List[Dict]:
        """é˜¶æ®µ2: ç”Ÿæˆå…³é”®å¸§ï¼ˆå—å›¾åƒç”ŸæˆèŠ‚ç‚¹æ§åˆ¶ï¼‰"""

        optimized_frames = storyboard_plan['optimized_frames']
        generated_keyframes = []

        # æŒ‰ç”Ÿæˆæ¨¡å¼åˆ†ç»„å¤„ç†
        frame_groups = self._group_frames_by_mode(optimized_frames)

        for mode, frames in frame_groups.items():
            print(f"  ç”Ÿæˆæ¨¡å¼ {mode}: {len(frames)} å¸§")

            if mode == GenerationMode.PRODUCT_GUIDED:
                # äº§å“å¼•å¯¼ç”Ÿæˆ - æœ€é«˜ä¼˜å…ˆçº§
                keyframes = await self._generate_product_guided_frames(frames)

            elif mode == GenerationMode.IMAGE_TO_IMAGE:
                # å›¾ç”Ÿå›¾ - ä¿æŒè¿ç»­æ€§
                keyframes = await self._generate_img2img_frames(frames)

            else:
                # æ–‡ç”Ÿå›¾ - ç‹¬ç«‹ç”Ÿæˆ
                keyframes = await self._generate_txt2img_frames(frames)

            generated_keyframes.extend(keyframes)

        # å¤„ç†å¸§å¤ç”¨
        generated_keyframes = self._process_frame_reuse(generated_keyframes)

        return generated_keyframes

    async def _generate_product_guided_frames(self, frames: List) -> List[Dict]:
        """äº§å“å¼•å¯¼çš„å›¾åƒç”Ÿæˆï¼ˆæœ€ä¸¥æ ¼æ§åˆ¶ï¼‰"""

        keyframes = []

        for frame in frames:
            # æå–äº§å“å‚è€ƒå›¾
            product_ref = frame.reference_product_image

            if not product_ref:
                print(f"  âš ï¸ è­¦å‘Šï¼šäº§å“å¸§ {frame.frame_id} ç¼ºå°‘å‚è€ƒå›¾")

            # ç”Ÿæˆå›¾åƒ - ä½¿ç”¨äº§å“çº¦æŸ
            image_result = await self.image_node.generate_single_image(
                prompt=frame.prompt_optimization['base_description'],
                style="product_photography",
                quality="high",
                provider="dalle"  # æˆ–å…¶ä»–æ”¯æŒäº§å“ä¸€è‡´æ€§çš„æä¾›å•†
            )

            if image_result:
                keyframes.append({
                    "frame_id": frame.frame_id,
                    "segment_id": frame.segment_id,
                    "image_path": image_result.image_path,
                    "generation_mode": "product_guided",
                    "is_reused": False
                })

        return keyframes

    async def _generate_img2img_frames(self, frames: List) -> List[Dict]:
        """å›¾ç”Ÿå›¾ç”Ÿæˆï¼ˆä¿æŒè¿ç»­æ€§ï¼‰"""

        keyframes = []

        for frame in frames:
            # è·å–å‚è€ƒå¸§
            ref_frame_id = frame.reference_frame_id
            ref_image = self._get_reference_image(ref_frame_id, keyframes)

            # åŸºäºå‚è€ƒå›¾ç”Ÿæˆ
            # è¿™é‡Œåº”è¯¥è°ƒç”¨æ”¯æŒimg2imgçš„API
            image_result = await self._generate_with_reference(
                frame,
                ref_image
            )

            if image_result:
                keyframes.append({
                    "frame_id": frame.frame_id,
                    "segment_id": frame.segment_id,
                    "image_path": image_result,
                    "generation_mode": "img2img",
                    "is_reused": False
                })

        return keyframes

    async def _generate_txt2img_frames(self, frames: List) -> List[Dict]:
        """æ–‡ç”Ÿå›¾ç”Ÿæˆï¼ˆç‹¬ç«‹ç”Ÿæˆï¼‰"""

        keyframes = []

        for frame in frames:
            image_result = await self.image_node.generate_single_image(
                prompt=frame.prompt_optimization['base_description'],
                style=frame.prompt_optimization.get('style_tags', ['modern'])[0],
                quality="high"
            )

            if image_result:
                keyframes.append({
                    "frame_id": frame.frame_id,
                    "segment_id": frame.segment_id,
                    "image_path": image_result.image_path,
                    "generation_mode": "txt2img",
                    "is_reused": False
                })

        return keyframes

    def _process_frame_reuse(self, keyframes: List[Dict]) -> List[Dict]:
        """å¤„ç†å¸§å¤ç”¨é€»è¾‘"""

        processed = []

        for i in range(len(keyframes)):
            frame = keyframes[i]

            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤ç”¨ä¸ºä¸‹ä¸€æ®µçš„é¦–å¸§
            if i > 0 and i % 2 == 0:  # æ¯ä¸ªå°¾å¸§ä½ç½®
                # åˆ›å»ºå¤ç”¨å¸§
                reused_frame = {
                    "frame_id": frame["frame_id"],
                    "segment_id": frame["segment_id"] + 1,
                    "image_path": frame["image_path"],
                    "generation_mode": frame["generation_mode"],
                    "is_reused": True,
                    "source_frame_id": frame["frame_id"]
                }
                processed.append(reused_frame)

            processed.append(frame)

        return processed

    async def _generate_video_clips(self, keyframes: List[Dict]) -> List[str]:
        """é˜¶æ®µ3: ç”Ÿæˆè§†é¢‘ç‰‡æ®µï¼ˆå—åƒé—®APIçº¦æŸï¼‰"""

        # ä½¿ç”¨è§†é¢‘å¤„ç†å™¨ç”Ÿæˆ5ç§’ç‰‡æ®µ
        clips = await self.video_processor.process_storyboard_frames(
            keyframes,
            str(self.work_dir / "clips")
        )

        print(f"  ç”Ÿæˆäº† {len(clips)} ä¸ª5ç§’è§†é¢‘ç‰‡æ®µ")

        return clips

    async def _merge_final_video(self, clips: List[str], output_path: Optional[str]) -> str:
        """é˜¶æ®µ4: åˆå¹¶æœ€ç»ˆè§†é¢‘"""

        if not output_path:
            output_path = str(self.work_dir / "output.mp4")

        final_video = await self.video_processor.merge_clips(
            clips,
            output_path
        )

        print(f"  æœ€ç»ˆè§†é¢‘: {final_video}")

        return final_video

    async def _validate_output(self, video_path: str, storyboard_plan: Dict) -> Dict:
        """é˜¶æ®µ5: è´¨é‡éªŒè¯"""

        validation = {
            "video_exists": Path(video_path).exists(),
            "expected_duration": True,  # åº”è¯¥éªŒè¯å®é™…æ—¶é•¿
            "product_consistency": None,
            "scene_continuity": None,
            "quality_score": 0.0
        }

        # éªŒè¯äº§å“ä¸€è‡´æ€§
        if storyboard_plan.get('optimization_report'):
            report = storyboard_plan['optimization_report']
            validation['quality_score'] = report['continuity_analysis']['average_score']

        # è¿™é‡Œå¯ä»¥è°ƒç”¨VLæ¨¡å‹è¿›è¡Œè§†è§‰éªŒè¯
        # validation['visual_check'] = await self._visual_validation(video_path)

        return validation

    # è¾…åŠ©æ–¹æ³•

    def _parse_text_to_segments(self, text: str) -> List[Dict]:
        """è§£ææ–‡æœ¬ä¸ºæ®µè½"""
        lines = [l.strip() for l in text.strip().split('\n') if l.strip()]
        return [{"description": line, "index": i} for i, line in enumerate(lines)]

    def _group_frames_by_mode(self, frames: List) -> Dict[str, List]:
        """æŒ‰ç”Ÿæˆæ¨¡å¼åˆ†ç»„"""
        groups = {}
        for frame in frames:
            mode = frame.generation_mode
            if mode not in groups:
                groups[mode] = []
            groups[mode].append(frame)
        return groups

    def _validate_optimization(self, optimization_result: Dict):
        """éªŒè¯ä¼˜åŒ–ç»“æœ"""
        report = optimization_result.get('optimization_report', {})

        # æ£€æŸ¥è­¦å‘Š
        if report.get('warnings'):
            for warning in report['warnings']:
                print(f"  âš ï¸ {warning}")

        # æ£€æŸ¥è¿ç»­æ€§åˆ†æ•°
        avg_score = report.get('continuity_analysis', {}).get('average_score', 0)
        if avg_score < 0.5:
            print(f"  âš ï¸ è¿ç»­æ€§åˆ†æ•°è¾ƒä½: {avg_score:.2f}")

    def _get_reference_image(self, frame_id: str, existing_frames: List) -> Optional[str]:
        """è·å–å‚è€ƒå›¾åƒ"""
        for frame in existing_frames:
            if frame['frame_id'] == frame_id:
                return frame['image_path']
        return None

    async def _generate_with_reference(self, frame, ref_image: str) -> str:
        """åŸºäºå‚è€ƒå›¾ç”Ÿæˆ"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„img2img API
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿè·¯å¾„
        return str(self.work_dir / f"{frame.frame_id}.png")


# ä½¿ç”¨ç¤ºä¾‹
async def demo():
    """å®Œæ•´æµç¨‹æ¼”ç¤º"""

    # é…ç½®
    config = {
        "qwen_api_key": "your_qwen_key",
        "openai_api_key": "your_openai_key",
        "work_dir": "/tmp/video_pipeline",
        "era_preference": "modern",
        "forbidden_elements": ["competitor", "low quality"]
    }

    # åˆå§‹åŒ–ç®¡é“
    pipeline = IntegratedVideoPipeline(config)

    # åˆ›å»ºè¯·æ±‚
    request = VideoGenerationRequest(
        text_description="""
        å±•ç¤ºæˆ‘ä»¬çš„æ–°æ¬¾æ™ºèƒ½æ‰‹è¡¨SmartWatch Proã€‚
        é¦–å…ˆå±•ç¤ºäº§å“çš„æ•´ä½“å¤–è§‚ï¼Œ360åº¦æ—‹è½¬ã€‚
        ç„¶åèšç„¦è¡¨ç›˜ï¼Œå±•ç¤ºæ—¶é—´å’Œæ™ºèƒ½ç•Œé¢ã€‚
        æ¥ç€æ¼”ç¤ºè¿åŠ¨è¿½è¸ªåŠŸèƒ½ï¼Œæ˜¾ç¤ºå¿ƒç‡ç›‘æµ‹ã€‚
        å±•ç¤ºé˜²æ°´åŠŸèƒ½ï¼Œæ°´ä¸‹ä½¿ç”¨åœºæ™¯ã€‚
        æœ€åå±•ç¤ºå……ç”µåº•åº§å’Œç²¾ç¾åŒ…è£…ã€‚
        """,
        duration_seconds=30,
        product_info={
            "name": "SmartWatch Pro",
            "constraints": ["ä¿æŒäº§å“é¢œè‰²ä¸€è‡´", "å¿…é¡»æ˜¾ç¤ºå“ç‰Œlogo"],
            "reference_images": ["product_ref.jpg"]
        },
        output_path="/tmp/final_video.mp4"
    )

    # ç”Ÿæˆè§†é¢‘
    result = await pipeline.generate_video(request)

    print("\nğŸ“Š ç”Ÿæˆç»“æœ:")
    print(f"è§†é¢‘è·¯å¾„: {result['video_path']}")
    print(f"æ—¶é•¿: {result['duration_seconds']}ç§’")
    print(f"ç‰‡æ®µæ•°: {result['segments_generated']}")
    print(f"å…³é”®å¸§æ•°: {result['keyframes_generated']}")
    print(f"è´¨é‡åˆ†æ•°: {result['validation']['quality_score']:.2f}")

    return result


if __name__ == "__main__":
    asyncio.run(demo())