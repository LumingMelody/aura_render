# nodes/subtitle_node.py

from video_generate_protocol import BaseNode
from typing import Dict, List, Any
import re
import json
from datetime import datetime
from llm import QwenLLM
import asyncio
from materials_supplies import TTSRequest, TTSResponse
from materials_supplies.matcher.tts_matcher import match_tts

# å­—å¹•æ ·å¼é¢„è®¾
SUBTITLE_STYLES = {
    "default": {
        "font": "Microsoft YaHei, sans-serif",
        "font_size": 24,
        "color": "#FFFFFF",
        "background": "rgba(0, 0, 0, 0.6)",
        "stroke": "#000000",
        "stroke_width": 2,
        "align": "center",
        "position": "bottom-center",
        "padding": "10px 16px",
        "border_radius": "6px",
        "shadow": "0 1px 4px rgba(0,0,0,0.5)"
    },
    "minimal": {
        "font": "Arial, sans-serif",
        "font_size": 20,
        "color": "#FFFFFF",
        "background": "none",
        "stroke": "#000000",
        "stroke_width": 1.5,
        "align": "center",
        "position": "bottom-center",
        "padding": "0",
        "border_radius": "0",
        "shadow": "0 1px 3px rgba(0,0,0,0.4)"
    },
    "karaoke": {
        "font": "Comic Sans MS, cursive",
        "font_size": 28,
        "color": "#FFFF00",
        "background": "rgba(0, 0, 0, 0.5)",
        "stroke": "#FF0000",
        "stroke_width": 2,
        "align": "center",
        "position": "bottom-center",
        "padding": "12px 20px",
        "border_radius": "8px",
        "shadow": "0 2px 6px rgba(0,0,0,0.6)"
    }
}
class SubtitleNode(BaseNode):
    required_inputs = [
        {
            "name": "shot_blocks_id",
            "label": "é•œå¤´åºåˆ—",
            "type": List[Dict],
            "required": True,
            "desc": "åŒ…å«è§†è§‰æè¿°ã€å­—å¹•æ–‡æœ¬å’Œå¯é€‰ frame_image çš„é•œå¤´åˆ—è¡¨",
            "field_type": "json"
        },
        {
            "name": "video_width",
            "label": "è§†é¢‘å®½åº¦",
            "type": int,
            "required": False,
            "default": 1920,
            "desc": "ç”¨äºå­—å¹•æ¢è¡Œè®¡ç®—",
            "field_type": "number"
        },
        {
            "name": "video_height",
            "label": "è§†é¢‘é«˜åº¦",
            "type": int,
            "required": False,
            "default": 1080,
            "desc": "ç”¨äºå®šä½",
            "field_type": "number"
        }
    ]

    output_schema=[
        {
            "name": "subtitle_sequence_id",
            "label": "å­—å¹•è½¨é“",
            "type": list,
            "required": True,
            "desc": "å­—å¹•è½¨é“ï¼ŒåŒ…å«å­—å¹•ç‰‡æ®µåˆ—è¡¨ï¼Œå¦‚ [{'start_time': 0.0, 'end_time': 10.0, 'text': 'æ¬¢è¿æ¥åˆ°æœºå™¨å­¦ä¹ ä¸–ç•Œï¼'}]",
            "field_type": "json"
        },
        {
            "name": "audio_tracks_id",
            "label": "è¯­éŸ³éŸ³è½¨",
            "type": float,
            "required": False,
            "desc": "ç”Ÿæˆçš„ TTS è¯­éŸ³éŸ³è½¨åˆ—è¡¨ï¼Œå¦‚ [{'start': 0.0, 'end': 10.0, 'url': 'https://...'}]",
            "field_type": "text"
        }
    ]

    system_parameters = {
        "max_chars_per_line": 20,
        "max_lines": 2,
        "style_preset": "default",
        "auto_align": True,
        "min_duration": 1.0,
        "word_wrap": True,
        "language": "zh-CN",
        "tts_voice": "zh-CN-XiaoyiNeural",  # é»˜è®¤éŸ³è‰²
        "tts_speed": 1.1
    }

    def __init__(self, node_id: str, name: str = "è§†é¢‘ä¸‹æ–¹çš„å­—å¹•"):
        super().__init__(node_id=node_id, node_type="subtitle", name=name)
        self.qwen = QwenLLM()
    
    
        
    async def generate(self, context: Dict[str, Any]) -> Dict[str, Any]:
        self.validate_context(context)

        shots: List[Dict] = context["shot_blocks_id"]
        video_width = context.get("video_width", 1920)
        video_height = context.get("video_height", 1080)

        # === å†³ç­– 1: æ˜¯å¦ç”Ÿæˆ TTSï¼Ÿä»¥åŠä½¿ç”¨ä»€ä¹ˆéŸ³è‰²ï¼Ÿ===
        should_tts, voice_description = self._should_generate_tts(shots)

        # === å†³ç­– 2: å­—å¹•é¢œè‰² + æè¾¹é¢œè‰²ï¼ˆä¸‰çº§ç­–ç•¥ï¼‰===
        color_result = self._decide_subtitle_colors(shots)
        final_color = color_result["text_color"]
        final_stroke = color_result["stroke_color"]

        # æ„å»ºæ ·å¼é…ç½®
        style_config = SUBTITLE_STYLES["default"].copy()
        style_config["color"] = final_color
        style_config["stroke"] = final_stroke

        # ç”Ÿæˆå­—å¹•ç‰‡æ®µ
        subtitle_clips = []
        current_time = 0.0

        for idx, shot in enumerate(shots):
            caption = shot.get("caption", "").strip()
            duration = shot.get("duration", 2.0)

            if not caption:
                current_time += duration
                continue

            start = current_time
            end = current_time + duration
            current_time = end

            if end - start < self.system_parameters["min_duration"]:
                continue

            # æ¢è¡Œå¤„ç†
            wrapped_lines = self._wrap_text(caption, video_width)
            if not wrapped_lines:
                continue

            clip = {
                "start": start,
                "end": end,
                "text": "\n".join(wrapped_lines),
                "lines": wrapped_lines,
                "duration": duration,
                "position": self._get_position(video_width, video_height),
                "style": "default",
                "metadata": {
                    "source": "visual_caption",
                    "shot_type": shot["shot_type"],
                    "pacing": shot["pacing"],
                    "should_tts": should_tts,
                    "text_color": final_color,
                    "stroke_color": final_stroke
                }
            }
            subtitle_clips.append(clip)

        # æ„å»ºå­—å¹•è½¨é“
        subtitle_track = {
            "track_name": "subtitle",
            "track_type": "text",
            "format": "srt_compatible",
            "style": "default",
            "style_config": style_config,
            "clips": subtitle_clips,
            "timing": {
                "first_start": subtitle_clips[0]["start"] if subtitle_clips else 0,
                "last_end": subtitle_clips[-1]["end"] if subtitle_clips else 0
            }
        }

        # === ç”Ÿæˆ TTS éŸ³è½¨ï¼ˆå¦‚æœéœ€è¦ï¼‰===
        # audio_tracks = []
        # if should_tts:
        #     print(f"ğŸ”Š å¼€å§‹ç”Ÿæˆ TTS è¯­éŸ³ï¼Œä½¿ç”¨éŸ³è‰²é£æ ¼ï¼š{voice_description}")
        #     tts_requests = []
        #     for clip in subtitle_clips:
        #         tts_requests.append(TTSRequest(
        #             text=clip["text"],
        #             voice=voice_description,  # âœ… ä½¿ç”¨éŸ³è‰²æè¿°ï¼ˆå¦‚â€œæ¸©æš–å¥³å£°â€ï¼‰
        #             speed=self.system_parameters["tts_speed"],
        #             duration=clip["duration"]
        #         ))

        #     # å¹¶è¡Œè°ƒç”¨ TTS
        #     tts_responses = await asyncio.gather(
        #         *[match_tts(req) for req in tts_requests],
        #         return_exceptions=True
        #     )

        #     # æ„å»ºéŸ³é¢‘è½¨é“
        #     for idx, response_list in enumerate(tts_responses):
        #         if isinstance(response_list, Exception):
        #             print(f"[TTS Error] Clip {idx}: {response_list}")
        #             continue
        #         response = response_list[0]
        #         audio_clip = {
        #             "start": subtitle_clips[idx]["start"],
        #             "end": response.duration + subtitle_clips[idx]["start"],
        #             "url": response.url,
        #             "voice": response.voice,  # å®é™…ä½¿ç”¨çš„éŸ³è‰²ï¼ˆå¯èƒ½ä¸æè¿°ç•¥æœ‰ä¸åŒï¼‰
        #             "duration": response.duration,
        #             "text": response.text,
        #             "type": "speech",
        #             "voice_preference": voice_description  # è®°å½•åŸå§‹åå¥½
        #         }
        #         audio_tracks.append(audio_clip)


        # === ç”Ÿæˆ TTS éŸ³è½¨ï¼ˆå¦‚æœéœ€è¦ï¼‰===
        tts_track = None
        if should_tts:
            print(f"ğŸ”Š å¼€å§‹ç”Ÿæˆ TTS è¯­éŸ³ï¼Œä½¿ç”¨éŸ³è‰²é£æ ¼ï¼š{voice_description}")
            tts_requests = []
            for clip in subtitle_clips:
                tts_requests.append(TTSRequest(
                    text=clip["text"],
                    voice=voice_description,
                    speed=self.system_parameters["tts_speed"],
                    duration=clip["duration"]
                ))

            # å¹¶è¡Œè°ƒç”¨ TTS
            tts_responses = await asyncio.gather(
                *[match_tts(req) for req in tts_requests],
                return_exceptions=True
            )

            # æ„å»º clips åˆ—è¡¨
            tts_clips = []
            total_duration = 0.0

            for idx, response_list in enumerate(tts_responses):
                if isinstance(response_list, Exception):
                    print(f"[TTS Error] Clip {idx}: {response_list}")
                    continue

                response = response_list[0]
                clip_info = subtitle_clips[idx]

                # å®é™…è¯­éŸ³æ—¶é•¿ï¼ˆå¯ç”¨äºå¯¹é½ï¼‰
                actual_duration = response.duration
                expected_duration = clip_info["duration"]

                # å¦‚æœ TTS è¿”å›çš„è¯­éŸ³å¤ªçŸ­ï¼Œå¯åšæ‹‰ä¼¸æˆ–ä¿ç•™åŸæ—¶é•¿ï¼ˆè¿™é‡Œæˆ‘ä»¬ä»¥å®é™…è¯­éŸ³ä¸ºå‡†ï¼‰
                # ä½†ä¸ºäº†å…¼å®¹ï¼Œæˆ‘ä»¬ä¿ç•™åŸå§‹æ—¶é—´è½´çš„ start/end
                start_time = clip_info["start"]
                end_time = start_time + expected_duration  # ä¿æŒä¸å­—å¹•åŒæ­¥

                tts_clip = {
                    "clip_id": f"tts_{idx}",
                    "start": start_time,
                    "end": end_time,
                    "duration": expected_duration,  # ä¿æŒä¸å­—å¹•ä¸€è‡´
                    "text": response.text,
                    "voice": response.voice,  # å®é™…ä½¿ç”¨çš„éŸ³è‰²ï¼ˆå¦‚ zh-CN-XiaoyiNeuralï¼‰
                    "voice_preference": voice_description,  # åŸå§‹åå¥½ï¼ˆå¦‚â€œå¹´è½»æ´»åŠ›å¥³å£°â€ï¼‰
                    "audio": {
                        "url": response.url,
                        "in_point": 0.0,
                        "out_point": actual_duration,
                        "duration": actual_duration
                    },
                    "volume_db": -10.0,  # é»˜è®¤éŸ³é‡
                    "pan": 0.0           # å±…ä¸­
                }
                tts_clips.append(tts_clip)
                total_duration = max(total_duration, end_time)

            # æ„å»ºå®Œæ•´éŸ³è½¨
            tts_track = {
                "track_id": "tts_track",
                "track_name": "è¯­éŸ³",
                "track_type": "speech",
                "total_duration": total_duration,
                "clips": tts_clips
            }
        # === è¿”å›å®Œæ•´ç»“æœ ===

        result = {
            "subtitle_sequence_id": subtitle_track,
            # "audio_tracks_id": audio_tracks if audio_tracks else None,#è€æ ¼å¼
            "tts_track_id": tts_track,  # âœ… ä½¿ç”¨æ–°æ ¼å¼
            # "tts_required": should_tts,
            # "tts_voice_preference": voice_description  # âœ… æ–°å¢ï¼šè®°å½•æ¨èéŸ³è‰²
        }

        return result

    def _should_generate_tts(self, shots: List[Dict]) -> tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦ç”Ÿæˆ TTSï¼Œå¹¶æ¨èåˆé€‚çš„éŸ³è‰²æè¿°ï¼ˆå¦‚â€œæ¸©æš–å¥³å£°â€ã€â€œæ²‰ç¨³ç”·å£°â€ï¼‰
        è¿”å›: (should_generate: bool, voice_description: str)
        """
        prompt = """
        è¯·åˆ†æä»¥ä¸‹æ•™å­¦è§†é¢‘çš„é•œå¤´å†…å®¹ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦ä¸ºå­—å¹•ç”Ÿæˆè¯­éŸ³ï¼ˆTTSï¼‰ï¼Œå¹¶æ¨èåˆé€‚çš„éŸ³è‰²é£æ ¼ã€‚

        è€ƒè™‘å› ç´ ï¼š
        - è§†é¢‘æ˜¯å¦ä»¥è®²è§£ã€å¼•å¯¼ä¸ºä¸»ï¼Ÿ
        - æ˜¯å¦é€‚åˆç”¨è¯­éŸ³å¢å¼ºæƒ…æ„Ÿæˆ–æ•™å­¦æ•ˆæœï¼Ÿ
        - ç›®æ ‡å—ä¼—æ˜¯å­¦ç”Ÿã€æˆäººè¿˜æ˜¯å„¿ç«¥ï¼Ÿ
        - æ•´ä½“æ°›å›´æ˜¯æ¸©é¦¨ã€æ¿€åŠ±ã€ä¸“ä¸šè¿˜æ˜¯è½»æ¾ï¼Ÿ

        è¿”å› JSON æ ¼å¼ï¼š
        {{
            "should_generate_tts": true/false,
            "voice_preference": "å¦‚ï¼šæ¸©æš–å¥³å£°ã€æ²‰ç¨³ç”·å£°ã€å¹´è½»æ´»åŠ›å¥³å£°ã€ä¸“ä¸šç”·å£°ã€äº²åˆ‡ç«¥å£°ç­‰",
            "reason": "ç®€è¦è¯´æ˜"
        }}

        é•œå¤´å†…å®¹ï¼š
        {}
        """.format(json.dumps(shots, ensure_ascii=False, indent=2))

        result = self.qwen.generate(
            prompt=prompt,
            parse_json=True,
            json_schema={
                "type": "object",
                "properties": {
                    "should_generate_tts": {"type": "boolean"},
                    "voice_preference": {"type": "string"},
                    "reason": {"type": "string"}
                },
                "required": ["should_generate_tts", "voice_preference"]
            }
        )

        if result and isinstance(result, dict):
            should_tts = result.get("should_generate_tts", False)
            voice_desc = result.get("voice_preference", "æ ‡å‡†å¥³å£°")
            reason = result.get("reason", "")
            print(f"[TTSå†³ç­–] {reason} | éŸ³è‰²å»ºè®®: {voice_desc}")
            return should_tts, voice_desc

        return False, "æ ‡å‡†å¥³å£°"
    def _decide_subtitle_colors(self, shots: List[Dict]) -> Dict[str, str]:
        """ä¸‰çº§ç­–ç•¥å†³å®šå­—å¹•é¢œè‰²ä¸æè¾¹"""
        images = [shot["frame_image"] for shot in shots if "frame_image" in shot]
        if images:
            prompt = "åˆ†æè¿™äº›æ•™å­¦è§†é¢‘å¸§çš„ç”»é¢æ°›å›´ï¼Œå¹¶å»ºè®®å­—å¹•æ–‡æœ¬é¢œè‰²å’Œæè¾¹é¢œè‰²ï¼ˆåå…­è¿›åˆ¶ï¼‰ã€‚è¦æ±‚é«˜å¯è¯»æ€§ã€‚"
            result = self.qwen.generate(
                prompt=prompt,
                images=images,
                parse_json=True,
                json_schema={
                    "type": "object",
                    "properties": {
                        "text_color": {"type": "string", "format": "color"},
                        "stroke_color": {"type": "string", "format": "color"}
                    },
                    "required": ["text_color", "stroke_color"]
                }
            )
            if result and isinstance(result, dict):
                print(f"[é¢œè‰²å†³ç­–] VL â†’ æ–‡å­—: {result['text_color']}, æè¾¹: {result['stroke_color']}")
                return {
                    "text_color": result["text_color"],
                    "stroke_color": result["stroke_color"]
                }

        descriptions = [shot["visual_description"] for shot in shots if "visual_description" in shot]
        if descriptions:
            prompt = f"""
            æ ¹æ®ä»¥ä¸‹è§†è§‰æè¿°åˆ¤æ–­æ•™å­¦è§†é¢‘çš„æ•´ä½“æ°›å›´ï¼Œæ¨èé€‚åˆçš„å­—å¹•é¢œè‰²å’Œæè¾¹é¢œè‰²ï¼ˆåå…­è¿›åˆ¶ï¼‰ï¼š
            {json.dumps(descriptions, ensure_ascii=False, indent=2)}
            è¦æ±‚ï¼šé¢œè‰²éœ€ä¸å¸¸è§èƒŒæ™¯æœ‰å¯¹æ¯”ï¼Œé€‚åˆé•¿æœŸé˜…è¯»ã€‚
            è¿”å› JSONï¼š{{"text_color": str, "stroke_color": str}}
            """
            result = self.qwen.generate(
                prompt=prompt,
                parse_json=True,
                json_schema={
                    "type": "object",
                    "properties": {
                        "text_color": {"type": "string", "format": "color"},
                        "stroke_color": {"type": "string", "format": "color"}
                    },
                    "required": ["text_color", "stroke_color"]
                }
            )
            if result and isinstance(result, dict):
                print(f"[é¢œè‰²å†³ç­–] LLM â†’ æ–‡å­—: {result['text_color']}, æè¾¹: {result['stroke_color']}")
                return {
                    "text_color": result["text_color"],
                    "stroke_color": result["stroke_color"]
                }

        print("[é¢œè‰²å†³ç­–] ä½¿ç”¨é»˜è®¤é¢œè‰²")
        return {
            "text_color": "#FFFFFF",
            "stroke_color": "#000000"
        }

    def _wrap_text(self, text: str, video_width: int) -> List[str]:
        max_chars = self.system_parameters["max_chars_per_line"]
        max_lines = self.system_parameters["max_lines"]
        if len(text) <= max_chars:
            return [text]
        lines = []
        words = text.split()
        current = ""
        for word in words:
            if len(current + word) <= max_chars:
                current += (word + " ")
            else:
                if current:
                    lines.append(current.strip())
                    if len(lines) >= max_lines:
                        break
                    current = word + " "
                else:
                    current = word + " "
        if current and len(lines) < max_lines:
            lines.append(current.strip())
        return lines

    def _get_position(self, width: int, height: int) -> Dict[str, Any]:
        margin = 40
        return {
            "x": "50%",
            "y": height - margin,
            "anchor": "bottom",
            "align": "center"
        }

    async def regenerate(self, context: Dict[str, Any], user_intent: Dict[str, Any]) -> Dict[str, Any]:
        result = await self.generate(context)
        override = user_intent.get("subtitle_override")
        if not override:
            return result

        track = result["subtitle_sequence"]

        if "change_color" in override:
            new_color = override["change_color"]
            new_stroke = override.get("stroke_color", "#000000")
            track["style_config"]["color"] = new_color
            track["style_config"]["stroke"] = new_stroke
            for clip in track["clips"]:
                clip["metadata"]["text_color"] = new_color
                clip["metadata"]["stroke_color"] = new_stroke

        if "add_manual_caption" in override:
            manual = override["add_manual_caption"]
            clip = {
                "start": manual["start"],
                "end": manual["end"],
                "text": manual["text"],
                "position": self._get_position(context.get("video_width", 1920), context.get("video_height", 1080)),
                "style": "default",
                "metadata": {
                    "source": "manual",
                    "text_color": track["style_config"]["color"],
                    "stroke_color": track["style_config"]["stroke"]
                }
            }
            track["clips"].append(clip)
            track["clips"] = sorted(track["clips"], key=lambda x: x["start"])

        return result

