# sfx_integration_node.py

from typing import Dict, List, Optional,Any
import asyncio
import json

 # å‡è®¾ä½ ä½¿ç”¨ Qwen-Agent æ¡†æ¶
from materials_supplies import match_sfx, SFXRequest  # æ›¿æ¢ä¸ºä½ çš„çœŸå®åŒ¹é…æ¨¡å—
from llm.qwen import QwenLLM  # å‡è®¾è¿™æ˜¯ä½ å°è£…å¥½çš„ Qwen è°ƒç”¨æ¨¡å—
from video_generate_protocol import BaseNode  # å‡è®¾è¿™æ˜¯ä½ çš„è§†é¢‘ç”Ÿæˆåè®®åŸºç±»


# é…ç½®å¸¸é‡
AVOIDANCE_MARGIN_DB = 12.0  # äººå£°é‡å æ—¶é™ 12dB
KEYWORD_ALIGNMENT_WINDOW = 0.3
MIN_GAP_FOR_INSERT = 0.5
SFX_FADE_DURATION = 0.1


class SFXIntegrationNode(BaseNode):

    required_inputs = [
            {   "name":"shot_blocks_id",
                "label": "é•œå¤´åºåˆ—",
                "type": List[Dict],
                "desc": "è§†é¢‘åˆ†é•œåºåˆ—ï¼ŒåŒ…å«é•œå¤´ç±»å‹ã€æ—¶é•¿ã€æè¿°ã€å­—å¹•ç­‰",
                "required": False,
                "default": [],
                "fields": {
                    "shot_type": {"type": "str", "desc": "é•œå¤´ç±»å‹ï¼Œå¦‚ close-up, wide"},
                    "duration": {"type": "float", "desc": "é•œå¤´æ—¶é•¿ï¼ˆç§’ï¼‰"},
                    "visual_description": {"type": "str", "desc": "ç”»é¢å†…å®¹æè¿°"},
                    "pacing": {"type": "str", "desc": "èŠ‚å¥ï¼šå¿«/ä¸­/æ…¢"},
                    "caption": {"type": "str", "desc": "å¯é€‰ï¼šå­—å¹•æ–‡æœ¬"}
                }
            },
            {   "name":"voice_track_id",
                "label": "äººå£°è½¨é“ä¿¡æ¯",
                "type": "Dict",
                "desc": "äººå£°è½¨é“ä¿¡æ¯ï¼ŒåŒ…å« ASR æ–‡æœ¬ä¸æ—¶é—´æˆ³",
                "required": False,
                "default": {},
                "fields": {
                    "clips": {
                        "type": "List[Dict]",
                        "desc": "è¯­éŸ³ç‰‡æ®µåˆ—è¡¨",
                        "item_fields": {
                            "start": {"type": "float", "desc": "èµ·å§‹æ—¶é—´ï¼ˆç§’ï¼‰"},
                            "duration": {"type": "float", "desc": "æŒç»­æ—¶é—´"},
                            "text": {"type": "str", "desc": "è½¬å½•æ–‡æœ¬"},
                            "words": {
                                "type": "List[Dict]",
                                "desc": "å¯é€‰ï¼šå­—/è¯çº§æ—¶é—´æˆ³",
                                "item_fields": {
                                    "word": {"type": "str", "desc": "å•ä¸ªå­—æˆ–è¯"},
                                    "start": {"type": "float", "desc": "èµ·å§‹æ—¶é—´"},
                                    "end": {"type": "float", "desc": "ç»“æŸæ—¶é—´"}
                                }
                            }
                        }
                    }
                }
            },
            {   "name":"existing_sfx",
                "label": "å·²æœ‰éŸ³æ•ˆè½¨é“",
                "type": "Dict",
                "desc": "å¯é€‰ï¼šå·²æœ‰éŸ³æ•ˆè½¨é“ï¼Œç”¨äº regenerate æ—¶å‚è€ƒ",
                "required": False,
                "default": None
            },
            {   "name":"system_parameters",
                "label": "ç³»ç»Ÿå‚æ•°",
                "type": "Dict",
                "desc": "ç³»ç»Ÿå‚æ•°ï¼Œå¦‚æ˜¯å¦é¿è®©äººå£°ã€æ··å“ç­‰",
                "required": False,
                "default": {},
                "example": {
                    "avoid_voice": True,
                    "reverb_apply": True,
                    "max_density_per_minute": 6
                }
            },
        ]
    
    output_schema=[
        {
            "name": "sfx_track_id",
            "label": "éŸ³æ•ˆç‰‡æ®µåˆ—è¡¨",
            "type": list,
            "required": True,
            "desc": "éŸ³æ•ˆç‰‡æ®µåˆ—è¡¨â€åŒ…å«æ¯ä¸ªç‰‡æ®µçš„å¼€å§‹æ—¶é—´å’Œç»“æŸæ—¶é—´ï¼Œä»¥åŠéŸ³æ•ˆçš„æè¿°ä¿¡æ¯",
            "field_type": "json"
        },
    ]
    
    def __init__(self,node_id: str, name: str = "éŸ³æ•ˆæ·»åŠ ",system_parameters: Dict = None):
        self.node_id = node_id
        self.name = name
        # self.required_inputs =  # è¾“å…¥é•œå¤´åˆ—è¡¨
        self.qwen = QwenLLM()
        self.system_parameters = system_parameters or {}
        self.last_sfx_time = 0.0  # ç”¨äºå¯†åº¦æ§åˆ¶

    def _is_overlapping(self, start1: float, end1: float, start2: float, end2: float) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªæ—¶é—´æ®µæ˜¯å¦é‡å """
        return max(start1, start2) < min(end1, end2)

    def _get_category_stats(self, history: List[Dict]) -> Dict[str, int]:
        """ç»Ÿè®¡å·²æ·»åŠ éŸ³æ•ˆçš„ç±»åˆ«åˆ†å¸ƒ"""
        stats = {}
        for item in history:
            cat = item["category"]
            stats[cat] = stats.get(cat, 0) + 1
        return stats

    def _fallback_narrative_arc(self, total: int) -> List[str]:
        """ç®€å•åŸºäºè¿›åº¦çš„ fallback æƒ…ç»ªæ›²çº¿"""
        arc = []
        for i in range(total):
            progress = i / total
            if progress < 0.3:
                arc.append("å¹³é™ / æ€»ç»“")
            elif progress < 0.6:
                arc.append("æœŸå¾… / å¯åŠ¨")
            elif progress < 0.8:
                arc.append("æŠ•å…¥ / é«˜æ½®")
            else:
                arc.append("æ¿€åŠ± / è¡ŒåŠ¨")
        return arc

    def _analyze_emotion_curve(self, shots: List[Dict]) -> List[str]:
        """
        ä½¿ç”¨ Qwen å¯¹å®Œæ•´åˆ†é•œåºåˆ—åˆ†æï¼Œç”Ÿæˆæƒ…ç»ªæ›²çº¿æ ‡ç­¾
        """
        scene_descriptions = "\n".join([
            f"é•œå¤´ {i+1} ({shot['shot_type']}, {shot['duration']}ç§’): "
            f"{shot['visual_description']} "
            f"[å­—å¹•: {shot.get('caption', '')}]"
            for i, shot in enumerate(shots)
        ])

        prompt = f"""
        ä½ æ˜¯ä¸€ä½å½±è§†å™äº‹åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹è§†é¢‘åˆ†é•œåºåˆ—çš„æƒ…æ„Ÿå‘å±•å’Œå™äº‹èŠ‚å¥ã€‚
        ä¸ºæ¯ä¸€ä¸ªé•œå¤´åˆ¤æ–­å…¶æ‰€å±çš„å™äº‹é˜¶æ®µæˆ–æƒ…ç»ªç±»å‹ï¼Œä»ä»¥ä¸‹ç±»åˆ«ä¸­é€‰æ‹©æœ€åˆé€‚çš„ï¼š

        - å¹³é™ / æ€»ç»“
        - é“ºå« / å¼•å…¥
        - æœŸå¾… / å¯åŠ¨
        - æŠ•å…¥ / é«˜æ½®
        - æ¿€åŠ± / è¡ŒåŠ¨
        - æ”¶å°¾ / å‡å

        è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¾“å‡ºï¼Œæ•°ç»„é•¿åº¦å¿…é¡»ç­‰äºé•œå¤´æ•°é‡ï¼Œé¡ºåºå¯¹åº”ï¼š

        [
          "å¹³é™ / æ€»ç»“",
          "æœŸå¾… / å¯åŠ¨"
        ]

        ã€åˆ†é•œåºåˆ—ã€‘
        {scene_descriptions}

        æ³¨æ„ï¼šåªè¾“å‡º JSON æ•°ç»„ï¼Œä¸è¦é¢å¤–è¯´æ˜ã€‚
        """

        try:
            response = self.qwen.generate(prompt=prompt,parse_json=True)
            # result = json.loads(response.strip())
            result = response
            if isinstance(result, list) and len(result) == len(shots):
                return result
            else:
                print("âš ï¸ æƒ…ç»ªæ›²çº¿åˆ†æå¤±è´¥ï¼šæ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤")
        except Exception as e:
            print(f"âš ï¸ æƒ…ç»ªæ›²çº¿åˆ†æå¤±è´¥: {e}")

        return self._fallback_narrative_arc(len(shots))

    def should_add_sfx(
        self,
        shot: Dict,
        history: List[Dict],
        index: int,
        total: int,
        emotion_label: str
    ) -> Optional[Dict[str, str]]:
        """ä½¿ç”¨ Qwen åˆ¤æ–­æ˜¯å¦æ·»åŠ éŸ³æ•ˆï¼Œå¹¶æ¨èç±»åˆ«"""
        history = history or []
        category_stats = self._get_category_stats(history)
        category_summary = ", ".join([f"{k}:{v}" for k, v in category_stats.items()]) if category_stats else "æ— "
        recent_sfx = "æ— " if not history else "; ".join([
            f"{h['title']}@{h['time']:.1f}s" for h in history[-2:]
        ])

        prompt = f"""
        ä½ æ˜¯ä¸€ä½èµ„æ·±å½±è§†éŸ³æ•ˆè®¾è®¡å¸ˆã€‚è¯·ç»“åˆå™äº‹èŠ‚å¥å’ŒéŸ³æ•ˆå¹³è¡¡ï¼Œå†³å®šæ˜¯å¦æ·»åŠ éŸ³æ•ˆã€‚

        ã€AI åˆ†æçš„æƒ…ç»ªé˜¶æ®µã€‘
        {emotion_label}

        ã€å…¨å±€ä¿¡æ¯ã€‘
        - è§†é¢‘å…± {total} ä¸ªé•œå¤´ï¼Œå½“å‰æ˜¯ç¬¬ {index + 1} ä¸ªã€‚
        - å·²æ·»åŠ éŸ³æ•ˆç±»åˆ«ç»Ÿè®¡ï¼š{category_summary}
        - æœ€è¿‘æ·»åŠ éŸ³æ•ˆï¼š{recent_sfx}

        ã€è®¾è®¡åŸåˆ™ã€‘
        1. å¹³é™/æ€»ç»“ï¼šä¿æŒå®‰é™ï¼Œé¿å…å¹²æ‰°ã€‚
        2. æœŸå¾…/å¯åŠ¨ï¼šå¯åŠ  UI éŸ³æ•ˆå¼•å¯¼ã€‚
        3. æŠ•å…¥/é«˜æ½®ï¼šé€‚åˆäº¤äº’éŸ³ã€æ‰“å­—å£°ã€‚
        4. æ¿€åŠ±/è¡ŒåŠ¨ï¼šå¯åŠ  impact éŸ³æ•ˆå¢å¼ºåŠ¨åŠ›ã€‚
        5. é¿å…åŒç±»éŸ³æ•ˆè¿ç»­å‡ºç°ã€‚

        ã€å½“å‰é•œå¤´ã€‘
        - ç±»å‹ï¼š{shot['shot_type']}
        - æ—¶é•¿ï¼š{shot['duration']}ç§’
        - èŠ‚å¥ï¼š{shot['pacing']}
        - æè¿°ï¼š{shot['visual_description']}
        - å­—å¹•ï¼š{shot.get('caption', '')}

        è¯·è¾“å‡º JSONï¼š
        {{
          "add_sfx": true/false,
          "category": "ui|foley|ambience|impact|typing",
          "reason": "å†³ç­–ç†ç”±ï¼ŒåŒ…å«å¯¹æƒ…ç»ªå’Œå†å²çš„åˆ†æ"
        }}
        """

        try:
            response = self.qwen.generate(prompt=prompt,parse_json=True)

            # ç¡®ä¿responseæ˜¯å­—å…¸ç±»å‹
            if isinstance(response, str):
                import json
                result = json.loads(response)
            elif isinstance(response, dict):
                result = response
            else:
                print(f"âš ï¸ Qwenè¿”å›äº†æ„å¤–çš„ç±»å‹: {type(response)}")
                return None

            if result.get("add_sfx"):
                return {
                    "category": result["category"],
                    "reason": result["reason"]
                }
        except Exception as e:
            print(f"âš ï¸ Qwen å†³ç­–å¤±è´¥: {e}")
        return None

    def _extract_voice_timeline(self, voice_track: Dict) -> List[Dict]:
        """æå–å¸¦æ–‡æœ¬çš„äººå£°æ—¶é—´çº¿"""
        timeline = []
        for clip in voice_track.get("clips", []):
            start = clip.get("start")
            duration = clip.get("duration", 0.0)
            text = clip.get("text", "").strip().lower()
            if start is not None and duration > 0 and text:
                timeline.append({
                    "start": start,
                    "end": start + duration,
                    "text": text
                })
        return sorted(timeline, key=lambda x: x["start"])

    def _find_keyword_timestamp(self, voice_timeline: List[Dict], keyword: str) -> Optional[float]:
        """æŸ¥æ‰¾å…³é”®è¯å‡ºç°çš„å¤§è‡´æ—¶é—´"""
        for segment in voice_timeline:
            if keyword in segment["text"]:
                words = segment["text"].split()
                try:
                    idx = words.index(keyword)
                    ratio = idx / len(words)
                    return segment["start"] + ratio * (segment["end"] - segment["start"])
                except:
                    continue
        return None

    def _find_available_gaps(self, voice_timeline: List[Dict], min_gap: float, sfx_duration: float) -> List[Dict]:
        """æŸ¥æ‰¾å¯ç”¨äºæ’å…¥éŸ³æ•ˆçš„é™éŸ³é—´éš™"""
        gaps = []
        last_end = 0.0
        for seg in sorted(voice_timeline, key=lambda x: x["start"]):
            gap_start = last_end
            gap_end = seg["start"]
            if gap_end - gap_start >= max(min_gap, sfx_duration):
                gaps.append({"start": gap_start, "end": gap_end})
            last_end = seg["end"]
        return gaps

    def _align_sfx_to_voice(self, sfx_track: Dict, voice_track: Dict):
        """
        æ™ºèƒ½è°ƒæ•´éŸ³æ•ˆï¼šå…³é”®è¯å¯¹é½ â†’ é—´éš™æ’å…¥ â†’ éŸ³é‡é¿è®© â†’ æ·¡å…¥æ·¡å‡º
        """
        if not self.system_parameters.get("avoid_voice", True):
            return

        sfx_clips = sfx_track.get("clips", [])
        voice_clips = voice_track.get("clips", [])
        if not sfx_clips or not voice_clips:
            return

        voice_timeline = self._extract_voice_timeline(voice_track)

        for sfx_clip in sfx_clips:
            original_duration = sfx_clip["duration"]
            category = sfx_clip["category"]
            title = sfx_clip["title"]

            # 1. å…³é”®è¯å¯¹é½ï¼ˆUI ç±»ï¼‰
            # 1. ä½¿ç”¨ Qwen æ™ºèƒ½è¯­ä¹‰å¯¹é½ï¼ˆæ›¿ä»£ keyword_mapï¼‰
            aligned = False
            if category in ["ui", "click", "impact", "typing", "foley"]:
                word_timestamps = []
                for clip in voice_track.get("clips", []):
                    if "words" in clip:  # å‡è®¾ words åŒ…å«å­—/è¯çº§æ—¶é—´æˆ³
                        for word_info in clip["words"]:
                            word_timestamps.append({
                                "word": word_info["word"],
                                "start": word_info["start"],
                                "end": word_info["end"]
                            })

                if word_timestamps:
                    transcript = "".join([w["word"] for w in word_timestamps])
                    words_json = json.dumps(word_timestamps, ensure_ascii=False, indent=2)

                    prompt = f"""
                    ä½ æ˜¯ä¸€ä½éŸ³æ•ˆåŒæ­¥ä¸“å®¶ã€‚è¯·æ ¹æ®éŸ³æ•ˆè¯­ä¹‰å’Œäººå£°æ–‡æœ¬çš„æ—¶é—´æˆ³ï¼Œåˆ¤æ–­éŸ³æ•ˆæœ€åº”â€œå¯¹é½â€åˆ°å“ªä¸€ä¸ªå­—æˆ–è¯ã€‚

                    ã€éŸ³æ•ˆä¿¡æ¯ã€‘
                    - åç§°ï¼š{title}
                    - ç±»åˆ«ï¼š{category}
                    - æ—¶é•¿ï¼š{original_duration:.2f}ç§’

                    ã€äººå£°æ–‡æœ¬ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰ã€‘
                    {transcript}

                    è¯¦ç»†æ—¶é—´æˆ³ï¼ˆå•ä½ï¼šç§’ï¼‰ï¼š
                    {words_json}

                    ã€ä»»åŠ¡ã€‘
                    1. åˆ†æéŸ³æ•ˆè¯­ä¹‰ï¼ˆå¦‚â€œç‚¹å‡»â€åº”åŒ¹é…â€œç‚¹å‡»â€ä¸€è¯ï¼‰
                    2. æ‰¾å‡ºæœ€åº”åŒæ­¥çš„**ç›®æ ‡è¯æˆ–å­—**
                    3. è¿”å›è¯¥è¯çš„**å»ºè®®å¯¹é½æ—¶é—´**ï¼š
                    - å¦‚æœæ˜¯â€œç¬æ—¶éŸ³æ•ˆâ€ï¼ˆå¦‚ clickï¼‰ï¼Œå¯¹é½åˆ°è¯çš„**èµ·å§‹æ—¶é—´**
                    - å¦‚æœæ˜¯â€œæŒç»­éŸ³æ•ˆâ€ï¼Œå¯å¯¹é½åˆ°è¯ä¸­é—´
                    4. è¾“å‡º JSONï¼š
                    {{
                        "target_word": "ç‚¹å‡»",
                        "alignment_time": 10.9,
                        "reason": "â€˜ç‚¹å‡»â€™éŸ³æ•ˆåº”ä¸â€˜ç‚¹å‡»â€™åŠ¨è¯åŒæ­¥ï¼Œå¢å¼ºæ“ä½œåé¦ˆ"
                    }}

                    æ³¨æ„ï¼šåªè¾“å‡º JSONï¼Œä¸è¦é¢å¤–è¯´æ˜ã€‚
                    """

                    try:
                        response = self.qwen.generate(prompt=prompt,parse_json=True)
                        # result = json.loads(response.strip())
                        result = response
                        target_time = result.get("alignment_time")
                        target_word = result.get("target_word", "æœªçŸ¥")

                        if target_time is not None:
                            new_start = max(0.0, target_time - original_duration * 0.5)
                            sfx_clip["start"] = new_start
                            # åœ¨ _align_sfx_to_voice ä¸­ï¼Œå½“ä½ ä¿®æ”¹äº† start æˆ– duration åï¼ŒåŠ ä¸Šï¼š
                            sfx_clip["end"] = sfx_clip["start"] + sfx_clip["duration"]
                            sfx_clip["audio"]["duration"] = sfx_clip["duration"]
                            sfx_clip["audio"]["out_point"] = sfx_clip["duration"]
                            sfx_clip["processing_note"] = (
                                f"AI æ™ºèƒ½å¯¹é½ '{target_word}' @ {target_time:.2f}s | {result.get('reason', '')}"
                            )
                            aligned = True
                    except Exception as e:
                        print(f"âš ï¸ Qwen å¯¹é½æ¨ç†å¤±è´¥: {e}")

            # 2. é—´éš™æ’å…¥ï¼ˆç¯å¢ƒéŸ³ç­‰ï¼‰
            if not aligned:
                gaps = self._find_available_gaps(voice_timeline, MIN_GAP_FOR_INSERT, original_duration)
                for gap in gaps:
                    if gap["end"] - gap["start"] >= original_duration:
                        insert_time = gap["start"] + (gap["end"] - gap["start"] - original_duration) / 2
                        sfx_clip["start"] = insert_time
                        sfx_clip["end"] = sfx_clip["start"] + sfx_clip["duration"]
                        sfx_clip["audio"]["duration"] = sfx_clip["duration"]
                        sfx_clip["audio"]["out_point"] = sfx_clip["duration"]
                        sfx_clip["processing_note"] = f"æ’å…¥é™éŸ³é—´éš™ [{gap['start']:.1f}s-{gap['end']:.1f}s]"
                        aligned = True
                        break

            # 3. éŸ³é‡é¿è®©ï¼ˆå…œåº•ï¼‰
            sfx_start = sfx_clip["start"]
            sfx_end = sfx_start + sfx_clip["duration"]


            for voice_clip in voice_clips:
                v_start = voice_clip.get("start")
                v_dur = voice_clip.get("duration", 0.0)
                v_end = v_start + v_dur
                if v_start is None or v_dur <= 0:
                    continue
                if self._is_overlapping(sfx_start, sfx_end, v_start, v_end):
                    orig_vol = sfx_clip.get("volume_db", -12.0)
                    sfx_clip["volume_db"] = orig_vol - AVOIDANCE_MARGIN_DB
                    note = f"å› äººå£° [{v_start:.1f}s-{v_end:.1f}s] é™ {AVOIDANCE_MARGIN_DB}dB"
                    if "processing_note" in sfx_clip:
                        sfx_clip["processing_note"] += "; " + note
                    else:
                        sfx_clip["processing_note"] = note
                    break
                sfx_clip["end"] = sfx_clip["start"] + sfx_clip["duration"]
                sfx_clip["audio"]["duration"] = sfx_clip["duration"]
                sfx_clip["audio"]["out_point"] = sfx_clip["duration"]

            # 4. æ·»åŠ æ·¡å…¥æ·¡å‡º
            if "fade_in" not in sfx_clip:
                sfx_clip["fade_in"] = SFX_FADE_DURATION
            if "fade_out" not in sfx_clip:
                sfx_clip["fade_out"] = SFX_FADE_DURATION

    # async def generate(self, context: Dict[str, Any]) -> Dict[str, Any]:
    #     """
    #     ä¸»å…¥å£ï¼šç”ŸæˆéŸ³æ•ˆè½¨é“
    #     """
    #     shots: List[Dict] = context["shot_blocks_id"]
    #     voice_track: Dict = context.get("voice_track", {})
    #     current_time = 0.0

    #     sfx_track = {
    #         "track_name": "sfx",
    #         "track_type": "sound_effects",
    #         "clips": []
    #     }
    #     sfx_history = []

    #     # ğŸ”¥ ç¬¬ä¸€æ­¥ï¼šAI åˆ†ææƒ…ç»ªæ›²çº¿
    #     emotion_curve = self._analyze_emotion_curve(shots)
    #     print("ğŸ“Š æƒ…ç»ªæ›²çº¿:", emotion_curve)

    #     # å¤„ç†æ¯ä¸ªé•œå¤´
    #     for idx, shot in enumerate(shots):
    #         duration = shot["duration"]
    #         start_time = current_time

    #         # å†³ç­–æ˜¯å¦åŠ éŸ³æ•ˆ
    #         decision =  self.should_add_sfx(
    #             shot=shot,
    #             history=sfx_history,
    #             index=idx,
    #             total=len(shots),
    #             emotion_label=emotion_curve[idx]
    #         )
    #         if not decision:
    #             current_time += duration
    #             continue

    #         # åŒ¹é…éŸ³æ•ˆ
    #         request = SFXRequest(
    #             description=shot["visual_description"],
    #             category=decision["category"]
    #         )
    #         candidates = await match_sfx(request)
    #         if not candidates:
    #             current_time += duration
    #             continue

    #         sfx = candidates[0]
    #         final_duration = min(sfx.duration, duration * 0.8)
    #         insert_time = start_time + max(0, (duration - final_duration) / 2)

    #         # åˆ›å»ºéŸ³æ•ˆç‰‡æ®µ
    #         clip = {
    #             "sfx_id": f"auto_{len(sfx_track['clips']) + 1}",
    #             "title": sfx.title,
    #             "file_path": sfx.url,
    #             "start": insert_time,
    #             "duration": final_duration,
    #             "in_point": 0.0,
    #             "volume_db": -12.0,
    #             "pan": 0.0,
    #             "reverb_level": 0.3 if self.system_parameters.get("reverb_apply") else 0.0,
    #             "category": sfx.category,
    #             "tags": [],
    #             "source": "ai_match",
    #             "reason": decision["reason"]
    #         }
    #         sfx_track["clips"].append(clip)

    #         # æ›´æ–°å†å²
    #         sfx_history.append({
    #             "time": insert_time,
    #             "category": sfx.category,
    #             "title": sfx.title,
    #             "shot_index": idx
    #         })

    #         print(f"âœ… æ·»åŠ éŸ³æ•ˆ: {sfx.title} @ {insert_time:.1f}s | {sfx.category}")
    #         current_time += duration

    #     # ğŸ”Š æœ€åä¸€æ­¥ï¼šæ™ºèƒ½å¯¹é½ä¸é¿è®©
    #     self._align_sfx_to_voice(sfx_track, voice_track)

    #     return {"sfx_track": sfx_track}
    

  
    #æ–°generateï¼Œä¿®æ”¹äº†æ ¼å¼ï¼Œæ²¡æ”¹é€»è¾‘
    async def generate(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¸»å…¥å£ï¼šç”Ÿæˆç¬¦åˆæ ‡å‡†æ ¼å¼çš„éŸ³æ•ˆè½¨é“
        """
        self.validate_context(context)
        shots: List[Dict] = context["shot_blocks_id"]
        voice_track: Dict = context.get("voice_track", {})
        system_params = context.get("system_parameters", {})
        current_time = 0.0

        # åˆå§‹åŒ–æ ‡å‡†éŸ³è½¨ç»“æ„
        sfx_track = {
            "track_id": "sfx_track",
            "track_name": "éŸ³æ•ˆ",
            "track_type": "sound_effects",
            "total_duration": sum(shot["duration"] for shot in shots),
            "clips": []
        }
        sfx_history = []

        # ğŸ”¥ ç¬¬ä¸€æ­¥ï¼šAI åˆ†ææƒ…ç»ªæ›²çº¿
        emotion_curve = self._analyze_emotion_curve(shots)
        print("ğŸ“Š æƒ…ç»ªæ›²çº¿:", emotion_curve)

        # å¤„ç†æ¯ä¸ªé•œå¤´
        for idx, shot in enumerate(shots):
            duration = shot["duration"]
            start_time = current_time

            # å†³ç­–æ˜¯å¦åŠ éŸ³æ•ˆ
            decision = self.should_add_sfx(
                shot=shot,
                history=sfx_history,
                index=idx,
                total=len(shots),
                emotion_label=emotion_curve[idx]
            )
            if not decision:
                current_time += duration
                continue

            # åŒ¹é…éŸ³æ•ˆ
            request = SFXRequest(
                description=shot["visual_description"],
                category=decision["category"]
            )
            candidates = await match_sfx(request)
            if not candidates:
                current_time += duration
                continue

            sfx = candidates[0]
            final_duration = min(sfx.duration, duration * 0.8)
            insert_time = start_time + max(0, (duration - final_duration) / 2)

            # âœ… æ„å»ºæ ‡å‡† clip ç»“æ„
            clip = {
                "clip_id": f"sfx_{len(sfx_track['clips'])}",
                "start": insert_time,
                "end": insert_time + final_duration,
                "duration": final_duration,
                "title": sfx.title,
                "category": sfx.category,
                "tags": [],
                "source": "ai_match",
                "reason": decision["reason"],
                "audio": {
                    "url": sfx.url,
                    "in_point": 0.0,
                    "out_point": final_duration,
                    "duration": final_duration
                },
                "volume_db": -12.0,
                "pan": 0.0
            }
            sfx_track["clips"].append(clip)

            # æ›´æ–°å†å²ï¼ˆç”¨äºç±»åˆ«ç»Ÿè®¡ï¼‰
            sfx_history.append({
                "time": insert_time,
                "category": sfx.category,
                "title": sfx.title,
                "shot_index": idx
            })

            print(f"âœ… æ·»åŠ éŸ³æ•ˆ: {sfx.title} @ {insert_time:.1f}s | {sfx.category}")
            current_time += duration

        # ğŸ”Š æœ€åä¸€æ­¥ï¼šæ™ºèƒ½å¯¹é½ä¸é¿è®©ï¼ˆä¼šä¿®æ”¹ clip çš„ start/end/duration/audioï¼‰
        self._align_sfx_to_voice(sfx_track, voice_track)

        # âœ… é‡æ–°è®¡ç®— end å’Œ audio durationï¼ˆå¯¹é½åå¯èƒ½å˜åŒ–ï¼‰
        for clip in sfx_track["clips"]:
            new_duration = clip["duration"]
            clip["end"] = clip["start"] + new_duration
            clip["audio"]["out_point"] = new_duration
            clip["audio"]["duration"] = new_duration

        return {"sfx_track": sfx_track}
    async def regenerate(
        self,
        context: Dict[str, Any],
        existing_result: Optional[Dict] = None,
        feedback: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        åŸºäºå·²æœ‰ç»“æœå’Œåé¦ˆï¼Œé‡æ–°ç”ŸæˆéŸ³æ•ˆè½¨é“ã€‚
        é€‚ç”¨äºï¼šç”¨æˆ·æ‰‹åŠ¨è°ƒæ•´åè¦æ±‚â€œå†ä¼˜åŒ–ä¸€æ¬¡â€æˆ–æä¾›åé¦ˆã€‚

        :param context: åŸå§‹ä¸Šä¸‹æ–‡ï¼ˆshot_blocks_id, voice_track ç­‰ï¼‰
        :param existing_result: ä¸Šä¸€æ¬¡ generate çš„è¾“å‡ºï¼Œå¯é€‰
        :param feedback: ç”¨æˆ·æˆ–ç³»ç»Ÿçš„åé¦ˆï¼Œå¦‚ "UIéŸ³æ•ˆå¤ªå¤š"ã€"ç‚¹å‡»éŸ³æ•ˆè¦æ›´é å‰"
        :return: æ–°çš„ sfx_track
        """
        print("ğŸ”„ æ‰§è¡Œ regenerateï¼Œæ¥æ”¶åé¦ˆ:", feedback or "æ— ")

        # æ„é€ å¢å¼ºä¸Šä¸‹æ–‡
        enhanced_context = context.copy()

        if existing_result and feedback:
            # å°†ä¸Šæ¬¡ç»“æœå’Œåé¦ˆæ³¨å…¥ contextï¼Œä¾› AI å†³ç­–æ—¶å‚è€ƒ
            enhanced_context["previous_sfx_track"] = existing_result.get("sfx_track")
            enhanced_context["regeneration_feedback"] = feedback

        # è°ƒç”¨ä¸»ç”Ÿæˆé€»è¾‘ï¼ˆå¯åœ¨æ­¤åŠ å…¥åé¦ˆæ„ŸçŸ¥çš„ prompt ä¿®æ”¹ï¼‰
        return await self.generate(enhanced_context)