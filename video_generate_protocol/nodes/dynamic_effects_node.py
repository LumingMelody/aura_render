# nodes/dynamic_effects_node.py

from video_generate_protocol import BaseNode
from typing import Dict, List, Any
import random
import math
import re
import json

from llm import QwenLLM  # å‡è®¾è¿™æ˜¯ä½ å°è£…å¥½çš„ Qwen è°ƒç”¨æ¨¡å—

# ç‰¹æ•ˆç±»å‹åº“
EFFECT_TEMPLATES = {
    "lens_flare": {
        "name": "é•œå¤´å…‰æ™•",
        "type": "overlay",
        "layer": "top",
        "position": "auto",
        "animation": "fadeInOut",
        "opacity": 0.6,
        "scale": 1.0,
        "blend_mode": "screen",
        "trigger": ["high_emotion", "bright_scene"],
        "description": "æ¨¡æ‹ŸçœŸå®å…‰å­¦é•œå¤´å…‰æ™•ï¼Œå¢å¼ºç”»é¢æˆå‰§æ€§"
    },
    "particle_sparkle": {
        "name": "ç²’å­æ˜Ÿå…‰",
        "type": "particle",
        "emitter": "point",
        "count": 50,
        "lifetime": 3.0,
        "speed": 0.8,
        "color": "gold",
        "trigger": ["high_emotion", "celebration"],
        "description": "ç»†å°é—ªçƒç²’å­ï¼Œè¥é€ æ¢¦å¹»æˆ–æ¿€åŠ¨æ°›å›´"
    },
    "glow_pulse": {
        "name": "è„‰å†²è¾‰å…‰",
        "type": "effect",
        "target": "subject",
        "radius": 15,
        "intensity": 1.2,
        "frequency": 2.0,
        "trigger": ["high_emotion", "focus_moment"],
        "description": "ä¸»ä½“å‘¨å›´æŸ”å’Œè„‰åŠ¨å…‰æ™•ï¼Œçªå‡ºå…³é”®ç¬é—´"
    },
    "animated_textbox": {
        "name": "åŠ¨æ€æ–‡å­—æ¡†",
        "type": "text",
        "style": "modern",
        "animation": "slideUp",
        "position": "safe_bottom",
        "background_opacity": 0.7,
        "trigger": ["info_scene", "narration"],
        "description": "å¸¦åŠ¨ç”»è¿›å…¥çš„åŠé€æ˜æ–‡å­—æ¡†ï¼Œç”¨äºè¯´æ˜æ€§å†…å®¹"
    },
    "border_glow": {
        "name": "è¾¹ç¼˜è¾‰å…‰",
        "type": "effect",
        "side": "all",
        "color": "blue",
        "width": 3,
        "opacity": 0.4,
        "trigger": ["suspense", "tech_scene"],
        "description": "ç”»é¢è¾¹ç¼˜å‘å…‰ï¼Œå¢å¼ºç§‘æŠ€æ„Ÿæˆ–ç´§å¼ æ°›å›´"
    },
    "film_grain": {
        "name": "èƒ¶ç‰‡é¢—ç²’",
        "type": "overlay",
        "intensity": 0.3,
        "trigger": ["cinematic", "retro"],
        "description": "è½»å¾®é¢—ç²’æ„Ÿï¼Œå¢å¼ºç”µå½±è´¨æ„Ÿ"
    }
}

# æƒ…æ„Ÿå¼ºåº¦æ˜ å°„
EMOTION_TO_INTENSITY = {
    "æ¿€æ˜‚": 0.9,
    "æ„ŸåŠ¨": 0.85,
    "åŠ±å¿—": 0.8,
    "å¹½é»˜": 0.6,
    "å†·é™": 0.3,
    "æ‚¬ç–‘": 0.7,
    "æ¸©é¦¨": 0.75
}

# å®‰å…¨åŒºåŸŸæ¯”ä¾‹ï¼ˆç›¸å¯¹äºåˆ†è¾¨ç‡ï¼‰
SAFE_AREA = {
    "title": 0.9,   # æ ‡é¢˜å®‰å…¨åŒºï¼ˆ90%å±…ä¸­ï¼‰
    "action": 0.95  # åŠ¨ä½œå®‰å…¨åŒºï¼ˆ95%ï¼‰
}

# é»˜è®¤åˆ†è¾¨ç‡
DEFAULT_RESOLUTION = (1920, 1080)


class DynamicEffectsNode(BaseNode):
    required_inputs = [
        {
            "name": "filter_sequence_id",
            "label": "å‰ªè¾‘åºåˆ—",
            "type": list[Dict],
            "required": False,
            "default": [],
            "desc": "åŒ…å«é•œå¤´ä¿¡æ¯çš„å‰ªè¾‘åºåˆ—ï¼Œæ”¯æŒ emotion_hint, scene_type, faces ç­‰å­—æ®µ",
            "field_type": "json"
        },
        {
            "name": "emotions_id",
            "label": "æƒ…æ„Ÿå¼ºåº¦",
            "type": dict,
            "required": False,
            "desc": "æƒ…æ„Ÿæ ‡ç­¾åŠæƒé‡ï¼Œå¦‚ {'åŠ±å¿—': 50, 'å†·é™': 30}",
            "field_type": "json"
        },
        {
            "name": "resolution",
            "label": "è¾“å‡ºåˆ†è¾¨ç‡",
            "type": tuple,
            "required": False,
            "default": DEFAULT_RESOLUTION,
            "desc": "è§†é¢‘è¾“å‡ºåˆ†è¾¨ç‡ï¼Œå¦‚ (1920, 1080)",
            "field_type": "text"
        }
    ]


    output_schema=[
         {
            "name": "effects_sequence_id",
            "label": "æ·»åŠ ç‰¹æ•ˆçš„åˆ†é•œå—åˆ—è¡¨",
            "type": list,
            "required": True,
            "desc": "åŒ…å«é•œå¤´ä¿¡æ¯åŠæ·»åŠ çš„ç‰¹æ•ˆï¼Œå¦‚ [{'shot_id': 's1', 'shot_type': 'å…¨æ™¯', 'pacing': 'å¿«å‰ª', 'emotion_hint': 'æ¿€æ˜‚', 'visual_effects': [{'effect_id': 'eff_lens_flare_1234', 'name': 'é•œå¤´å…‰æ™•', 'position': {'x': 960, 'y': 540}, 'safe_position': True, 'position_reason': 'ç”»é¢å³ä¸Šè§’ç©ºæ—·åŒºåŸŸ'}]}]",
            "field_type": "json"
        }
        
    ]
    file_upload_config = {
        "image": {"enabled": True, "accept": ".png,.jpg,.svg", "desc": "å¯ä¸Šä¼ è‡ªå®šä¹‰è´´å›¾ï¼ˆå¦‚å…‰æ™•PNGï¼‰"},
        "video": {"enabled": False}
    }

    system_parameters = {
        "max_effects_per_clip": 2,
        "default_emotion": "å†·é™",
        "min_emotion_threshold": 0.6  # è§¦å‘é«˜æƒ…æ„Ÿç‰¹æ•ˆçš„é˜ˆå€¼
    }


    def __init__(self, node_id: str, name: str = "åŠ¨æ€ç‰¹æ•ˆæ·»åŠ "):
        self.node_id = node_id
        self.node_type = "dynamic_effects"
        self.name = name
        self.qwen_caller = QwenLLM() 
        self.resolution = DEFAULT_RESOLUTION
        self.uploaded_files = []

    def set_qwen_caller(self, caller: QwenLLM):
        """æ³¨å…¥ Qwen è°ƒç”¨å™¨"""
        self.qwen_caller = caller
        return self

    def set_resolution(self, width: int, height: int):
        self.resolution = (width, height)
        return self

    def _extract_text_from_clip(self, clip: Dict) -> str:
        desc = clip.get("metadata", {}).get("description", "")
        match = re.search(r"Placeholder for: (.+)", desc)
        return match.group(1).strip() if match else desc

    def _get_effect_suggestions_from_qwen(self, description: str) -> List[str]:
        """ä½¿ç”¨æ–‡æœ¬ Qwen æ¨èç‰¹æ•ˆï¼ˆå¯æ”¹ä¸º VL ç»Ÿä¸€æ¥å£ï¼‰"""
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å‰ªè¾‘å¯¼æ¼”...

å¯ç”¨ç‰¹æ•ˆï¼š
{list(EFFECT_TEMPLATES.keys())}

é•œå¤´æè¿°ï¼š
"{description}"

è¦æ±‚ï¼šåªè¿”å› JSON æ•°ç»„ï¼Œå¦‚ ["lens_flare", "film_grain"]
        """
        resp = self.qwen_caller.generate(prompt)
        if not resp:
            return []

        try:
            json_start = resp.find("[")
            json_end = resp.rfind("]") + 1
            if json_start == -1 or json_end <= 0:
                return []
            effects = json.loads(resp[json_start:json_end])
            return [e for e in effects if e in EFFECT_TEMPLATES][:2]
        except:
            return []

    def _get_position_from_image_vl(self, clip: Dict, effect_name: str) -> Dict:
        
        return {
            "position_px": {"x": int(1920), "y": int(1080)},
            "position_norm": {"x": 1, "y": 1},
            "safe": False,
            "reason": "å›¾åƒåˆ†æå¤±è´¥ï¼Œä½¿ç”¨éšæœºä½ç½®",
            "raw": None
        }
        
        image_url = clip["source_url"]
        description = self._extract_text_from_clip(clip)
        width, height = self.resolution

        # ğŸ”¹ ç”±æœ¬ç±»æ„é€  prompt
        prompt = f"""
            è¯·åˆ†æå›¾åƒï¼Œä¸ºã€{effect_name}ã€‘ç‰¹æ•ˆæ¨èä¸€ä¸ªåˆé€‚çš„æ·»åŠ ä½ç½®ã€‚

            ã€ç”»é¢æè¿°ã€‘
            {description}

            ã€è¦æ±‚ã€‘
            - è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡
            - åŒ…å«å­—æ®µï¼šposition (x, y)ï¼ŒèŒƒå›´ 0~1ï¼›safe (boolean)ï¼›reason (string)
            - ä½ç½®åº”ä¸ºç©ºæ—·åŒºåŸŸï¼Œé¿å…é®æŒ¡äººè„¸æˆ–ä¸»ä½“

            ç¤ºä¾‹ï¼š
            {{"position": {{"x": 0.8, "y": 0.2}}, "safe": true, "reason": "å³ä¸Šè§’æ˜¯å¤©ç©º"}}
                    """

        # ğŸ”¹ è°ƒç”¨é€šç”¨æ¥å£
        result = self.qwen_caller.generate(
            prompt=prompt,
            images=[image_url],
            parse_json=True,
            json_schema={
                "position": "dict",
                "safe": "bool",
                "reason": "str"
            },
            max_retries=3
        )

        # ğŸ”¹ æœ¬ç±»è´Ÿè´£è§£æå’Œ fallback
        if isinstance(result, dict):
            pos = result.get("position", {})
            x = max(0.0, min(1.0, pos.get("x", random.uniform(0.3, 0.7))))
            y = max(0.0, min(1.0, pos.get("y", random.uniform(0.3, 0.7))))
            return {
                "position_px": {"x": int(x * width), "y": int(y * height)},
                "position_norm": {"x": x, "y": y},
                "safe": bool(result.get("safe", False)),
                "reason": str(result.get("reason", "AI æ¨èä½ç½®")),
                "raw": result
            }

        # ğŸ”¹ Fallback
        x_norm, y_norm = random.uniform(0.2, 0.8), random.uniform(0.2, 0.6)
        return {
            "position_px": {"x": int(x_norm * width), "y": int(y_norm * height)},
            "position_norm": {"x": x_norm, "y": y_norm},
            "safe": False,
            "reason": "å›¾åƒåˆ†æå¤±è´¥ï¼Œä½¿ç”¨éšæœºä½ç½®",
            "raw": None
        }
    async def generate(self, context: Dict[str, Any]) -> Dict[str, Any]:
        self.validate_context(context)
        if not self.qwen_caller:
            raise RuntimeError("å¿…é¡»å…ˆè°ƒç”¨ set_qwen_caller() è®¾ç½® Qwen è°ƒç”¨å™¨")

        sequence: List[Dict] = context.get("filter_sequence_id", [])
        self.resolution = context.get("resolution", DEFAULT_RESOLUTION)

        processed_sequence = []

        for clip in sequence:
            description = self._extract_text_from_clip(clip)
            if not description.strip():
                clip["visual_effects"] = []
                processed_sequence.append(clip)
                continue

            # Step 1: Qwen æ¨èç‰¹æ•ˆ
            suggested_keys = self._get_effect_suggestions_from_qwen(description)
            if not suggested_keys:
                clip["visual_effects"] = []
                processed_sequence.append(clip)
                continue

            # Step 2: ä¸ºæ¯ä¸ªç‰¹æ•ˆè°ƒç”¨å›¾åƒç†è§£å®šä½
            effects_to_add = []
            for key in suggested_keys:
                template = EFFECT_TEMPLATES[key]
                effect = {**template}

                # è°ƒç”¨ VL è·å–æ™ºèƒ½ä½ç½®
                pos_result = self._get_position_from_image_vl(clip, effect["name"])
                effect["position"] = pos_result["position_px"]  # åƒç´ åæ ‡
                effect["safe_position"] = pos_result["safe"]
                effect["position_reason"] = pos_result["reason"]

                # å”¯ä¸€ ID
                effect["effect_id"] = f"eff_{key}_{random.randint(1000, 9999)}"

                effects_to_add.append(effect)

            # è‡ªå®šä¹‰è´´å›¾æ”¯æŒ
            if self.uploaded_files and "lens_flare" in suggested_keys:
                for file_info in self.uploaded_files:
                    if file_info["type"] == "image" and "flare" in file_info["filename"].lower():
                        for eff in effects_to_add:
                            if eff["name"] == "é•œå¤´å…‰æ™•":
                                eff["texture_path"] = file_info["path"]
                                eff["custom"] = True
                        break

            clip["visual_effects"] = effects_to_add
            processed_sequence.append(clip)

        return {"effects_sequence_id": processed_sequence}

    def regenerate(self, context: Dict[str, Any], user_intent: Dict[str, Any]) -> Dict[str, Any]:
        """æ”¯æŒç”¨æˆ·å¹²é¢„ï¼Œå¦‚ï¼šâ€œè¿™ä¸ªé•œå¤´åŠ é‡‘è‰²ç²’å­â€ æˆ– â€œç§»é™¤å…‰æ™•â€"""
        super().regenerate(context, user_intent)

        override = user_intent.get("effects_override")
        if not override:
            return self.generate(context)

        result = self.generate(context)
        sequence = result["edited_sequence"]

        if "clear_all" in override:
            for clip in sequence:
                if override["clear_all"]:
                    clip["visual_effects"] = []
        elif "per_clip" in override:
            for item in override["per_clip"]:
                shot_id = item["shot_id"]
                effects = item["effects"]  # list of effect keys
                for clip in sequence:
                    if clip.get("shot_id") == shot_id:
                        clip["visual_effects"] = []
                        for eff_key in effects:
                            if eff_key in EFFECT_TEMPLATES:
                                clip["visual_effects"].append(
                                    self._create_effect(eff_key, context.get("resolution", DEFAULT_RESOLUTION), clip.get("faces", []))
                                )
                        # é™åˆ¶æ•°é‡
                        clip["visual_effects"] = clip["visual_effects"][:self.system_parameters["max_effects_per_clip"]]

        return result
    


if __name__ == "__main__":
     # åŠ è½½ä½ çš„è¾“å…¥æ•°æ®
    input_data = {
  "edited_sequence": [
    {
      "id": "clip_1a2b3c4d",
      "index": 0,
      "asset_id": "placeholder_0",
      "source_url": "https://example.com/assets/placeholder.mp4",
      "start": 0.0,
      "end": 4.5,
      "duration": 4.5,
      "source": {
        "in": 0.0,
        "out": 4.5
      },
      "transition_in": {
        "type": "cross_dissolve",
        "duration": 0
      },
      "transition_out": {
        "type": "cross_dissolve",
        "name": "å åŒ–",
        "duration": 1.2
      },
      "metadata": {
        "description": "Placeholder for: ä¸€ä¸ªé˜³å…‰æ˜åªšçš„æ—©æ™¨ï¼Œé¸Ÿå„¿åœ¨æ ‘ä¸Šæ­Œå”±ï¼ŒåŸå¸‚ æ…¢æ…¢è‹é†’ã€‚...",
        "tags": [],
        "provider": "system_placeholder"
      },
      "transform": {
        "scale": 1.0,
        "position": "center"
      },
      "color_filter": {
        "preset": "cinematic",
        "name": "ç”µå½±æ„Ÿ",
        "lut_path": "luts/cinematic.cube",
        "intensity": 0.8,
        "applied_params": {
          "contrast": 1.792,
          "saturation": 1.664,
          "shadows": 0.98,
          "highlights": 1.69,
          "temperature": 0.24
        },
        "scene_adaptation": "æˆ·å¤–æ™´å¤©",
        "description": "ä½é¥±å’Œã€é«˜å¯¹æ¯”ï¼Œé€‚åˆå™äº‹ç±»è§†é¢‘",
        "source": "ai_decision_with_consistency",
        "global_theme": "cinematic",
        "local_decision": "cinematic",
        "smooth_adjusted": False
      }
    },
    {
      "id": "clip_5e6f7g8h",
      "index": 1,
      "asset_id": "placeholder_1",
      "source_url": "https://example.com/assets/placeholder.mp4",
      "start": 5.0,
      "end": 8.0,
      "duration": 3.0,
      "source": {
        "in": 0.0,
        "out": 3.0
      },
      "transition_in": {
        "type": "cross_dissolve",
        "duration": 0.5
      },
      "transition_out": {
        "type": "cross_dissolve",
        "name": "å åŒ–",
        "duration": 1.2
      },
      "metadata": {
        "description": "Placeholder for: ä¸€ä½å¹´è½»äººåœ¨å’–å•¡é¦†é‡Œä¸“æ³¨åœ°æ•²ç€ç¬”è®°æœ¬ç”µè„‘ ï¼Œå‘¨å›´äººæ¥å¾€ã€‚...",
        "tags": [],
        "provider": "system_placeholder"
      },
      "transform": {
        "scale": 1.0,
        "position": "center"
      },
      "color_filter": {
        "preset": "cinematic",
        "name": "ç”µå½±æ„Ÿ",
        "lut_path": "luts/cinematic.cube",
        "intensity": 0.8,
        "applied_params": {
          "contrast": 1.12,
          "saturation": 0.96,
          "shadows": 1.82,
          "highlights": 0.93,
          "temperature": 0.304
        },
        "scene_adaptation": "å®¤å†…",
        "description": "ä½é¥±å’Œã€é«˜å¯¹æ¯”ï¼Œé€‚åˆå™äº‹ç±»è§†é¢‘",
        "source": "ai_decision_with_consistency",
        "global_theme": "cinematic",
        "local_decision": "cinematic",
        "smooth_adjusted": False
      }
    },
    {
      "id": "clip_9i0j1k2l",
      "index": 2,
      "asset_id": "placeholder_2",
      "source_url": "https://example.com/assets/placeholder.mp4",
      "start": 8.5,
      "end": 11.5,
      "duration": 3.0,
      "source": {
        "in": 0.0,
        "out": 3.0
      },
      "transition_in": {
        "type": "cross_dissolve",
        "duration": 0.5
      },
      "transition_out": {
        "type": "none",
        "duration": 0.0
      },
      "metadata": {
        "description": "Placeholder for: å¤•é˜³ä¸‹ï¼Œä¸€å¯¹æƒ…ä¾£åœ¨æµ·è¾¹æ•£æ­¥ï¼ŒèƒŒå½±æ¸©é¦¨ã€‚...",
        "tags": [],
        "provider": "system_placeholder"
      },
      "transform": {
        "scale": 1.0,
        "position": "center"
      },
      "color_filter": {
        "preset": "cinematic",
        "name": "ç”µå½±æ„Ÿ",
        "lut_path": "luts/cinematic.cube",
        "intensity": 0.8,
        "applied_params": {
          "contrast": 1.12,
          "saturation": 1.696,
          "shadows": 0.98,
          "highlights": 1.81,
          "temperature": 0.368
        },
        "scene_adaptation": "é»„æ˜",
        "description": "ä½é¥±å’Œã€é«˜å¯¹æ¯”ï¼Œé€‚åˆå™äº‹ç±»è§†é¢‘",
        "source": "ai_decision_with_consistency",
        "global_theme": "cinematic",
        "local_decision": "cinematic",
        "smooth_adjusted": False
      }
    }
  ],
  "color_consistency_report": {
    "main_theme": "cinematic",
    "filter_chain": [
      "cinematic",
      "cinematic",
      "cinematic"
    ],
    "consistency_score": 0.83,
    "total_clips": 3
  }
}
    # åˆ›å»ºèŠ‚ç‚¹
    node = DynamicEffectsNode(node_id="dynamic_fx_01")

    # è°ƒç”¨ generate
    result = node.generate(input_data)

    # è¾“å‡ºç»“æœ
    import json
    print(json.dumps(result, indent=2, ensure_ascii=False))