# nodes/timeline_integration_node.py

from video_generate_protocol import BaseNode
import logging

logger = logging.getLogger(__name__)

from typing import Dict, List, Any
import uuid
from datetime import datetime
import json
from decimal import Decimal

# æ—¶é—´çº¿è½¨é“ç±»å‹
TRACK_TYPES = {
    "video": 0,
    "audio": 1,
    "subtitle": 2,
    "overlay": 3,
    "effect": 4
}

# æ”¯æŒçš„è¾“å‡ºå·¥ç¨‹æ ¼å¼
SUPPORTED_EDL_FORMATS = ["edl", "fcpxml", "aaf", "json_timeline", "csv"]

class TimelineIntegrationNode(BaseNode):
    required_inputs = [
        {
            "name": "video_clips",
            "label": "ä¸»è§†é¢‘å‰ªè¾‘åºåˆ—",
            "type": list,
            "required": True,
            "desc": "ä¸»å†…å®¹è§†é¢‘ç‰‡æ®µåˆ—è¡¨ï¼Œå«æ—¶é—´ç ",
            "field_type": "json"
        },
        {
            "name": "audio_tracks",
            "label": "éŸ³é¢‘è½¨é“",
            "type": list,
            "required": False,
            "default": [],
            "desc": "BGMã€éŸ³æ•ˆã€äººå£°ç­‰éŸ³é¢‘è½¨é“",
            "field_type": "json"
        },
        {
            "name": "subtitle_sequence",
            "label": "å­—å¹•åºåˆ—",
            "type": dict,
            "required": False,
            "default": None,
            "desc": "å¸¦æ—¶é—´ç çš„å­—å¹•è½¨é“",
            "field_type": "json"
        },
        {
            "name": "intro_outro_sequence",
            "label": "ç‰‡å¤´ç‰‡å°¾",
            "type": dict,
            "required": False,
            "default": None,
            "desc": "åŒ…å«introå’Œoutroçš„ç»“æ„",
            "field_type": "json"
        }
    ]

    system_parameters = {
        "frame_rate": 30,
        "timecode_start": "00:00:00:00",
        "sync_tolerance": 0.05,  # éŸ³è§†é¢‘åŒæ­¥å®¹å·®ï¼ˆç§’ï¼‰
        "output_format": "json_timeline",  # edl, fcpxml, aaf, json_timeline, csv
        "include_transition": True,
        "default_transition_duration": 0.5,
        "use_timecode": True,
        "project_name": "AutoGenerated_Project"
    }

    def __init__(self, node_id: str, name: str = "æœ€ç»ˆæ—¶é—´çº¿æ•´åˆ"):
        super().__init__(node_id=node_id, node_type="timeline", name=name)

    async def generate(self, context: Dict[str, Any]) -> Dict[str, Any]:
        self.validate_context(context)

        # è·å–è¾“å…¥
        # âœ¨ ä» Node 5 (asset_request) çš„è¾“å‡ºä¸­è·å–è§†é¢‘ç‰‡æ®µ
        video_clips = context.get("video_clips", [])
        audio_tracks = context.get("audio_tracks", [])
        subtitle_seq = context.get("subtitle_sequence_id")  # âœ¨ ä¿®æ­£ï¼šNode 14è¾“å‡ºçš„å­—æ®µåæ˜¯ subtitle_sequence_id
        intro_outro_seq = context.get("intro_outro_sequence")

        # ===  æ‰§è¡Œæœ€ç»ˆè§†é¢‘åˆæˆ ===
        logger.info(f"ğŸ¬ [Node 16] Starting final video composition...")
        logger.info(f"ğŸ“Š [Node 16] Received {len(video_clips)} video clips from Node 5")

        final_video_url = None
        final_video_path = None
        merge_success = False

        if video_clips and len(video_clips) > 0:
            try:
                # å¯¼å…¥å¿…è¦æ¨¡å—
                import sys
                import os
                import uuid
                sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

                # âœ… ä½¿ç”¨é˜¿é‡Œäº‘IMS APIåˆå¹¶è§†é¢‘ï¼ˆç»Ÿä¸€å¤„ç†OSSå’Œä¸‡ç›¸ç”Ÿæˆçš„è§†é¢‘ï¼‰
                logger.info(f"ğŸ¬ [Node 16] ä½¿ç”¨é˜¿é‡Œäº‘IMS APIåˆå¹¶è§†é¢‘...")
                logger.info(f"ğŸ“Š [Node 16] å¾…åˆå¹¶è§†é¢‘æ•°é‡: {len(video_clips)}")

                from video_generate_protocol.nodes.qwen_integration import StoryboardToVideoProcessor

                # è·å–APIå¯†é’¥
                qwen_key = os.getenv('DASHSCOPE_API_KEY') or os.getenv('AI__DASHSCOPE_API_KEY')
                if not qwen_key:
                    raise ValueError("ç¼ºå°‘åƒé—®APIå¯†é’¥")

                video_processor = StoryboardToVideoProcessor(qwen_key)

                # æ‰§è¡Œè§†é¢‘åˆå¹¶
                final_video_path_temp = f"/tmp/final_video_{uuid.uuid4().hex[:8]}.mp4"
                logger.info(f"ğŸ”— [Node 16] Merging {len(video_clips)} clips into final video...")

                # ä¼ é€’å­—å¹•åºåˆ—åˆ°merge_clips
                merge_result = await video_processor.merge_clips(
                    video_clips,
                    final_video_path_temp,
                    subtitle_sequence=subtitle_seq  # âœ¨ ä¼ é€’å­—å¹•åºåˆ—
                )

                if merge_result.get("success"):
                    final_video_url = merge_result.get("video_url")
                    final_video_path = merge_result.get("local_path")
                    merge_success = True

                    # é†’ç›®çš„æœ€ç»ˆè§†é¢‘URLæ‰“å°
                    logger.info(f"\n{'='*80}")
                    logger.info(f"âœ… [Node 16] æœ€ç»ˆè§†é¢‘åˆæˆæˆåŠŸï¼")
                    logger.info(f"{'='*80}")
                    logger.info(f"ğŸ¬ æœ€ç»ˆè§†é¢‘URL: {final_video_url}")
                    if final_video_path:
                        logger.info(f"ğŸ“ æœ¬åœ°è·¯å¾„: {final_video_path}")
                    logger.info(f"{'='*80}\n")
                else:
                    logger.info(f"âŒ [Node 16] Video merge failed")

            except Exception as e:
                logger.info(f"âŒ [Node 16] Final video composition failed: {e}")
                import traceback
                traceback.print_exc()
        else:
            logger.info(f"âš ï¸ [Node 16] No video clips to merge")

        # æ„å»ºç»Ÿä¸€æ—¶é—´çº¿
        timeline = {
            "project": {
                "name": self.system_parameters["project_name"],
                "id": f"proj_{uuid.uuid4().hex[:8]}",
                "frame_rate": self.system_parameters["frame_rate"],
                "timecode_start": self.system_parameters["timecode_start"],
                "created_at": datetime.now().isoformat(),
                "duration": 0.0
            },
            "tracks": {
                "video": [],
                "audio": [],
                "subtitle": [],
                "intro": [],
                "outro": []
            },
            "metadata": {
                "sync_tolerance": self.system_parameters["sync_tolerance"],
                "transition_enabled": self.system_parameters["include_transition"]
            }
        }

        # 1. æ·»åŠ ç‰‡å¤´
        if intro_outro_seq and intro_outro_seq.get("intro"):
            intro_clip = self._create_timeline_clip(
                source=intro_outro_seq["intro"],
                track_type="video",
                start=0.0,
                layer=10  # é«˜å±‚ä¼˜å…ˆ
            )
            timeline["tracks"]["video"].append(intro_clip)
            timeline["tracks"]["intro"].append(intro_clip)

        # 2. æ·»åŠ ä¸»è§†é¢‘ç‰‡æ®µï¼ˆæ¥åœ¨ç‰‡å¤´åï¼‰
        current_time = self._get_intro_duration(intro_outro_seq)
        video_start_time = current_time

        for clip in video_clips:
            # å…¼å®¹ä¸¤ç§æ ¼å¼ï¼š
            # 1. æœ‰ start/end çš„æ ¼å¼ (ä¼ ç»Ÿæ ¼å¼)
            # 2. åªæœ‰ duration çš„æ ¼å¼ (ä»ä¸‡ç›¸ç”Ÿæˆçš„è§†é¢‘)
            if "end" in clip and "start" in clip:
                duration = clip["end"] - clip["start"]
            elif "duration" in clip:
                duration = clip["duration"]
            else:
                logger.info(f"âš ï¸ [Node 16] Clip missing duration info: {clip}")
                duration = 5.0  # é»˜è®¤5ç§’

            video_clip = self._create_timeline_clip(
                source=clip,
                track_type="video",
                start=current_time,
                duration=duration,
                layer=0
            )
            timeline["tracks"]["video"].append(video_clip)
            current_time += duration

        # 3. æ·»åŠ éŸ³é¢‘è½¨é“ï¼ˆå¯¹é½ä¸»è§†é¢‘èµ·å§‹æ—¶é—´ï¼‰
        for audio_track in audio_tracks:
            clips = audio_track.get("clips", [])
            track_start = video_start_time  # éŸ³é¢‘å¯¹é½è§†é¢‘å¼€å§‹
            for clip in clips:
                # å…¼å®¹ä¸¤ç§æ ¼å¼ï¼šæœ‰ start/end æˆ–åªæœ‰ duration
                if "end" in clip and "start" in clip:
                    duration = clip["end"] - clip["start"]
                elif "duration" in clip:
                    duration = clip["duration"]
                else:
                    duration = 3.0  # éŸ³é¢‘é»˜è®¤3ç§’

                audio_clip = self._create_timeline_clip(
                    source=clip,
                    track_type="audio",
                    start=track_start,
                    duration=duration,
                    layer=audio_track.get("layer", 0),
                    metadata={"source_track": audio_track.get("track_name")}
                )
                timeline["tracks"]["audio"].append(audio_clip)
                track_start += duration

        # 4. æ·»åŠ å­—å¹•ï¼ˆå¯¹é½ä¸»è§†é¢‘æ—¶é—´è½´ï¼‰
        if subtitle_seq:
            for sub_clip in subtitle_seq["clips"]:
                subtitle_clip = self._create_timeline_clip(
                    source=sub_clip,
                    track_type="subtitle",
                    start=video_start_time + sub_clip["start"],
                    duration=sub_clip["duration"],
                    layer=1
                )
                timeline["tracks"]["subtitle"].append(subtitle_clip)

        # 5. æ·»åŠ ç‰‡å°¾ï¼ˆæ¥åœ¨ä¸»å†…å®¹åï¼‰
        outro_start = current_time
        if intro_outro_seq and intro_outro_seq.get("outro"):
            outro_duration = intro_outro_seq["outro"]["duration"]
            outro_clip = self._create_timeline_clip(
                source=intro_outro_seq["outro"],
                track_type="video",
                start=outro_start,
                duration=outro_duration,
                layer=10
            )
            timeline["tracks"]["video"].append(outro_clip)
            timeline["tracks"]["outro"].append(outro_clip)

        # è®¡ç®—æ€»æ—¶é•¿
        total_duration = outro_start + (intro_outro_seq["outro"]["duration"] if intro_outro_seq and intro_outro_seq.get("outro") else 0)
        timeline["project"]["duration"] = round(float(total_duration), 3)

        # æ’åºå„è½¨é“ç‰‡æ®µ
        for track_name in timeline["tracks"]:
            timeline["tracks"][track_name].sort(key=lambda x: x["start"])

        # ç”Ÿæˆ EDL / å·¥ç¨‹æ–‡ä»¶
        edl_data = self._generate_edl(timeline)

        # è¿”å›å®Œæ•´å·¥ç¨‹ï¼ˆåŒ…æ‹¬æœ€ç»ˆåˆæˆçš„è§†é¢‘ï¼‰
        return {
            "final_timeline": timeline,
            "edl_export": {
                "format": self.system_parameters["output_format"],
                "data": edl_data,
                "timestamp": datetime.now().isoformat()
            },
            # âœ¨ æ–°å¢ï¼šæœ€ç»ˆåˆæˆçš„è§†é¢‘ä¿¡æ¯
            "final_video_url": final_video_url,
            "final_video_path": final_video_path,
            "merge_success": merge_success,
            "video_clips_count": len(video_clips)
        }

    def _create_timeline_clip(self, source: Dict, track_type: str, start: float, duration=None, layer=0, metadata=None):
        """åˆ›å»ºæ ‡å‡†æ—¶é—´çº¿ç‰‡æ®µ"""
        if duration is None:
            duration = source.get("duration", (source["end"] - source["start"]) if "end" in source and "start" in source else 2.0)

        end = start + duration

        clip = {
            "id": f"clip_{uuid.uuid4().hex[:6]}",
            "source_id": source.get("id", None),
            "type": track_type,
            "start": round(start, 3),
            "end": round(end, 3),
            "duration": round(duration, 3),
            "layer": layer,
            "source": {
                "in": source.get("start", 0),
                "out": source.get("end", duration)
            },
            "transform": {
                "position": source.get("position", "center"),
                "scale": source.get("scale", 1.0)
            },
            "metadata": metadata or {}
        }

        # æ·»åŠ æ—¶é—´ç ï¼ˆå¯é€‰ï¼‰
        if self.system_parameters["use_timecode"]:
            clip["timecode_in"] = self._seconds_to_timecode(start)
            clip["timecode_out"] = self._seconds_to_timecode(end)

        return clip

    def _get_intro_duration(self, intro_outro_seq: Dict) -> float:
        """è·å–ç‰‡å¤´æ—¶é•¿"""
        if intro_outro_seq and intro_outro_seq.get("intro"):
            return intro_outro_seq["intro"]["duration"]
        return 0.0

    def _generate_edl(self, timeline: Dict) -> str:
        """ç”ŸæˆæŒ‡å®šæ ¼å¼çš„å·¥ç¨‹æ–‡ä»¶"""
        fmt = self.system_parameters["output_format"]

        if fmt == "json_timeline":
            return json.dumps(timeline, ensure_ascii=False, indent=2)

        elif fmt == "edl":
            return self._to_edl_format(timeline)

        elif fmt == "csv":
            return self._to_csv_format(timeline)

        elif fmt in ["fcpxml", "aaf"]:
            # è¿™é‡Œå¯é›†æˆç¬¬ä¸‰æ–¹åº“ï¼ˆå¦‚ `opentimelineio`ï¼‰å¯¼å‡ºæ ‡å‡†æ ¼å¼
            return f"<!-- {fmt.upper()} æ ¼å¼æ”¯æŒéœ€é›†æˆ OpenTimelineIO -->\n<!-- å½“å‰è¿”å›ç®€åŒ–ç»“æ„ -->\n" + \
                   json.dumps({"format": fmt, "project": timeline["project"], "clip_count": len(timeline["tracks"]["video"])}, indent=2)

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {fmt}")

    def _to_edl_format(self, timeline: Dict) -> str:
        """ç”Ÿæˆæ ‡å‡†EDLï¼ˆEdit Decision Listï¼‰æ–‡æœ¬"""
        fps = self.system_parameters["frame_rate"]
        lines = [
            f"TITLE: {timeline['project']['name']}",
            f"FCM: NON-DROP FRAME",
            ""
        ]

        index = 1
        video_clips = sorted(
            timeline["tracks"]["video"] + timeline["tracks"]["intro"] + timeline["tracks"]["outro"],
            key=lambda x: x["start"]
        )

        for clip in video_clips:
            start_frames = self._time_to_frames(clip["start"], fps)
            end_frames = self._time_to_frames(clip["end"], fps)
            # ç®€åŒ–æºç ï¼šä½¿ç”¨å ä½ç¬¦
            source_in = "00:00:00:00"
            source_out = self._seconds_to_timecode(clip["duration"], fps)

            lines.append(f"{str(index).zfill(3)}  V001  AA/V  C        {source_in} {source_out} "
                         f"{clip['timecode_in']} {clip['timecode_out']}")
            index += 1

        return "\n".join(lines)

    def _to_csv_format(self, timeline: Dict) -> str:
        """ç”ŸæˆCSVæ ¼å¼æ—¶é—´çº¿"""
        lines = ["type,id,start,end,duration,layer,source_in,source_out,timecode_in,timecode_out"]
        all_clips = []
        for track_type, clips in timeline["tracks"].items():
            for clip in clips:
                all_clips.append({
                    **clip,
                    "track_type": track_type
                })

        all_clips.sort(key=lambda x: x["start"])

        for clip in all_clips:
            lines.append(
                f"{clip['track_type']},{clip['id']},{clip['start']},{clip['end']},{clip['duration']},"
                f"{clip['layer']},{clip['source']['in']},{clip['source']['out']},"
                f"{clip.get('timecode_in', '')},{clip.get('timecode_out', '')}"
            )

        return "\n".join(lines)

    def _seconds_to_timecode(self, seconds: float, fps: int = None) -> str:
        """ç§’è½¬æ—¶é—´ç  00:00:00:00"""
        if fps is None:
            fps = self.system_parameters["frame_rate"]
        secs = Decimal(str(seconds))
        f = int((secs % 1) * fps)
        s = int(secs) % 60
        m = (int(secs) // 60) % 60
        h = int(secs) // 3600
        return f"{h:02}:{m:02}:{s:02}:{f:02}"

    def _time_to_frames(self, seconds: float, fps: int) -> int:
        """æ—¶é—´è½¬å¸§æ•°"""
        return int(float(seconds) * fps)

    def regenerate(self, context: Dict[str, Any], user_intent: Dict[str, Any]) -> Dict[str, Any]:
        """æ”¯æŒç”¨æˆ·å¹²é¢„ï¼ˆå¦‚è°ƒæ•´é¡ºåºã€åˆ é™¤ç‰‡æ®µï¼‰"""
        super().regenerate(context, user_intent)

        override = user_intent.get("timeline_override")
        if not override:
            return self.generate(context)

        result = self.generate(context)
        timeline = result["final_timeline"]

        # åˆ é™¤æŸä¸ªç‰‡æ®µ
        if "remove_clip_id" in override:
            clip_id = override["remove_clip_id"]
            for track_list in timeline["tracks"].values():
                track_list[:] = [c for c in track_list if c["id"] != clip_id]

        # è°ƒæ•´éŸ³é‡
        if "adjust_audio_volume" in override:
            vol = override["adjust_audio_volume"]
            for clip in timeline["tracks"]["audio"]:
                clip["metadata"]["volume"] = vol

        # é‡æ–°è®¡ç®—æ—¶é•¿
        all_video = timeline["tracks"]["video"]
        if all_video:
            ends = [c["end"] for c in all_video]
            timeline["project"]["duration"] = max(ends)

        return result