"""
å›¾ç‰‡+æè¿°åˆ°è§†é¢‘ç”Ÿæˆå™¨ - åŸºäºå›¾ç‰‡å’Œæè¿°ç”Ÿæˆè§†é¢‘
"""
from typing import Dict, List, Any, Optional, Tuple
import asyncio
from dataclasses import dataclass, field
from pathlib import Path
import json

from image_generation_node import ImageGenerationNode, ImageGenerationTask, ImageGenerationNodeRequest
from vgp_optimization_node import VGPOptimizationNode, VGPOptimizationConfig
from qwen_integration import QwenVideoGenerator
from video_generate_protocol.prompt_manager import get_prompt_manager


@dataclass
class ImageDescriptionToVideoRequest:
    """
    å›¾ç‰‡+æè¿°åˆ°è§†é¢‘è¯·æ±‚ä½“

    æ”¯æŒä¸¤ç§è¾“å…¥æ–¹å¼ï¼š
    1. å•å¼ å›¾ç‰‡+æ•´ä½“æè¿° â†’ ç”Ÿæˆå®Œæ•´è§†é¢‘
    2. å¤šå¼ å›¾ç‰‡+å¯¹åº”æè¿° â†’ åŸºäºåˆ†é•œç”Ÿæˆè§†é¢‘
    """

    # æ–¹å¼1ï¼šå•å¼ å›¾ç‰‡+æè¿°
    image_path: Optional[str] = None  # å•å¼ å›¾ç‰‡è·¯å¾„
    description: Optional[str] = None  # æ•´ä½“è§†é¢‘æè¿°

    # æ–¹å¼2ï¼šå¤šå¼ å›¾ç‰‡+å¤šä¸ªæè¿°ï¼ˆåˆ†é•œï¼‰
    storyboard_items: Optional[List[Dict[str, str]]] = None  # [{"image": "path", "description": "..."}, ...]

    # å…±åŒå‚æ•°
    total_duration_seconds: int = 30  # æ€»æ—¶é•¿

    # ç”Ÿæˆå‚æ•°
    product_info: Optional[Dict] = None  # äº§å“ä¿¡æ¯ï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
    style_config: Dict[str, Any] = field(default_factory=lambda: {
        "visual_style": "realistic",
        "motion_intensity": "medium",
        "transition_type": "smooth"
    })

    # é«˜çº§é€‰é¡¹
    auto_generate_intermediate_frames: bool = True  # è‡ªåŠ¨ç”Ÿæˆä¸­é—´å¸§
    use_vl_validation: bool = True  # ä½¿ç”¨VLéªŒè¯

    # è¾“å‡ºé…ç½®
    output_path: Optional[str] = None
    save_intermediate_frames: bool = False

    # VGPç³»ç»Ÿå¿…éœ€çš„IDå‚æ•°
    theme_id: Optional[str] = None
    keywords_id: Optional[str] = None
    target_duration_id: Optional[str] = None
    user_description_id: Optional[str] = None


@dataclass
class VideoSegmentPlan:
    """è§†é¢‘æ®µè½è§„åˆ’"""
    segment_id: int
    start_time_ms: int
    end_time_ms: int
    duration_ms: int

    # å…³é”®å¸§
    start_frame: Dict[str, Any]  # {"image_path": str, "description": str, "is_generated": bool}
    end_frame: Dict[str, Any]

    # ç”Ÿæˆç­–ç•¥
    generation_mode: str  # "first_last_frame" or "single_frame_extend"
    needs_generation: bool  # æ˜¯å¦éœ€è¦ç”Ÿæˆæ–°å¸§

    # æè¿°
    segment_description: str
    transition_from_previous: Optional[str] = None


class ImageDescriptionToVideoOrchestrator:
    """
    å›¾ç‰‡+æè¿°åˆ°è§†é¢‘ç¼–æ’å™¨

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. è§£æå›¾ç‰‡å’Œæè¿°ï¼Œç”Ÿæˆåˆ†é•œè®¡åˆ’
    2. æ ¹æ®æè¿°ç”Ÿæˆç¼ºå¤±çš„å…³é”®å¸§
    3. ä½¿ç”¨åƒé—®APIç”Ÿæˆ5ç§’è§†é¢‘ç‰‡æ®µ
    4. åˆå¹¶æˆæœ€ç»ˆè§†é¢‘
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config

        # åˆå§‹åŒ–å„ä¸ªèŠ‚ç‚¹
        self.image_generation_node = ImageGenerationNode(config)

        # VGPä¼˜åŒ–èŠ‚ç‚¹
        vgp_config = VGPOptimizationConfig(
            product_protection_level="maximum" if config.get("product_info") else "medium",
            prefer_wide_shots=True,
            enhance_lighting=True
        )
        self.vgp_node = VGPOptimizationNode(vgp_config)

        # åƒé—®è§†é¢‘ç”Ÿæˆå™¨
        qwen_key = config.get("qwen_api_key")
        if not qwen_key:
            raise ValueError("qwen_api_key is required")
        self.qwen_generator = QwenVideoGenerator(qwen_key)

        # å·¥ä½œç›®å½•
        self.work_dir = Path(config.get("work_dir", "/tmp/image_desc_video"))
        self.work_dir.mkdir(parents=True, exist_ok=True)

    async def process_request(self, request: ImageDescriptionToVideoRequest) -> Dict[str, Any]:
        """
        å¤„ç†å›¾ç‰‡+æè¿°åˆ°è§†é¢‘çš„è¯·æ±‚
        """

        print("\n" + "="*60)
        print("ğŸ¬ å¼€å§‹å¤„ç†å›¾ç‰‡+æè¿°åˆ°è§†é¢‘è¯·æ±‚")
        print(f"â±ï¸ ç›®æ ‡æ—¶é•¿: {request.total_duration_seconds}ç§’")
        print("="*60)

        try:
            # ç¬¬1æ­¥ï¼šè§£æè¾“å…¥ï¼Œç”Ÿæˆåˆ†é•œè®¡åˆ’
            segment_plans = await self._create_segment_plans(request)

            # ç¬¬2æ­¥ï¼šç”Ÿæˆç¼ºå¤±çš„å…³é”®å¸§
            if request.auto_generate_intermediate_frames:
                segment_plans = await self._generate_missing_frames(segment_plans, request)

            # ç¬¬3æ­¥ï¼šVLéªŒè¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if request.use_vl_validation:
                validation_result = await self._validate_frames(segment_plans)
                if not validation_result["passed"]:
                    print(f"âš ï¸ VLéªŒè¯æœªå®Œå…¨é€šè¿‡ï¼Œä½†ç»§ç»­ç”Ÿæˆ")

            # ç¬¬4æ­¥ï¼šç”Ÿæˆè§†é¢‘ç‰‡æ®µ
            video_clips = await self._generate_video_clips(segment_plans)

            # ç¬¬5æ­¥ï¼šåˆå¹¶æœ€ç»ˆè§†é¢‘
            final_video = await self._merge_final_video(
                video_clips,
                request.output_path or str(self.work_dir / "final_output.mp4")
            )

            return {
                "success": True,
                "video_path": final_video,
                "duration_seconds": request.total_duration_seconds,
                "segments_count": len(segment_plans),
                "clips_generated": len(video_clips),
                "segment_plans": [self._segment_plan_to_dict(plan) for plan in segment_plans]
            }

        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def _create_segment_plans(self, request: ImageDescriptionToVideoRequest) -> List[VideoSegmentPlan]:
        """
        ç¬¬1æ­¥ï¼šåˆ›å»ºè§†é¢‘æ®µè½è§„åˆ’
        """

        print("\n[ç¬¬1æ­¥] ğŸ“‹ åˆ›å»ºåˆ†é•œè§„åˆ’...")

        # è®¡ç®—éœ€è¦çš„5ç§’ç‰‡æ®µæ•°
        num_segments = (request.total_duration_seconds + 4) // 5
        segment_duration_ms = 5000

        segment_plans = []

        if request.image_path and request.description:
            # æ–¹å¼1ï¼šå•å¼ å›¾ç‰‡+æè¿°
            print("  æ¨¡å¼ï¼šå•å¼ å›¾ç‰‡+æè¿°")
            segment_plans = await self._create_plans_from_single_image(
                request.image_path,
                request.description,
                num_segments,
                segment_duration_ms,
                request
            )

        elif request.storyboard_items:
            # æ–¹å¼2ï¼šå¤šå¼ å›¾ç‰‡+æè¿°ï¼ˆåˆ†é•œï¼‰
            print("  æ¨¡å¼ï¼šå¤šå¼ åˆ†é•œå›¾ç‰‡")
            segment_plans = await self._create_plans_from_storyboard(
                request.storyboard_items,
                num_segments,
                segment_duration_ms,
                request
            )
        else:
            raise ValueError("å¿…é¡»æä¾› (image_path + description) æˆ– storyboard_items")

        print(f"  âœ… åˆ›å»ºäº† {len(segment_plans)} ä¸ªæ®µè½è§„åˆ’")

        return segment_plans

    async def _create_plans_from_single_image(self,
                                             image_path: str,
                                             description: str,
                                             num_segments: int,
                                             segment_duration_ms: int,
                                             request: ImageDescriptionToVideoRequest) -> List[VideoSegmentPlan]:
        """
        ä»å•å¼ å›¾ç‰‡+æè¿°åˆ›å»ºåˆ†é•œè§„åˆ’
        """

        # è§£ææè¿°ï¼Œåˆ†è§£æˆå¤šä¸ªé˜¶æ®µ
        stage_descriptions = self._parse_description_to_stages(description, num_segments)

        plans = []

        for i in range(num_segments):
            start_time = i * segment_duration_ms
            end_time = min((i + 1) * segment_duration_ms, request.total_duration_seconds * 1000)

            # ç¬¬ä¸€ä¸ªæ®µè½ä½¿ç”¨åŸå§‹å›¾ç‰‡ä½œä¸ºèµ·ç‚¹
            if i == 0:
                start_frame = {
                    "image_path": image_path,
                    "description": stage_descriptions[i]["start"],
                    "is_generated": False,
                    "is_original": True
                }
            else:
                # å¤ç”¨å‰ä¸€æ®µçš„å°¾å¸§ï¼ˆæˆ–ç”Ÿæˆæ–°çš„ï¼‰
                start_frame = {
                    "image_path": None,  # å¾…ç”Ÿæˆ
                    "description": stage_descriptions[i]["start"],
                    "is_generated": True,
                    "is_original": False,
                    "needs_generation": True
                }

            # å°¾å¸§éƒ½éœ€è¦ç”Ÿæˆ
            end_frame = {
                "image_path": None,  # å¾…ç”Ÿæˆ
                "description": stage_descriptions[i]["end"],
                "is_generated": True,
                "is_original": False,
                "needs_generation": True
            }

            plan = VideoSegmentPlan(
                segment_id=i,
                start_time_ms=start_time,
                end_time_ms=end_time,
                duration_ms=end_time - start_time,
                start_frame=start_frame,
                end_frame=end_frame,
                generation_mode="first_last_frame",
                needs_generation=True,
                segment_description=stage_descriptions[i]["description"],
                transition_from_previous=stage_descriptions[i].get("transition")
            )

            plans.append(plan)

        return plans

    async def _create_plans_from_storyboard(self,
                                           storyboard_items: List[Dict],
                                           num_segments: int,
                                           segment_duration_ms: int,
                                           request: ImageDescriptionToVideoRequest) -> List[VideoSegmentPlan]:
        """
        ä»å¤šå¼ åˆ†é•œå›¾åˆ›å»ºè§„åˆ’
        """

        plans = []

        # åˆ†é…åˆ†é•œåˆ°æ®µè½
        items_per_segment = max(1, len(storyboard_items) // num_segments)

        for i in range(num_segments):
            start_time = i * segment_duration_ms
            end_time = min((i + 1) * segment_duration_ms, request.total_duration_seconds * 1000)

            # è·å–å¯¹åº”çš„åˆ†é•œé¡¹
            start_idx = min(i * items_per_segment, len(storyboard_items) - 1)
            end_idx = min((i + 1) * items_per_segment, len(storyboard_items) - 1)

            start_item = storyboard_items[start_idx]
            end_item = storyboard_items[end_idx] if end_idx != start_idx else storyboard_items[min(start_idx + 1, len(storyboard_items) - 1)]

            # åˆ¤æ–­æ˜¯å¦éœ€è¦å¤ç”¨å¸§
            if i > 0:
                prev_plan = plans[-1]
                # æ£€æŸ¥æ˜¯å¦è¿ç»­åœºæ™¯
                if self._is_continuous_scene(prev_plan.end_frame["description"], start_item["description"]):
                    # å¤ç”¨å‰ä¸€æ®µçš„å°¾å¸§
                    start_frame = {
                        "image_path": prev_plan.end_frame["image_path"],
                        "description": start_item["description"],
                        "is_generated": False,
                        "is_reused": True
                    }
                else:
                    # ä½¿ç”¨æ–°çš„åˆ†é•œå›¾
                    start_frame = {
                        "image_path": start_item["image"],
                        "description": start_item["description"],
                        "is_generated": False,
                        "is_original": True
                    }
            else:
                # ç¬¬ä¸€æ®µ
                start_frame = {
                    "image_path": start_item["image"],
                    "description": start_item["description"],
                    "is_generated": False,
                    "is_original": True
                }

            # å°¾å¸§
            end_frame = {
                "image_path": end_item["image"] if end_item != start_item else None,
                "description": end_item["description"],
                "is_generated": end_item == start_item,  # å¦‚æœæ˜¯åŒä¸€ä¸ªé¡¹ï¼Œéœ€è¦ç”Ÿæˆ
                "needs_generation": end_item == start_item
            }

            plan = VideoSegmentPlan(
                segment_id=i,
                start_time_ms=start_time,
                end_time_ms=end_time,
                duration_ms=end_time - start_time,
                start_frame=start_frame,
                end_frame=end_frame,
                generation_mode="first_last_frame",
                needs_generation=end_frame.get("needs_generation", False),
                segment_description=f"{start_item['description']} â†’ {end_item['description']}"
            )

            plans.append(plan)

        return plans

    def _parse_description_to_stages(self, description: str, num_segments: int) -> List[Dict]:
        """
        å°†æè¿°è§£ææˆå¤šä¸ªé˜¶æ®µ
        """

        # å°è¯•æŒ‰å¥å·ã€åˆ†å·ç­‰åˆ†å‰²
        sentences = description.replace('ã€‚', '.').replace('ï¼›', ';').replace('ï¼Œ', ',').split('.')
        sentences = [s.strip() for s in sentences if s.strip()]

        stages = []

        if len(sentences) >= num_segments:
            # å¥å­å¤Ÿå¤šï¼Œç›´æ¥åˆ†é…
            for i in range(num_segments):
                stage_desc = sentences[i] if i < len(sentences) else sentences[-1]
                stages.append({
                    "description": stage_desc,
                    "start": f"Beginning of: {stage_desc}",
                    "end": f"Completion of: {stage_desc}"
                })
        else:
            # å¥å­ä¸å¤Ÿï¼Œéœ€è¦æ’å€¼
            for i in range(num_segments):
                progress = i / max(1, num_segments - 1)

                if progress < 0.3:
                    stage = "opening"
                    desc = f"Opening scene: {description[:50]}"
                elif progress < 0.7:
                    stage = "development"
                    desc = f"Developing: {description}"
                else:
                    stage = "conclusion"
                    desc = f"Concluding: {description}"

                stages.append({
                    "description": desc,
                    "start": f"{stage} - beginning",
                    "end": f"{stage} - end",
                    "transition": "smooth" if i > 0 else None
                })

        return stages

    async def _generate_missing_frames(self,
                                      segment_plans: List[VideoSegmentPlan],
                                      request: ImageDescriptionToVideoRequest) -> List[VideoSegmentPlan]:
        """
        ç¬¬2æ­¥ï¼šç”Ÿæˆç¼ºå¤±çš„å…³é”®å¸§
        """

        print("\n[ç¬¬2æ­¥] ğŸ¨ ç”Ÿæˆç¼ºå¤±çš„å…³é”®å¸§...")

        frames_to_generate = []

        # æ”¶é›†éœ€è¦ç”Ÿæˆçš„å¸§
        for plan in segment_plans:
            if plan.start_frame.get("needs_generation"):
                frames_to_generate.append(("start", plan.segment_id, plan.start_frame))
            if plan.end_frame.get("needs_generation"):
                frames_to_generate.append(("end", plan.segment_id, plan.end_frame))

        if not frames_to_generate:
            print("  âœ… æ‰€æœ‰å¸§éƒ½å·²æä¾›ï¼Œæ— éœ€ç”Ÿæˆ")
            return segment_plans

        print(f"  éœ€è¦ç”Ÿæˆ {len(frames_to_generate)} ä¸ªå…³é”®å¸§")

        # æ‰¹é‡ç”Ÿæˆ
        image_tasks = []
        for position, seg_id, frame_info in frames_to_generate:
            # æ„å»ºæç¤ºè¯ - ä½¿ç”¨PromptManager
            prompt = self._build_frame_prompt_with_manager(frame_info["description"], request)

            # ç¡®å®šå‚è€ƒå›¾åƒ
            reference_image = None
            if request.image_path:
                # ä¼˜å…ˆä½¿ç”¨æä¾›çš„äº§å“å›¾ç‰‡ä½œä¸ºå‚è€ƒ
                reference_image = request.image_path
            elif frame_info.get("image_path") and Path(frame_info["image_path"]).exists():
                # ä½¿ç”¨å·²æœ‰çš„å¸§å›¾ç‰‡
                reference_image = frame_info["image_path"]

            task = ImageGenerationTask(
                prompt=prompt,
                reference_image=reference_image,  # æ·»åŠ å‚è€ƒå›¾åƒ
                style=request.style_config.get("visual_style", "realistic"),
                quality="high",
                aspect_ratio="16:9",
                width=1920,
                height=1080
            )

            image_tasks.append({
                "task": task,
                "position": position,
                "segment_id": seg_id
            })

        # è°ƒç”¨å›¾åƒç”ŸæˆèŠ‚ç‚¹
        tasks_only = [item["task"] for item in image_tasks]

        generation_request = ImageGenerationNodeRequest(
            tasks=tasks_only,
            batch_mode=True,
            generation_config={
                "enhance_prompts": True,
                "fallback_enabled": True
            }
        )

        print("  ğŸ”„ æ‰¹é‡ç”Ÿæˆå›¾åƒ...")
        response = await self.image_generation_node.process(generation_request)

        if response.generated_images:
            print(f"  âœ… æˆåŠŸç”Ÿæˆ {len(response.generated_images)} ä¸ªå…³é”®å¸§")

            # æ›´æ–°segment_plansä¸­çš„å›¾ç‰‡è·¯å¾„
            for i, generated_image in enumerate(response.generated_images):
                task_info = image_tasks[i]
                seg_id = task_info["segment_id"]
                position = task_info["position"]

                # æ‰¾åˆ°å¯¹åº”çš„planå¹¶æ›´æ–°
                for plan in segment_plans:
                    if plan.segment_id == seg_id:
                        if position == "start":
                            plan.start_frame["image_path"] = generated_image.image_path
                            plan.start_frame["is_generated"] = True
                        else:
                            plan.end_frame["image_path"] = generated_image.image_path
                            plan.end_frame["is_generated"] = True
                        break
        else:
            print("  âŒ å›¾åƒç”Ÿæˆå¤±è´¥")

        # å¤„ç†å¸§å¤ç”¨é€»è¾‘
        segment_plans = self._apply_frame_reuse(segment_plans)

        return segment_plans

    def _apply_frame_reuse(self, segment_plans: List[VideoSegmentPlan]) -> List[VideoSegmentPlan]:
        """
        åº”ç”¨å¸§å¤ç”¨é€»è¾‘
        """

        for i in range(1, len(segment_plans)):
            curr_plan = segment_plans[i]
            prev_plan = segment_plans[i-1]

            # åˆ¤æ–­æ˜¯å¦åº”è¯¥å¤ç”¨
            if self._should_reuse_frame(prev_plan, curr_plan):
                # å¤ç”¨å‰ä¸€æ®µçš„å°¾å¸§ä½œä¸ºå½“å‰æ®µçš„é¦–å¸§
                curr_plan.start_frame["image_path"] = prev_plan.end_frame["image_path"]
                curr_plan.start_frame["is_reused"] = True
                print(f"    ğŸ”„ æ®µ{curr_plan.segment_id}å¤ç”¨æ®µ{prev_plan.segment_id}çš„å°¾å¸§")

        return segment_plans

    def _should_reuse_frame(self, prev_plan: VideoSegmentPlan, curr_plan: VideoSegmentPlan) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥å¤ç”¨å¸§
        """

        # æ£€æŸ¥æè¿°çš„è¿ç»­æ€§
        prev_desc = prev_plan.segment_description.lower()
        curr_desc = curr_plan.segment_description.lower()

        # è¿ç»­æ€§å…³é”®è¯
        continuity_keywords = ["continue", "then", "next", "follow", "progress", "develop"]

        for keyword in continuity_keywords:
            if keyword in curr_desc:
                return True

        # åœºæ™¯ç›¸ä¼¼åº¦åˆ¤æ–­
        return self._is_continuous_scene(prev_desc, curr_desc)

    def _is_continuous_scene(self, desc1: str, desc2: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦è¿ç»­åœºæ™¯
        """

        # ç®€å•çš„è¯æ±‡é‡å åˆ¤æ–­
        words1 = set(desc1.lower().split())
        words2 = set(desc2.lower().split())

        if not words1 or not words2:
            return False

        overlap = len(words1.intersection(words2))
        total = len(words1.union(words2))

        similarity = overlap / total if total > 0 else 0

        return similarity > 0.5

    async def _validate_frames(self, segment_plans: List[VideoSegmentPlan]) -> Dict:
        """
        ç¬¬3æ­¥ï¼šVLéªŒè¯
        """

        print("\n[ç¬¬3æ­¥] ğŸ‘ï¸ VLè§†è§‰éªŒè¯...")

        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„VLæ¨¡å‹
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœ

        return {
            "passed": True,
            "confidence": 0.9,
            "issues": []
        }

    async def _generate_video_clips(self, segment_plans: List[VideoSegmentPlan]) -> List[str]:
        """
        ç¬¬4æ­¥ï¼šç”Ÿæˆè§†é¢‘ç‰‡æ®µ
        """

        print("\n[ç¬¬4æ­¥] ğŸ¥ ç”Ÿæˆ5ç§’è§†é¢‘ç‰‡æ®µ...")

        video_clips = []

        for plan in segment_plans:
            print(f"  ç”Ÿæˆç‰‡æ®µ {plan.segment_id + 1}/{len(segment_plans)}")

            # ç¡®ä¿å¸§è·¯å¾„å­˜åœ¨
            if not plan.start_frame.get("image_path") or not plan.end_frame.get("image_path"):
                print(f"    âš ï¸ ç‰‡æ®µ{plan.segment_id}ç¼ºå°‘å…³é”®å¸§ï¼Œè·³è¿‡")
                continue

            # è°ƒç”¨åƒé—®APIç”Ÿæˆè§†é¢‘
            video_result = await self.qwen_generator.generate_video_from_frames(
                start_image_path=plan.start_frame["image_path"],
                end_image_path=plan.end_frame["image_path"],
                duration_seconds=plan.duration_ms / 1000
            )

            if video_result["success"]:
                # ç­‰å¾…ç”Ÿæˆå®Œæˆ
                completion = await self.qwen_generator.wait_for_completion(video_result["task_id"])

                if completion["success"]:
                    # ä¸‹è½½è§†é¢‘
                    clip_path = str(self.work_dir / f"clip_{plan.segment_id:03d}.mp4")
                    await self._download_video(completion["video_url"], clip_path)
                    video_clips.append(clip_path)
                    print(f"    âœ… ç‰‡æ®µ{plan.segment_id}ç”ŸæˆæˆåŠŸ")
                else:
                    print(f"    âŒ ç‰‡æ®µ{plan.segment_id}ç”Ÿæˆå¤±è´¥: {completion.get('error')}")
            else:
                print(f"    âŒ ç‰‡æ®µ{plan.segment_id} APIè°ƒç”¨å¤±è´¥")

        print(f"  âœ… æˆåŠŸç”Ÿæˆ {len(video_clips)} ä¸ªè§†é¢‘ç‰‡æ®µ")

        return video_clips

    async def _merge_final_video(self, video_clips: List[str], output_path: str) -> str:
        """
        ç¬¬5æ­¥ï¼šåˆå¹¶è§†é¢‘
        """

        print("\n[ç¬¬5æ­¥] ğŸ”„ åˆå¹¶æœ€ç»ˆè§†é¢‘...")

        # åˆ›å»ºffmpegæ–‡ä»¶åˆ—è¡¨
        list_file = self.work_dir / "clips.txt"
        with open(list_file, "w") as f:
            for clip_path in video_clips:
                f.write(f"file '{clip_path}'\n")

        # ä½¿ç”¨ffmpegåˆå¹¶
        import subprocess

        cmd = [
            "ffmpeg",
            "-f", "concat",
            "-safe", "0",
            "-i", str(list_file),
            "-c", "copy",
            "-y",
            output_path
        ]

        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode != 0:
            raise Exception(f"è§†é¢‘åˆå¹¶å¤±è´¥: {result.stderr}")

        print(f"  âœ… æœ€ç»ˆè§†é¢‘: {output_path}")

        return output_path

    async def _download_video(self, url: str, output_path: str):
        """ä¸‹è½½è§†é¢‘"""

        import aiohttp

        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                if response.status == 200:
                    content = await response.read()
                    with open(output_path, "wb") as f:
                        f.write(content)
                else:
                    raise Exception(f"ä¸‹è½½å¤±è´¥: {response.status}")

    def _build_frame_prompt(self, description: str, request: ImageDescriptionToVideoRequest) -> str:
        """æ„å»ºå¸§æç¤ºè¯ï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰"""
        return self._build_frame_prompt_with_manager(description, request)

    def _build_frame_prompt_with_manager(self, description: str, request: ImageDescriptionToVideoRequest) -> str:
        """ä½¿ç”¨PromptManageræ„å»ºå¸§æç¤ºè¯"""

        # è·å–PromptManager
        prompt_manager = get_prompt_manager()

        # åŸºç¡€æç¤ºè¯
        base_prompt = description

        # æ·»åŠ äº§å“ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if request.product_info:
            product_name = request.product_info.get("name", "product")
            base_prompt = f"{description}, featuring {product_name}"

        # ä½¿ç”¨PromptManagerå¢å¼ºæç¤ºè¯
        enhanced_prompt = prompt_manager.enhance_prompt(
            base_prompt,
            "frame_refinement",  # é¦–å¸§ç»†åŒ–é˜¶æ®µ
            context={
                "product": request.product_info,
                "input": description,
                "style": request.style_config
            }
        )

        return enhanced_prompt

    def _segment_plan_to_dict(self, plan: VideoSegmentPlan) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""

        return {
            "segment_id": plan.segment_id,
            "duration_ms": plan.duration_ms,
            "description": plan.segment_description,
            "start_frame": plan.start_frame,
            "end_frame": plan.end_frame,
            "generation_mode": plan.generation_mode
        }


# ä¾¿æ·API
async def generate_video_from_image_and_description(
    image_path: str,
    description: str,
    duration_seconds: int = 30,
    config: Optional[Dict] = None
) -> Dict[str, Any]:
    """
    ä¾¿æ·APIï¼šä»å›¾ç‰‡+æè¿°ç”Ÿæˆè§†é¢‘

    å‚æ•°:
        image_path: å›¾ç‰‡è·¯å¾„
        description: è§†é¢‘æè¿°
        duration_seconds: è§†é¢‘æ—¶é•¿
        config: é…ç½®

    è¿”å›:
        ç”Ÿæˆç»“æœ
    """

    if not config:
        config = {
            "qwen_api_key": "your_qwen_api_key",
            "openai_api_key": "your_openai_api_key",
            "work_dir": "/tmp/image_desc_video"
        }

    orchestrator = ImageDescriptionToVideoOrchestrator(config)

    request = ImageDescriptionToVideoRequest(
        image_path=image_path,
        description=description,
        total_duration_seconds=duration_seconds
    )

    return await orchestrator.process_request(request)


# ä½¿ç”¨ç¤ºä¾‹
async def demo():
    """æ¼”ç¤ºç”¨æ³•"""

    config = {
        "qwen_api_key": "your_actual_key",
        "openai_api_key": "your_actual_key",
        "work_dir": "/tmp/demo_video"
    }

    # ç¤ºä¾‹1ï¼šå•å¼ å›¾ç‰‡+æè¿°
    print("\nğŸ¬ ç¤ºä¾‹1: å•å¼ å›¾ç‰‡+æè¿°")
    result1 = await generate_video_from_image_and_description(
        image_path="/path/to/product.jpg",
        description="""
        å±•ç¤ºæ™ºèƒ½æ‰‹è¡¨çš„å®Œæ•´åŠŸèƒ½ã€‚
        é¦–å…ˆå±•ç¤ºå¤–è§‚è®¾è®¡ï¼Œ360åº¦æ—‹è½¬ã€‚
        ç„¶åå±•ç¤ºå±å¹•ç•Œé¢å’Œæ“ä½œã€‚
        æ¥ç€æ¼”ç¤ºè¿åŠ¨è¿½è¸ªåŠŸèƒ½ã€‚
        æœ€åå±•ç¤ºå……ç”µå’Œé…ä»¶ã€‚
        """,
        duration_seconds=20,
        config=config
    )

    if result1["success"]:
        print(f"âœ… è§†é¢‘ç”ŸæˆæˆåŠŸ: {result1['video_path']}")

    # ç¤ºä¾‹2ï¼šå¤šå¼ åˆ†é•œå›¾
    print("\nğŸ¬ ç¤ºä¾‹2: å¤šå¼ åˆ†é•œå›¾")

    orchestrator = ImageDescriptionToVideoOrchestrator(config)

    request2 = ImageDescriptionToVideoRequest(
        storyboard_items=[
            {"image": "/path/to/scene1.jpg", "description": "äº§å“æ•´ä½“å±•ç¤º"},
            {"image": "/path/to/scene2.jpg", "description": "ç»†èŠ‚ç‰¹å†™"},
            {"image": "/path/to/scene3.jpg", "description": "ä½¿ç”¨åœºæ™¯"},
            {"image": "/path/to/scene4.jpg", "description": "æœ€ç»ˆæ•ˆæœ"}
        ],
        total_duration_seconds=20,
        product_info={
            "name": "SmartWatch Pro",
            "constraints": ["ä¿æŒé¢œè‰²ä¸€è‡´"]
        }
    )

    result2 = await orchestrator.process_request(request2)

    if result2["success"]:
        print(f"âœ… åˆ†é•œè§†é¢‘ç”ŸæˆæˆåŠŸ: {result2['video_path']}")
        print(f"ğŸ“Š ç”Ÿæˆäº† {result2['segments_count']} ä¸ªç‰‡æ®µ")


if __name__ == "__main__":
    asyncio.run(demo())