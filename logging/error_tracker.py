"""
Error Tracking and Logging System
é”™è¯¯è¿½è¸ªå’Œæ—¥å¿—ç³»ç»Ÿ - æä¾›ç»Ÿä¸€çš„é”™è¯¯å¤„ç†ã€è¿½è¸ªå’Œåˆ†æåŠŸèƒ½
"""
import asyncio
import logging
import traceback
import sys
import os
import json
import uuid
from datetime import datetime, timedelta
from dataclasses import dataclass, field, asdict
from typing import Dict, List, Optional, Any, Callable, Union
from pathlib import Path
from enum import Enum
from collections import defaultdict, deque
import threading
from contextlib import contextmanager, asynccontextmanager
from functools import wraps
import inspect

from cache.redis_cache_manager import get_cache_manager


class LogLevel(Enum):
    """æ—¥å¿—çº§åˆ«"""
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"


class ErrorCategory(Enum):
    """é”™è¯¯åˆ†ç±»"""
    SYSTEM = "system"           # ç³»ç»Ÿé”™è¯¯
    APPLICATION = "application" # åº”ç”¨é”™è¯¯
    DATABASE = "database"       # æ•°æ®åº“é”™è¯¯
    NETWORK = "network"         # ç½‘ç»œé”™è¯¯
    VALIDATION = "validation"   # éªŒè¯é”™è¯¯
    PERMISSION = "permission"   # æƒé™é”™è¯¯
    RESOURCE = "resource"       # èµ„æºé”™è¯¯
    TIMEOUT = "timeout"         # è¶…æ—¶é”™è¯¯
    UNKNOWN = "unknown"         # æœªçŸ¥é”™è¯¯


@dataclass
class ErrorContext:
    """é”™è¯¯ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    request_id: Optional[str] = None
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    endpoint: Optional[str] = None
    method: Optional[str] = None
    client_ip: Optional[str] = None
    user_agent: Optional[str] = None
    additional_data: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ErrorRecord:
    """é”™è¯¯è®°å½•"""
    error_id: str
    timestamp: datetime
    level: LogLevel
    category: ErrorCategory
    message: str
    exception_type: str
    exception_message: str
    traceback_text: str
    module: str
    function: str
    line_number: int
    context: ErrorContext
    tags: List[str] = field(default_factory=list)
    count: int = 1
    first_seen: datetime = None
    last_seen: datetime = None

    def __post_init__(self):
        if self.first_seen is None:
            self.first_seen = self.timestamp
        if self.last_seen is None:
            self.last_seen = self.timestamp

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        result = asdict(self)
        result["timestamp"] = self.timestamp.isoformat()
        result["first_seen"] = self.first_seen.isoformat()
        result["last_seen"] = self.last_seen.isoformat()
        result["level"] = self.level.value
        result["category"] = self.category.value
        return result

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ErrorRecord':
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        data = data.copy()
        data["timestamp"] = datetime.fromisoformat(data["timestamp"])
        data["first_seen"] = datetime.fromisoformat(data["first_seen"])
        data["last_seen"] = datetime.fromisoformat(data["last_seen"])
        data["level"] = LogLevel(data["level"])
        data["category"] = ErrorCategory(data["category"])
        data["context"] = ErrorContext(**data["context"])
        return cls(**data)


@dataclass
class LogEntry:
    """æ—¥å¿—æ¡ç›®"""
    log_id: str
    timestamp: datetime
    level: LogLevel
    message: str
    module: str
    function: str
    line_number: int
    context: Optional[ErrorContext] = None
    extra_data: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        result = asdict(self)
        result["timestamp"] = self.timestamp.isoformat()
        result["level"] = self.level.value
        if self.context:
            result["context"] = asdict(self.context)
        return result


class CustomLogHandler(logging.Handler):
    """è‡ªå®šä¹‰æ—¥å¿—å¤„ç†å™¨"""

    def __init__(self, error_tracker):
        super().__init__()
        self.error_tracker = error_tracker

    def emit(self, record):
        """å¤„ç†æ—¥å¿—è®°å½•"""
        try:
            # è·å–è°ƒç”¨æ ˆä¿¡æ¯
            frame = sys._getframe()
            while frame:
                if frame.f_code.co_filename != __file__:
                    module = frame.f_globals.get('__name__', 'unknown')
                    function = frame.f_code.co_name
                    line_number = frame.f_lineno
                    break
                frame = frame.f_back
            else:
                module = 'unknown'
                function = 'unknown'
                line_number = 0

            # åˆ›å»ºæ—¥å¿—æ¡ç›®
            log_entry = LogEntry(
                log_id=str(uuid.uuid4()),
                timestamp=datetime.fromtimestamp(record.created),
                level=LogLevel(record.levelname),
                message=record.getMessage(),
                module=module,
                function=function,
                line_number=line_number,
                extra_data=getattr(record, 'extra_data', {})
            )

            # å¦‚æœæ˜¯é”™è¯¯çº§åˆ«ï¼Œåˆ›å»ºé”™è¯¯è®°å½•
            if record.levelno >= logging.ERROR:
                if hasattr(record, 'exc_info') and record.exc_info:
                    self.error_tracker._create_error_from_log(record, log_entry)

            # è®°å½•æ—¥å¿—
            asyncio.create_task(self.error_tracker._store_log_entry(log_entry))

        except Exception:
            self.handleError(record)


class ErrorTracker:
    """é”™è¯¯è¿½è¸ªå™¨"""

    def __init__(self,
                 log_directory: str = "/tmp/aura_render_logs",
                 max_log_files: int = 10,
                 max_file_size_mb: int = 100,
                 retention_days: int = 30):
        self.log_directory = Path(log_directory)
        self.log_directory.mkdir(parents=True, exist_ok=True)

        self.max_log_files = max_log_files
        self.max_file_size_bytes = max_file_size_mb * 1024 * 1024
        self.retention_period = timedelta(days=retention_days)

        # é”™è¯¯å­˜å‚¨
        self.error_records: Dict[str, ErrorRecord] = {}
        self.error_patterns: Dict[str, List[str]] = defaultdict(list)
        self.recent_logs: deque = deque(maxlen=1000)

        # é”™è¯¯ç»Ÿè®¡
        self.error_counts: Dict[ErrorCategory, int] = defaultdict(int)
        self.hourly_error_counts: Dict[str, int] = defaultdict(int)

        # å›è°ƒå‡½æ•°
        self.error_handlers: List[Callable] = []

        # ç¼“å­˜
        self.cache = get_cache_manager()

        # é…ç½®æ—¥å¿—è®°å½•å™¨
        self.logger = self._setup_logger()

        # å¯åŠ¨æ¸…ç†ä»»åŠ¡
        self.cleanup_task = None

    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger("aura_render")
        logger.setLevel(logging.DEBUG)

        # ç§»é™¤å·²å­˜åœ¨çš„å¤„ç†å™¨
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)

        # æ–‡ä»¶å¤„ç†å™¨ - è¯¦ç»†æ—¥å¿—
        log_file = self.log_directory / "application.log"
        file_handler = logging.handlers.RotatingFileHandler(
            log_file,
            maxBytes=self.max_file_size_bytes,
            backupCount=self.max_log_files
        )
        file_handler.setLevel(logging.DEBUG)
        file_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(funcName)s:%(lineno)d - %(message)s'
        )
        file_handler.setFormatter(file_formatter)

        # é”™è¯¯æ–‡ä»¶å¤„ç†å™¨ - ä»…é”™è¯¯
        error_file = self.log_directory / "errors.log"
        error_handler = logging.handlers.RotatingFileHandler(
            error_file,
            maxBytes=self.max_file_size_bytes,
            backupCount=self.max_log_files
        )
        error_handler.setLevel(logging.ERROR)
        error_handler.setFormatter(file_formatter)

        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_formatter = logging.Formatter(
            '%(levelname)s - %(message)s'
        )
        console_handler.setFormatter(console_formatter)

        # è‡ªå®šä¹‰å¤„ç†å™¨
        custom_handler = CustomLogHandler(self)
        custom_handler.setLevel(logging.DEBUG)

        logger.addHandler(file_handler)
        logger.addHandler(error_handler)
        logger.addHandler(console_handler)
        logger.addHandler(custom_handler)

        return logger

    def start_cleanup_task(self):
        """å¯åŠ¨æ¸…ç†ä»»åŠ¡"""
        if self.cleanup_task is None:
            self.cleanup_task = asyncio.create_task(self._cleanup_loop())

    def stop_cleanup_task(self):
        """åœæ­¢æ¸…ç†ä»»åŠ¡"""
        if self.cleanup_task:
            self.cleanup_task.cancel()
            self.cleanup_task = None

    async def _cleanup_loop(self):
        """æ¸…ç†å¾ªç¯"""
        while True:
            try:
                await self._cleanup_old_data()
                await asyncio.sleep(3600)  # æ¯å°æ—¶æ¸…ç†ä¸€æ¬¡
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"Cleanup task error: {e}")
                await asyncio.sleep(3600)

    async def _cleanup_old_data(self):
        """æ¸…ç†è¿‡æœŸæ•°æ®"""
        cutoff_time = datetime.now() - self.retention_period

        # æ¸…ç†å†…å­˜ä¸­çš„é”™è¯¯è®°å½•
        expired_errors = [
            error_id for error_id, error in self.error_records.items()
            if error.last_seen < cutoff_time
        ]

        for error_id in expired_errors:
            del self.error_records[error_id]

        # æ¸…ç†æœ€è¿‘æ—¥å¿—
        while self.recent_logs and self.recent_logs[0].timestamp < cutoff_time:
            self.recent_logs.popleft()

        # æ¸…ç†å°æ—¶ç»Ÿè®¡
        current_hour = datetime.now().strftime("%Y%m%d%H")
        expired_hours = [
            hour for hour in self.hourly_error_counts.keys()
            if hour < current_hour and
               datetime.strptime(hour, "%Y%m%d%H") < cutoff_time
        ]

        for hour in expired_hours:
            del self.hourly_error_counts[hour]

        if expired_errors or expired_hours:
            self.logger.info(f"Cleaned up {len(expired_errors)} error records and {len(expired_hours)} hourly stats")

    def track_error(self,
                   exception: Exception,
                   category: ErrorCategory = ErrorCategory.APPLICATION,
                   context: Optional[ErrorContext] = None,
                   tags: Optional[List[str]] = None) -> str:
        """è¿½è¸ªé”™è¯¯"""
        # è·å–å¼‚å¸¸ä¿¡æ¯
        exc_type = type(exception).__name__
        exc_message = str(exception)
        tb_text = ''.join(traceback.format_exception(type(exception), exception, exception.__traceback__))

        # è·å–è°ƒç”¨æ ˆä¿¡æ¯
        frame = inspect.currentframe().f_back
        module = frame.f_globals.get('__name__', 'unknown')
        function = frame.f_code.co_name
        line_number = frame.f_lineno

        # ç”Ÿæˆé”™è¯¯ç‰¹å¾ï¼ˆç”¨äºèšåˆç›¸åŒé”™è¯¯ï¼‰
        error_signature = self._generate_error_signature(exc_type, module, function, line_number)

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒé”™è¯¯
        if error_signature in self.error_records:
            error_record = self.error_records[error_signature]
            error_record.count += 1
            error_record.last_seen = datetime.now()
            error_id = error_record.error_id
        else:
            # åˆ›å»ºæ–°é”™è¯¯è®°å½•
            error_id = str(uuid.uuid4())
            error_record = ErrorRecord(
                error_id=error_id,
                timestamp=datetime.now(),
                level=LogLevel.ERROR,
                category=category,
                message=exc_message,
                exception_type=exc_type,
                exception_message=exc_message,
                traceback_text=tb_text,
                module=module,
                function=function,
                line_number=line_number,
                context=context or ErrorContext(),
                tags=tags or []
            )
            self.error_records[error_signature] = error_record

        # æ›´æ–°ç»Ÿè®¡
        self.error_counts[category] += 1
        hour_key = datetime.now().strftime("%Y%m%d%H")
        self.hourly_error_counts[hour_key] += 1

        # å­˜å‚¨åˆ°ç¼“å­˜å’Œæ—¥å¿—
        asyncio.create_task(self._store_error_record(error_record))

        # è§¦å‘é”™è¯¯å¤„ç†å›è°ƒ
        asyncio.create_task(self._trigger_error_handlers(error_record))

        # è®°å½•åˆ°ç³»ç»Ÿæ—¥å¿—
        self.logger.error(f"Error tracked: {exc_message}", exc_info=exception)

        return error_id

    def _generate_error_signature(self, exc_type: str, module: str, function: str, line_number: int) -> str:
        """ç”Ÿæˆé”™è¯¯ç‰¹å¾ç­¾å"""
        return f"{exc_type}:{module}:{function}:{line_number}"

    async def _store_error_record(self, error_record: ErrorRecord):
        """å­˜å‚¨é”™è¯¯è®°å½•"""
        try:
            # å­˜å‚¨åˆ°ç¼“å­˜
            cache_key = f"error:{error_record.error_id}"
            await self.cache.set(cache_key, error_record.to_dict(), ttl=86400)  # 24å°æ—¶

            # å­˜å‚¨åˆ°æ–‡ä»¶ï¼ˆJSONæ ¼å¼ï¼Œä¾¿äºåˆ†æï¼‰
            error_file = self.log_directory / "errors.json"

            # è¯»å–ç°æœ‰é”™è¯¯
            existing_errors = []
            if error_file.exists():
                try:
                    with open(error_file, 'r', encoding='utf-8') as f:
                        existing_errors = json.load(f)
                except (json.JSONDecodeError, FileNotFoundError):
                    existing_errors = []

            # æ·»åŠ æ–°é”™è¯¯æˆ–æ›´æ–°ç°æœ‰é”™è¯¯
            error_dict = error_record.to_dict()
            updated = False
            for i, existing_error in enumerate(existing_errors):
                if existing_error.get('error_id') == error_record.error_id:
                    existing_errors[i] = error_dict
                    updated = True
                    break

            if not updated:
                existing_errors.append(error_dict)

            # é™åˆ¶æ–‡ä»¶å¤§å°ï¼ˆä¿ç•™æœ€è¿‘çš„1000ä¸ªé”™è¯¯ï¼‰
            if len(existing_errors) > 1000:
                existing_errors = existing_errors[-1000:]

            # å†™å›æ–‡ä»¶
            with open(error_file, 'w', encoding='utf-8') as f:
                json.dump(existing_errors, f, indent=2, ensure_ascii=False)

        except Exception as e:
            self.logger.error(f"Failed to store error record: {e}")

    async def _store_log_entry(self, log_entry: LogEntry):
        """å­˜å‚¨æ—¥å¿—æ¡ç›®"""
        try:
            self.recent_logs.append(log_entry)

            # å­˜å‚¨åˆ°æ–‡ä»¶ï¼ˆç»“æ„åŒ–æ—¥å¿—ï¼‰
            log_file = self.log_directory / "structured.log"
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry.to_dict(), ensure_ascii=False) + '\n')

        except Exception as e:
            print(f"Failed to store log entry: {e}")

    async def _trigger_error_handlers(self, error_record: ErrorRecord):
        """è§¦å‘é”™è¯¯å¤„ç†å›è°ƒ"""
        for handler in self.error_handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler(error_record)
                else:
                    handler(error_record)
            except Exception as e:
                self.logger.error(f"Error in error handler: {e}")

    def _create_error_from_log(self, record, log_entry: LogEntry):
        """ä»æ—¥å¿—è®°å½•åˆ›å»ºé”™è¯¯è®°å½•"""
        if hasattr(record, 'exc_info') and record.exc_info:
            exc_type, exc_value, exc_traceback = record.exc_info
            if exc_value:
                self.track_error(
                    exc_value,
                    category=ErrorCategory.APPLICATION,
                    context=log_entry.context
                )

    def add_error_handler(self, handler: Callable):
        """æ·»åŠ é”™è¯¯å¤„ç†å›è°ƒ"""
        self.error_handlers.append(handler)

    def get_error_statistics(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–é”™è¯¯ç»Ÿè®¡"""
        cutoff_time = datetime.now() - timedelta(hours=hours)

        # è¿‡æ»¤æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„é”™è¯¯
        recent_errors = [
            error for error in self.error_records.values()
            if error.last_seen >= cutoff_time
        ]

        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        category_stats = defaultdict(int)
        for error in recent_errors:
            category_stats[error.category.value] += error.count

        # æŒ‰å°æ—¶ç»Ÿè®¡
        hourly_stats = {}
        for hour_key, count in self.hourly_error_counts.items():
            hour_time = datetime.strptime(hour_key, "%Y%m%d%H")
            if hour_time >= cutoff_time:
                hourly_stats[hour_key] = count

        # Topé”™è¯¯
        top_errors = sorted(
            recent_errors,
            key=lambda x: x.count,
            reverse=True
        )[:10]

        return {
            "time_range_hours": hours,
            "total_errors": sum(error.count for error in recent_errors),
            "unique_errors": len(recent_errors),
            "category_breakdown": dict(category_stats),
            "hourly_breakdown": hourly_stats,
            "top_errors": [
                {
                    "error_id": error.error_id,
                    "message": error.message,
                    "count": error.count,
                    "category": error.category.value,
                    "first_seen": error.first_seen.isoformat(),
                    "last_seen": error.last_seen.isoformat()
                }
                for error in top_errors
            ]
        }

    def get_recent_logs(self, count: int = 100, level: Optional[LogLevel] = None) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘æ—¥å¿—"""
        logs = list(self.recent_logs)

        if level:
            logs = [log for log in logs if log.level == level]

        # æŒ‰æ—¶é—´å€’åºæ’åˆ—
        logs.sort(key=lambda x: x.timestamp, reverse=True)

        return [log.to_dict() for log in logs[:count]]

    def search_errors(self,
                     query: Optional[str] = None,
                     category: Optional[ErrorCategory] = None,
                     start_time: Optional[datetime] = None,
                     end_time: Optional[datetime] = None) -> List[Dict[str, Any]]:
        """æœç´¢é”™è¯¯"""
        results = []

        for error in self.error_records.values():
            # æ—¶é—´è¿‡æ»¤
            if start_time and error.last_seen < start_time:
                continue
            if end_time and error.first_seen > end_time:
                continue

            # ç±»åˆ«è¿‡æ»¤
            if category and error.category != category:
                continue

            # æ–‡æœ¬æœç´¢
            if query:
                search_text = f"{error.message} {error.exception_message} {error.module} {error.function}".lower()
                if query.lower() not in search_text:
                    continue

            results.append(error.to_dict())

        # æŒ‰æœ€åå‘ç”Ÿæ—¶é—´æ’åº
        results.sort(key=lambda x: x['last_seen'], reverse=True)
        return results

    async def export_error_report(self, output_path: str, hours: int = 24):
        """å¯¼å‡ºé”™è¯¯æŠ¥å‘Š"""
        try:
            statistics = self.get_error_statistics(hours)
            recent_logs = self.get_recent_logs(200, LogLevel.ERROR)

            report = {
                "generated_at": datetime.now().isoformat(),
                "statistics": statistics,
                "recent_error_logs": recent_logs,
                "system_info": {
                    "log_directory": str(self.log_directory),
                    "retention_days": self.retention_period.days,
                    "active_error_records": len(self.error_records),
                    "recent_log_entries": len(self.recent_logs)
                }
            }

            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)

            self.logger.info(f"Error report exported to: {output_path}")

        except Exception as e:
            self.logger.error(f"Failed to export error report: {e}")
            raise

    # è£…é¥°å™¨å’Œä¸Šä¸‹æ–‡ç®¡ç†å™¨

    def track_exceptions(self, category: ErrorCategory = ErrorCategory.APPLICATION):
        """å¼‚å¸¸è¿½è¸ªè£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            async def async_wrapper(*args, **kwargs):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    self.track_error(e, category)
                    raise

            @wraps(func)
            def sync_wrapper(*args, **kwargs):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    self.track_error(e, category)
                    raise

            if asyncio.iscoroutinefunction(func):
                return async_wrapper
            else:
                return sync_wrapper
        return decorator

    @asynccontextmanager
    async def error_context(self, context: ErrorContext):
        """é”™è¯¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        # è®¾ç½®ä¸Šä¸‹æ–‡åˆ°çº¿ç¨‹å±€éƒ¨å­˜å‚¨
        import threading
        local = threading.local()
        original_context = getattr(local, 'error_context', None)
        local.error_context = context

        try:
            yield
        except Exception as e:
            self.track_error(e, context=context)
            raise
        finally:
            local.error_context = original_context


# å…¨å±€é”™è¯¯è¿½è¸ªå™¨å®ä¾‹
_global_error_tracker: Optional[ErrorTracker] = None


def get_error_tracker() -> ErrorTracker:
    """è·å–å…¨å±€é”™è¯¯è¿½è¸ªå™¨å®ä¾‹"""
    global _global_error_tracker
    if _global_error_tracker is None:
        _global_error_tracker = ErrorTracker()
        _global_error_tracker.start_cleanup_task()
    return _global_error_tracker


# ä¾¿æ·è£…é¥°å™¨
def track_errors(category: ErrorCategory = ErrorCategory.APPLICATION):
    """é”™è¯¯è¿½è¸ªè£…é¥°å™¨ï¼ˆä½¿ç”¨å…¨å±€è¿½è¸ªå™¨ï¼‰"""
    tracker = get_error_tracker()
    return tracker.track_exceptions(category)


# ä¾¿æ·æ—¥å¿—å‡½æ•°
def log_info(message: str, **kwargs):
    """è®°å½•ä¿¡æ¯æ—¥å¿—"""
    tracker = get_error_tracker()
    tracker.logger.info(message, extra={'extra_data': kwargs})


def log_warning(message: str, **kwargs):
    """è®°å½•è­¦å‘Šæ—¥å¿—"""
    tracker = get_error_tracker()
    tracker.logger.warning(message, extra={'extra_data': kwargs})


def log_error(message: str, exception: Optional[Exception] = None, **kwargs):
    """è®°å½•é”™è¯¯æ—¥å¿—"""
    tracker = get_error_tracker()
    if exception:
        tracker.logger.error(message, exc_info=exception, extra={'extra_data': kwargs})
    else:
        tracker.logger.error(message, extra={'extra_data': kwargs})


# æµ‹è¯•ä»£ç 
async def test_error_tracker():
    """æµ‹è¯•é”™è¯¯è¿½è¸ªç³»ç»Ÿ"""
    print("ğŸ› æµ‹è¯•é”™è¯¯è¿½è¸ªå’Œæ—¥å¿—ç³»ç»Ÿ")
    print("=" * 50)

    tracker = ErrorTracker(log_directory="/tmp/aura_render_logs")

    # æ·»åŠ é”™è¯¯å¤„ç†å›è°ƒ
    def error_alert_handler(error_record):
        print(f"ğŸš¨ Error Alert: {error_record.message} ({error_record.count} times)")

    tracker.add_error_handler(error_alert_handler)

    try:
        # å¯åŠ¨æ¸…ç†ä»»åŠ¡
        tracker.start_cleanup_task()

        # æµ‹è¯•åŸºæœ¬æ—¥å¿—è®°å½•
        log_info("System started", component="main")
        log_warning("High memory usage detected", memory_usage=85.5)

        # æµ‹è¯•é”™è¯¯è¿½è¸ª
        try:
            raise ValueError("Test validation error")
        except Exception as e:
            context = ErrorContext(
                request_id="req_123",
                user_id="user_456",
                endpoint="/api/test"
            )
            tracker.track_error(e, ErrorCategory.VALIDATION, context, ["test", "validation"])

        # æµ‹è¯•è£…é¥°å™¨
        @tracker.track_exceptions(ErrorCategory.APPLICATION)
        def test_function():
            raise RuntimeError("Test runtime error")

        try:
            test_function()
        except Exception:
            pass

        # æ¨¡æ‹Ÿç›¸åŒé”™è¯¯å¤šæ¬¡å‘ç”Ÿ
        for i in range(3):
            try:
                raise ConnectionError("Database connection failed")
            except Exception as e:
                tracker.track_error(e, ErrorCategory.DATABASE)

        # ç­‰å¾…ä¸€ä¸‹è®©å¼‚æ­¥æ“ä½œå®Œæˆ
        await asyncio.sleep(2)

        # è·å–é”™è¯¯ç»Ÿè®¡
        stats = tracker.get_error_statistics(24)
        print(f"\nğŸ“Š é”™è¯¯ç»Ÿè®¡:")
        print(f"  æ€»é”™è¯¯æ•°: {stats['total_errors']}")
        print(f"  å”¯ä¸€é”™è¯¯æ•°: {stats['unique_errors']}")
        print(f"  ç±»åˆ«åˆ†å¸ƒ: {stats['category_breakdown']}")

        print(f"\nğŸ”¥ Topé”™è¯¯:")
        for error in stats['top_errors'][:3]:
            print(f"  - {error['message'][:50]}... ({error['count']}æ¬¡)")

        # è·å–æœ€è¿‘æ—¥å¿—
        recent_logs = tracker.get_recent_logs(10)
        print(f"\nğŸ“‹ æœ€è¿‘æ—¥å¿—æ•°: {len(recent_logs)}")

        # å¯¼å‡ºé”™è¯¯æŠ¥å‘Š
        report_path = "/tmp/aura_render_outputs/error_report.json"
        await tracker.export_error_report(report_path, 24)
        print(f"ğŸ“„ é”™è¯¯æŠ¥å‘Šå·²å¯¼å‡º: {report_path}")

        print("\nâœ… é”™è¯¯è¿½è¸ªå’Œæ—¥å¿—ç³»ç»Ÿæµ‹è¯•å®Œæˆ")

    finally:
        tracker.stop_cleanup_task()


if __name__ == "__main__":
    # éœ€è¦å¯¼å…¥logging.handlers
    import logging.handlers
    asyncio.run(test_error_tracker())