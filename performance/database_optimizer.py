"""
Database Performance Optimizer
æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–å™¨ - æä¾›æŸ¥è¯¢ä¼˜åŒ–ã€è¿æ¥æ± ç®¡ç†å’Œæ€§èƒ½ç›‘æ§
"""
import sqlite3
import asyncio
import aiosqlite
import time
import threading
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from contextlib import asynccontextmanager, contextmanager
from concurrent.futures import ThreadPoolExecutor
import logging
import json

from cache.redis_cache_manager import get_cache_manager, cache_result


@dataclass
class QueryStats:
    """æŸ¥è¯¢ç»Ÿè®¡"""
    query_hash: str
    sql: str
    execution_count: int = 0
    total_time: float = 0.0
    avg_time: float = 0.0
    min_time: float = float('inf')
    max_time: float = 0.0
    last_executed: datetime = field(default_factory=datetime.now)
    rows_affected: int = 0

    def add_execution(self, execution_time: float, rows: int = 0):
        """æ·»åŠ æ‰§è¡Œè®°å½•"""
        self.execution_count += 1
        self.total_time += execution_time
        self.avg_time = self.total_time / self.execution_count
        self.min_time = min(self.min_time, execution_time)
        self.max_time = max(self.max_time, execution_time)
        self.last_executed = datetime.now()
        self.rows_affected += rows


@dataclass
class ConnectionPoolStats:
    """è¿æ¥æ± ç»Ÿè®¡"""
    total_connections: int = 0
    active_connections: int = 0
    idle_connections: int = 0
    peak_connections: int = 0
    connection_errors: int = 0
    total_queries: int = 0
    avg_query_time: float = 0.0


class SQLiteOptimizer:
    """SQLiteæ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self, db_path: str, max_connections: int = 10):
        self.db_path = db_path
        self.max_connections = max_connections
        self.query_stats: Dict[str, QueryStats] = {}
        self.connection_stats = ConnectionPoolStats()
        self.logger = logging.getLogger(__name__)

        # è¿æ¥æ± 
        self._connection_pool = []
        self._pool_lock = threading.Lock()
        self._active_connections = set()

        # æ€§èƒ½ä¼˜åŒ–è®¾ç½®
        self.enable_query_cache = True
        self.cache_ttl = 300  # 5åˆ†é’Ÿ
        self.slow_query_threshold = 1.0  # 1ç§’

        # åˆå§‹åŒ–ä¼˜åŒ–
        self._init_database_optimizations()

    def _init_database_optimizations(self):
        """åˆå§‹åŒ–æ•°æ®åº“ä¼˜åŒ–è®¾ç½®"""
        optimization_queries = [
            # å¯ç”¨WALæ¨¡å¼ä»¥æé«˜å¹¶å‘æ€§èƒ½
            "PRAGMA journal_mode = WAL;",

            # ä¼˜åŒ–åŒæ­¥è®¾ç½®
            "PRAGMA synchronous = NORMAL;",

            # å¢åŠ ç¼“å­˜å¤§å°ï¼ˆä»¥KBä¸ºå•ä½ï¼‰
            "PRAGMA cache_size = -64000;",  # 64MB

            # å¯ç”¨å†…å­˜æ˜ å°„
            "PRAGMA mmap_size = 134217728;",  # 128MB

            # ä¼˜åŒ–ä¸´æ—¶å­˜å‚¨
            "PRAGMA temp_store = MEMORY;",

            # è®¾ç½®é¡µé¢å¤§å°
            "PRAGMA page_size = 4096;",

            # å¯ç”¨å¤–é”®çº¦æŸ
            "PRAGMA foreign_keys = ON;",

            # åˆ†æç»Ÿè®¡ä¿¡æ¯
            "PRAGMA optimize;",
        ]

        try:
            with sqlite3.connect(self.db_path) as conn:
                for query in optimization_queries:
                    conn.execute(query)
                conn.commit()

            self.logger.info("Database optimizations applied successfully")
        except Exception as e:
            self.logger.error(f"Failed to apply database optimizations: {e}")

    @contextmanager
    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        conn = None
        try:
            with self._pool_lock:
                if self._connection_pool:
                    conn = self._connection_pool.pop()
                else:
                    conn = sqlite3.connect(
                        self.db_path,
                        check_same_thread=False,
                        timeout=10.0
                    )
                    # ä¸ºæ¯ä¸ªè¿æ¥åº”ç”¨ä¼˜åŒ–è®¾ç½®
                    conn.execute("PRAGMA journal_mode = WAL;")
                    conn.execute("PRAGMA synchronous = NORMAL;")
                    conn.execute("PRAGMA cache_size = -8000;")  # 8MB per connection
                    conn.execute("PRAGMA foreign_keys = ON;")

                self._active_connections.add(conn)
                self.connection_stats.active_connections += 1
                self.connection_stats.total_connections = max(
                    self.connection_stats.total_connections,
                    len(self._active_connections)
                )
                self.connection_stats.peak_connections = max(
                    self.connection_stats.peak_connections,
                    self.connection_stats.active_connections
                )

            yield conn

        except Exception as e:
            self.connection_stats.connection_errors += 1
            self.logger.error(f"Database connection error: {e}")
            raise
        finally:
            if conn:
                with self._pool_lock:
                    self._active_connections.discard(conn)
                    self.connection_stats.active_connections -= 1

                    if len(self._connection_pool) < self.max_connections:
                        self._connection_pool.append(conn)
                    else:
                        conn.close()

    async def execute_query(self, sql: str, params: Tuple = (),
                           fetch: str = "none") -> Any:
        """æ‰§è¡ŒæŸ¥è¯¢ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        query_hash = hash(sql)
        start_time = time.time()

        try:
            # æ£€æŸ¥ç¼“å­˜ï¼ˆå¯¹äºSELECTæŸ¥è¯¢ï¼‰
            if self.enable_query_cache and sql.strip().upper().startswith('SELECT'):
                cache = get_cache_manager()
                cache_key = f"query:{query_hash}:{hash(params)}"
                cached_result = await cache.get(cache_key)
                if cached_result is not None:
                    return cached_result

            # æ‰§è¡ŒæŸ¥è¯¢
            result = await self._execute_with_stats(sql, params, fetch, query_hash)

            # ç¼“å­˜ç»“æœï¼ˆå¯¹äºSELECTæŸ¥è¯¢ï¼‰
            if (self.enable_query_cache and
                sql.strip().upper().startswith('SELECT') and
                result is not None):
                cache_key = f"query:{query_hash}:{hash(params)}"
                await cache.set(cache_key, result, self.cache_ttl)

            return result

        except Exception as e:
            self.logger.error(f"Query execution failed: {e}\nSQL: {sql}")
            raise
        finally:
            execution_time = time.time() - start_time
            self._record_query_stats(query_hash, sql, execution_time)

    async def _execute_with_stats(self, sql: str, params: Tuple,
                                 fetch: str, query_hash: str) -> Any:
        """æ‰§è¡ŒæŸ¥è¯¢å¹¶æ”¶é›†ç»Ÿè®¡ä¿¡æ¯"""
        loop = asyncio.get_event_loop()

        def sync_execute():
            with self.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(sql, params)

                if fetch == "all":
                    return cursor.fetchall()
                elif fetch == "one":
                    return cursor.fetchone()
                elif fetch == "many":
                    return cursor.fetchmany()
                else:
                    conn.commit()
                    return cursor.rowcount

        result = await loop.run_in_executor(None, sync_execute)

        self.connection_stats.total_queries += 1
        return result

    def _record_query_stats(self, query_hash: str, sql: str, execution_time: float):
        """è®°å½•æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯"""
        if query_hash not in self.query_stats:
            self.query_stats[query_hash] = QueryStats(
                query_hash=str(query_hash),
                sql=sql[:200] + "..." if len(sql) > 200 else sql
            )

        stats = self.query_stats[query_hash]
        stats.add_execution(execution_time)

        # è®°å½•æ…¢æŸ¥è¯¢
        if execution_time > self.slow_query_threshold:
            self.logger.warning(
                f"Slow query detected: {execution_time:.3f}s\n"
                f"SQL: {sql[:100]}..."
            )

        # æ›´æ–°è¿æ¥æ± ç»Ÿè®¡
        if self.connection_stats.total_queries > 0:
            total_time = sum(stat.total_time for stat in self.query_stats.values())
            self.connection_stats.avg_query_time = (
                total_time / self.connection_stats.total_queries
            )

    async def execute_batch(self, sql: str, params_list: List[Tuple]) -> int:
        """æ‰¹é‡æ‰§è¡ŒæŸ¥è¯¢"""
        start_time = time.time()

        try:
            loop = asyncio.get_event_loop()

            def sync_batch_execute():
                with self.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.executemany(sql, params_list)
                    conn.commit()
                    return cursor.rowcount

            result = await loop.run_in_executor(None, sync_batch_execute)

            execution_time = time.time() - start_time
            query_hash = hash(sql)
            self._record_query_stats(query_hash, f"BATCH: {sql}", execution_time)

            return result

        except Exception as e:
            self.logger.error(f"Batch execution failed: {e}\nSQL: {sql}")
            raise

    def create_index(self, table: str, columns: List[str],
                    unique: bool = False, if_not_exists: bool = True):
        """åˆ›å»ºç´¢å¼•"""
        index_name = f"idx_{table}_{'_'.join(columns)}"
        unique_clause = "UNIQUE " if unique else ""
        if_not_exists_clause = "IF NOT EXISTS " if if_not_exists else ""

        sql = (f"CREATE {unique_clause}INDEX {if_not_exists_clause}"
               f"{index_name} ON {table} ({', '.join(columns)})")

        try:
            with self.get_connection() as conn:
                conn.execute(sql)
                conn.commit()
                self.logger.info(f"Index created: {index_name}")
        except Exception as e:
            self.logger.error(f"Failed to create index {index_name}: {e}")

    def analyze_table(self, table: str):
        """åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯"""
        try:
            with self.get_connection() as conn:
                conn.execute(f"ANALYZE {table}")
                conn.commit()
                self.logger.info(f"Table analyzed: {table}")
        except Exception as e:
            self.logger.error(f"Failed to analyze table {table}: {e}")

    async def vacuum_database(self):
        """æ¸…ç†æ•°æ®åº“"""
        start_time = time.time()

        try:
            loop = asyncio.get_event_loop()

            def sync_vacuum():
                with self.get_connection() as conn:
                    conn.execute("VACUUM")
                    conn.commit()

            await loop.run_in_executor(None, sync_vacuum)

            execution_time = time.time() - start_time
            self.logger.info(f"Database vacuumed in {execution_time:.2f}s")

        except Exception as e:
            self.logger.error(f"Vacuum failed: {e}")

    def get_query_performance_report(self) -> Dict[str, Any]:
        """è·å–æŸ¥è¯¢æ€§èƒ½æŠ¥å‘Š"""
        # æŒ‰æ‰§è¡Œæ¬¡æ•°æ’åºçš„çƒ­é—¨æŸ¥è¯¢
        hot_queries = sorted(
            self.query_stats.values(),
            key=lambda x: x.execution_count,
            reverse=True
        )[:10]

        # æŒ‰å¹³å‡æ‰§è¡Œæ—¶é—´æ’åºçš„æ…¢æŸ¥è¯¢
        slow_queries = sorted(
            self.query_stats.values(),
            key=lambda x: x.avg_time,
            reverse=True
        )[:10]

        # æŒ‰æ€»æ‰§è¡Œæ—¶é—´æ’åºçš„è€—æ—¶æŸ¥è¯¢
        time_consuming_queries = sorted(
            self.query_stats.values(),
            key=lambda x: x.total_time,
            reverse=True
        )[:10]

        return {
            "connection_pool": {
                "total_connections": self.connection_stats.total_connections,
                "active_connections": self.connection_stats.active_connections,
                "peak_connections": self.connection_stats.peak_connections,
                "connection_errors": self.connection_stats.connection_errors,
                "total_queries": self.connection_stats.total_queries,
                "avg_query_time": self.connection_stats.avg_query_time
            },
            "hot_queries": [
                {
                    "sql": q.sql,
                    "execution_count": q.execution_count,
                    "avg_time": q.avg_time,
                    "total_time": q.total_time
                } for q in hot_queries
            ],
            "slow_queries": [
                {
                    "sql": q.sql,
                    "avg_time": q.avg_time,
                    "max_time": q.max_time,
                    "execution_count": q.execution_count
                } for q in slow_queries
            ],
            "time_consuming_queries": [
                {
                    "sql": q.sql,
                    "total_time": q.total_time,
                    "execution_count": q.execution_count,
                    "avg_time": q.avg_time
                } for q in time_consuming_queries
            ],
            "cache_enabled": self.enable_query_cache,
            "cache_ttl": self.cache_ttl
        }

    def optimize_materials_database(self):
        """ä¼˜åŒ–ç´ ææ•°æ®åº“çš„ç‰¹å®šè®¾ç½®"""
        optimizations = [
            # ä¸ºç´ æè¡¨åˆ›å»ºå¸¸ç”¨ç´¢å¼•
            ("materials", ["media_type"], False),
            ("materials", ["created_at"], False),
            ("materials", ["file_size"], False),
            ("materials", ["material_id"], True),  # å”¯ä¸€ç´¢å¼•

            # åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯
            "materials"
        ]

        try:
            for optimization in optimizations:
                if isinstance(optimization, tuple):
                    table, columns, unique = optimization
                    self.create_index(table, columns, unique)
                else:
                    self.analyze_table(optimization)

            self.logger.info("Materials database optimizations completed")

        except Exception as e:
            self.logger.error(f"Materials database optimization failed: {e}")

    async def get_database_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“ä¿¡æ¯"""
        info_queries = [
            ("database_size", "SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()"),
            ("table_count", "SELECT COUNT(*) FROM sqlite_master WHERE type='table'"),
            ("index_count", "SELECT COUNT(*) FROM sqlite_master WHERE type='index'"),
            ("journal_mode", "PRAGMA journal_mode"),
            ("cache_size", "PRAGMA cache_size"),
            ("synchronous", "PRAGMA synchronous"),
        ]

        info = {}
        for name, sql in info_queries:
            try:
                result = await self.execute_query(sql, fetch="one")
                info[name] = result[0] if result else None
            except Exception as e:
                info[name] = f"Error: {e}"

        return info

    def close(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        with self._pool_lock:
            for conn in self._connection_pool:
                conn.close()
            for conn in self._active_connections.copy():
                conn.close()

            self._connection_pool.clear()
            self._active_connections.clear()


# å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹
_global_db_optimizer: Optional[SQLiteOptimizer] = None


def get_db_optimizer(db_path: str = None) -> SQLiteOptimizer:
    """è·å–å…¨å±€æ•°æ®åº“ä¼˜åŒ–å™¨å®ä¾‹"""
    global _global_db_optimizer
    if _global_db_optimizer is None and db_path:
        _global_db_optimizer = SQLiteOptimizer(db_path)
    return _global_db_optimizer


def optimize_materials_db(db_path: str) -> SQLiteOptimizer:
    """ä¼˜åŒ–ç´ ææ•°æ®åº“"""
    optimizer = SQLiteOptimizer(db_path)
    optimizer.optimize_materials_database()
    return optimizer


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
async def test_database_optimizer():
    """æµ‹è¯•æ•°æ®åº“ä¼˜åŒ–å™¨"""
    print("ğŸ§ª æµ‹è¯•æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–å™¨")
    print("=" * 50)

    # åˆ›å»ºæµ‹è¯•æ•°æ®åº“
    test_db_path = "/tmp/test_optimizer.db"

    # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    optimizer = SQLiteOptimizer(test_db_path)

    try:
        # åˆ›å»ºæµ‹è¯•è¡¨
        await optimizer.execute_query("""
            CREATE TABLE IF NOT EXISTS test_materials (
                id INTEGER PRIMARY KEY,
                name TEXT NOT NULL,
                type TEXT NOT NULL,
                size INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        print("âœ… æµ‹è¯•è¡¨åˆ›å»ºæˆåŠŸ")

        # æ’å…¥æµ‹è¯•æ•°æ®
        test_data = [
            (f"material_{i}", "video" if i % 2 == 0 else "audio", i * 1000)
            for i in range(100)
        ]

        await optimizer.execute_batch(
            "INSERT INTO test_materials (name, type, size) VALUES (?, ?, ?)",
            test_data
        )

        print(f"âœ… æ’å…¥ {len(test_data)} æ¡æµ‹è¯•æ•°æ®")

        # åˆ›å»ºç´¢å¼•
        optimizer.create_index("test_materials", ["type"])
        optimizer.create_index("test_materials", ["size"])

        print("âœ… åˆ›å»ºç´¢å¼•å®Œæˆ")

        # æ‰§è¡Œä¸€äº›æµ‹è¯•æŸ¥è¯¢
        queries = [
            ("SELECT COUNT(*) FROM test_materials", "one"),
            ("SELECT * FROM test_materials WHERE type = ?", "all", ("video",)),
            ("SELECT AVG(size) FROM test_materials", "one"),
            ("SELECT * FROM test_materials ORDER BY size DESC LIMIT 10", "all"),
        ]

        for i, query_info in enumerate(queries):
            if len(query_info) == 3:
                sql, fetch, params = query_info
            else:
                sql, fetch = query_info
                params = ()

            result = await optimizer.execute_query(sql, params, fetch)
            print(f"  æŸ¥è¯¢ {i+1}: è¿”å› {len(result) if isinstance(result, list) else 1} è¡Œ")

        # åˆ†æè¡¨ç»Ÿè®¡
        optimizer.analyze_table("test_materials")

        # è·å–æ€§èƒ½æŠ¥å‘Š
        performance_report = optimizer.get_query_performance_report()

        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"  æ€»æŸ¥è¯¢æ•°: {performance_report['connection_pool']['total_queries']}")
        print(f"  å¹³å‡æŸ¥è¯¢æ—¶é—´: {performance_report['connection_pool']['avg_query_time']:.4f}s")
        print(f"  å³°å€¼è¿æ¥æ•°: {performance_report['connection_pool']['peak_connections']}")
        print(f"  çƒ­é—¨æŸ¥è¯¢æ•°: {len(performance_report['hot_queries'])}")

        # è·å–æ•°æ®åº“ä¿¡æ¯
        db_info = await optimizer.get_database_info()
        print(f"\nğŸ”§ æ•°æ®åº“ä¿¡æ¯:")
        for key, value in db_info.items():
            print(f"  {key}: {value}")

        print("\nâœ… æ•°æ®åº“ä¼˜åŒ–å™¨æµ‹è¯•å®Œæˆ")

    finally:
        optimizer.close()

        # æ¸…ç†æµ‹è¯•æ•°æ®åº“
        import os
        try:
            os.remove(test_db_path)
            # æ¸…ç†WALå’ŒSHMæ–‡ä»¶
            for suffix in ['-wal', '-shm']:
                try:
                    os.remove(test_db_path + suffix)
                except FileNotFoundError:
                    pass
        except FileNotFoundError:
            pass


if __name__ == "__main__":
    asyncio.run(test_database_optimizer())