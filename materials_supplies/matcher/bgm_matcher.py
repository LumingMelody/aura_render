# matcher/bgm_matcher.py
from materials_supplies.models import BGMRequest, BGMResponse
import random
from typing import List

async def match_bgm(request: BGMRequest) -> List[BGMResponse]:
    # æ¨¡æ‹Ÿä» Java è·å– BGM å€™é€‰
    candidates = [
        {"url": "https://audio.com/tech-bgm.mp3", "duration": 120.0},
        {"url": "https://audio.com/epic-bgm.mp3", "duration": 90.0}
    ]

    results = []
    for c in candidates:
        duration = c["duration"]
        if duration >= request.duration:
            cut_start = random.uniform(0, duration - request.duration)
            cut_end = cut_start + request.duration
        else:
            cut_start = 0.0
            cut_end = duration

        results.append(BGMResponse(
            url=c["url"],
            cut_start=cut_start,
            cut_end=cut_end,
            duration=c["duration"]
        ))
    return results


    def generate(self, context: Dict[str, Any]) -> Dict[str, Any]:
        super().generate(context)

        shot_blocks: List[Dict] = context["shot_blocks"]
        target_duration: float = context["target_duration"]

        # --- 1. è‡ªåŠ¨åˆ†æåˆ†é•œ â†’ æƒ…æ„Ÿã€èŠ‚å¥ã€ä¸»é¢˜ ---
        analysis = self._analyze_shot_blocks(shot_blocks)
        primary_mood = analysis["primary_mood"]
        avg_bpm_hint = analysis["estimated_bpm"]
        dominant_theme = analysis["dominant_theme"]

        print(f"ğŸ” åˆ†æç»“æœ: ä¸»æƒ…ç»ª={primary_mood}, å»ºè®®BPM={avg_bpm_hint:.1f}, ä¸»é¢˜={dominant_theme}")

        # --- 2. ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·ä¸Šä¼ éŸ³é¢‘ ---
        if self.uploaded_files:
            for file_info in self.uploaded_files:
                if file_info["type"] == "audio" and file_info["filename"].endswith((".mp3", ".wav", ".aiff")):
                    bgm_track = self._create_custom_track(file_info["path"], target_duration)
                    return {"bgm_track": bgm_track}

        # --- 3. è°ƒç”¨éŸ³ä¹ API è·å–å€™é€‰ ---
        candidate_tracks = self._fetch_matching_tracks(
            mood=primary_mood,
            bpm=avg_bpm_hint,
            duration=target_duration,
            theme=dominant_theme
        )

        if not candidate_tracks:
            print("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…éŸ³ä¹ï¼Œå°è¯•é€šç”¨â€˜åŠ±å¿—â€™ç±»éŸ³ä¹...")
            candidate_tracks = self._fetch_matching_tracks(mood="åŠ±å¿—", bpm=100, duration=target_duration)

        if not candidate_tracks:
            print("âš ï¸ ä»æ— ç»“æœï¼Œä½¿ç”¨é»˜è®¤é™éŸ³æˆ–æç¤ºéŸ³")
            return {
                "bgm_track": {
                    "source": "fallback",
                    "title": "No BGM Available",
                    "duration": target_duration,
                    "file_path": None,
                    "volume_db": -20.0
                }
            }

        # --- 4. é€‰è¯„åˆ†æœ€é«˜çš„ ---
        best_track = max(candidate_tracks, key=lambda t: t.get("match_score", 0.5))
        bgm_track = self._create_library_track(best_track, target_duration)

        return {"bgm_track": bgm_track}

    def _analyze_shot_blocks(self, shots: List[Dict]) -> Dict[str, Any]:
        """ä»åˆ†é•œä¸­æ¨æ–­æƒ…ç»ªã€BPMã€ä¸»é¢˜"""
        total_duration = 0.0
        mood_counter = {}
        pacing_weights = {"å¿«": 1.8, "å¸¸è§„": 1.0, "æ…¢é•œå¤´": 0.6}
        theme_counter = {}

        for shot in shots:
            duration = shot.get("duration", 5.0)
            total_duration += duration

            # 1. æƒ…ç»ªæ¨æ–­ï¼ˆåŸºäº shot_type + visual_description + caption å…³é”®è¯ï¼‰
            mood = self._infer_mood_from_text(shot)
            mood_counter[mood] = mood_counter.get(mood, 0) + duration * MOOD_WEIGHTS.get(mood, 0.5)

            # 2. ä¸»é¢˜æ¨æ–­ï¼ˆæ•™è‚²ã€ç§‘æŠ€ã€ç”Ÿæ´»ã€åŠ±å¿—ç­‰ï¼‰
            theme = self._infer_theme_from_text(shot)
            theme_counter[theme] = theme_counter.get(theme, 0) + duration

            # 3. èŠ‚å¥æƒé‡ï¼ˆç”¨äºBPMä¼°ç®—ï¼‰
            pacing = shot.get("pacing", "å¸¸è§„")
            speed_factor = pacing_weights.get(pacing, 1.0)

        # ä¸»æƒ…ç»ª
        primary_mood = max(mood_counter, key=mood_counter.get) if mood_counter else "å†·é™"

        # ä¸»é¢˜
        dominant_theme = max(theme_counter, key=theme_counter.get) if theme_counter else "é€šç”¨"

        # BPM ä¼°ç®—ï¼šåŸºäºå‰ªè¾‘å¯†åº¦ Ã— èŠ‚å¥å› å­
        cuts_per_minute = (len(shots) / (total_duration or 1)) * 60
        estimated_bpm = cuts_per_minute * 4  # æ¯å°èŠ‚4æ‹
        estimated_bpm *= pacing_weights.get("å¸¸è§„", 1.0)  # å¯åŠ å…¥ pacing è°ƒæ•´

        return {
            "primary_mood": primary_mood,
            "estimated_bpm": round(estimated_bpm, 1),
            "dominant_theme": dominant_theme,
            "mood_dist": {k: v / sum(mood_counter.values()) for k, v in mood_counter.items()}
        }

    def _infer_mood_from_text(self, shot: Dict) -> str:
        """åŸºäºæ–‡æœ¬å…³é”®è¯æ¨æ–­æƒ…ç»ª"""
        text = f"{shot.get('visual_description', '')} {shot.get('caption', '')} {shot.get('shot_type', '')}"
        text_lower = text.lower()

        mood_keywords = {
            "æ¿€æ˜‚": ["æ¿€æƒ…", "æ¿€åŠ¨", "é«˜æ½®", "çªç ´", "æŒ‘æˆ˜"],
            "åŠ±å¿—": ["åŠ æ²¹", "ä½ å¯ä»¥", "åšæŒ", "æ¢¦æƒ³", "åŠªåŠ›", "æ—…ç¨‹", "å¼€å¯"],
            "æ„ŸåŠ¨": ["æ„ŸåŠ¨", "å›å¿†", "æ¸©æš–", "é™ªä¼´", "æˆé•¿"],
            "æ¸©é¦¨": ["æ¸©é¦¨", "å®¶åº­", "å¾®ç¬‘", "æ˜äº®", "æ•´æ´"],
            "å¹½é»˜": ["æç¬‘", "æ»‘ç¨½", "è°ƒçš®", "ç¬‘"],
            "å†·é™": ["åˆ†æ", "æ•°æ®", "é€»è¾‘", "æ€è€ƒ", "ç™½æ¿"],
            "æ‚¬ç–‘": ["ç§˜å¯†", "æœªçŸ¥", "æ¢ç´¢", "é»‘å½±"],
            "ç§‘æŠ€": ["AI", "æœºå™¨å­¦ä¹ ", "ä»£ç ", "ç”µè„‘", "ç®—æ³•", "é¡¹ç›®", "æŠ€æœ¯"]
        }

        for mood, keywords in mood_keywords.items():
            if any(k in text_lower for k in keywords):
                return mood
        return "å†·é™"  # é»˜è®¤

    def _infer_theme_from_text(self, shot: Dict) -> str:
        """æ¨æ–­ä¸»é¢˜ï¼ˆå¯ç”¨äºAPIè¿‡æ»¤ï¼‰"""
        text = f"{shot.get('visual_description', '')} {shot.get('caption', '')}".lower()
        if "å­¦ä¹ " in text or "è¯¾ç¨‹" in text or "å­¦ç”Ÿ" in text or "æ•™è‚²" in text:
            return "æ•™è‚²"
        elif "ç§‘æŠ€" in text or "AI" in text or "æœºå™¨å­¦ä¹ " in text or "ç¼–ç¨‹" in text:
            return "ç§‘æŠ€"
        elif "å®¶åº­" in text or "å®¶" in text or "ç”Ÿæ´»" in text:
            return "ç”Ÿæ´»"
        elif "è¿åŠ¨" in text or "æ¯”èµ›" in text:
            return "è¿åŠ¨"
        return "é€šç”¨"

    def _fetch_matching_tracks(self, mood: str, bpm: float, duration: float, theme: str = "é€šç”¨") -> List[Dict]:
        """è°ƒç”¨å¤–éƒ¨éŸ³ä¹APIæœç´¢åŒ¹é…æ›²ç›®"""
        try:
            response = requests.post(MUSIC_SEARCH_API, json={
                "mood": mood,
                "bpm": bpm,
                "bpm_tolerance": BPM_TOLERANCE,
                "duration": duration,
                "duration_tolerance": 10.0,
                "genre_hint": theme,
                "limit": 10
            }, timeout=5)

            if response.status_code == 200:
                tracks_data = response.json().get("tracks", [])
                candidates = []
                for item in tracks_data:
                    # è®¡ç®—åŒ¹é…åº¦è¯„åˆ†
                    bpm_diff = abs(item["bpm"] - bpm)
                    bpm_match = 1.0 if bpm_diff <= BPM_TOLERANCE else max(0, 1 - bpm_diff / 20)
                    mood_match = 1.0 if mood in item.get("mood", []) else 0.4

                    stretch_ratio = duration / item["duration"]
                    if stretch_ratio < 0.8 or stretch_ratio > 1.3:
                        continue

                    score = (mood_match * 0.6 + bpm_match * 0.4)

                    if score >= self.system_parameters["min_match_score"]:
                        item["match_score"] = score
                        item["stretch_ratio"] = stretch_ratio
                        candidates.append(item)
                return candidates
            else:
                print(f"âŒ APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
                return []

        except Exception as e:
            print(f"âš ï¸  è¯·æ±‚éŸ³ä¹APIå‡ºé”™: {e}")
            return []

    def _create_library_track(self, track: Dict, target_duration: float) -> Dict:
        """åˆ›å»ºåº“å†…éŸ³ä¹çš„è½¨é“é…ç½®"""
        stretch_ratio = target_duration / track["duration"]

        return {
            "source": "library_api",
            "track_id": track["id"],
            "title": track["title"],
            "file_path": track["file_path"],
            "original_bpm": track["bpm"],
            "applied_bpm": track["bpm"],
            "duration": target_duration,
            "stretch_ratio": round(stretch_ratio, 3),
            "fade_in": DEFAULT_FADE_IN,
            "fade_out": max(DEFAULT_FADE_OUT, target_duration * 0.05),
            "volume_db": self.system_parameters["default_volume"],
            "processing": {
                "pitch_preserved": True,
                "time_stretch": True
            },
            "metadata": {
                "genre": track.get("genre", []),
                "mood": track.get("mood", []),
                "key": track.get("key", "N/A"),
                "has_bass_drop": track.get("has_bass_drop", False)
            }
        }

    def _create_custom_track(self, file_path: str, target_duration: float) -> Dict:
        """åˆ›å»ºè‡ªå®šä¹‰éŸ³é¢‘è½¨é“"""
        return {
            "source": "custom",
            "title": "ç”¨æˆ·ä¸Šä¼ BGM",
            "file_path": file_path,
            "duration": target_duration,
            "stretch_ratio": 1.0,
            "fade_in": DEFAULT_FADE_IN,
            "fade_out": max(DEFAULT_FADE_OUT, target_duration * 0.05),
            "volume_db": self.system_parameters["default_volume"],
            "processing": {
                "pitch_preserved": True,
                "time_stretch": True
            },
            "metadata": {},
            "anchor_processing": []
        }
