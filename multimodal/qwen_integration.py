#!/usr/bin/env python3
"""
Qwenæ¨¡å‹é›†æˆ
é›†æˆQwenå’ŒQwenVLè¿›è¡Œè§†é¢‘å†…å®¹ç†è§£
- Qwen: åŸºäºYOLOç‰¹å¾çš„è½»é‡çº§ç†è§£
- QwenVL: å…³é”®å¸§çš„æ·±åº¦è§†è§‰ç†è§£
"""

import asyncio
import logging
import os
import json
from typing import Dict, List, Any, Optional, Tuple
import cv2
import numpy as np
from pathlib import Path

# Qwenç›¸å…³å¯¼å…¥
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
    HAS_QWEN = True
except ImportError:
    HAS_QWEN = False
    print("è­¦å‘Š: æœªå®‰è£…transformers/Qwenï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå®ç°")

# YOLOç›¸å…³å¯¼å…¥
try:
    from ultralytics import YOLO
    HAS_YOLO = True
except ImportError:
    HAS_YOLO = False
    print("è­¦å‘Š: æœªå®‰è£…ultralytics/YOLOï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå®ç°")

logger = logging.getLogger(__name__)


class YOLOFeatureExtractor:
    """YOLOç‰¹å¾æå–å™¨ - ä¸ºQwenæä¾›ç»“æ„åŒ–ç‰¹å¾"""

    def __init__(self, offline_mode: bool = None):
        self.model = None
        # é»˜è®¤ç¦ç”¨æœ¬åœ°æ¨¡å‹ï¼Œé¿å…å¯åŠ¨æ—¶åŠ è½½å¤±è´¥
        self.offline_mode = offline_mode or os.environ.get('HAS_VL_MODELS', 'false') == 'false' or True

        if not self.offline_mode and HAS_YOLO:
            try:
                self.model = YOLO('yolov8n.pt')  # é€šç”¨æ£€æµ‹æ¨¡å‹
                logger.info("YOLOç‰¹å¾æå–å™¨åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"YOLOæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.model = None
        else:
            logger.info("YOLOç‰¹å¾æå–å™¨è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼")

    async def extract_video_features(self, video_path: str, sample_rate: int = 5) -> Dict[str, Any]:
        """
        æå–è§†é¢‘çš„ç»“æ„åŒ–ç‰¹å¾

        Args:
            video_path: è§†é¢‘è·¯å¾„
            sample_rate: é‡‡æ ·ç‡ï¼ˆæ¯Nå¸§å–ä¸€å¸§ï¼‰

        Returns:
            ç»“æ„åŒ–ç‰¹å¾ä¿¡æ¯
        """
        if self.model and Path(video_path).exists():
            return await self._extract_real_features(video_path, sample_rate)
        else:
            return await self._extract_mock_features(video_path, sample_rate)

    async def _extract_real_features(self, video_path: str, sample_rate: int) -> Dict[str, Any]:
        """ä½¿ç”¨çœŸå®YOLOæ¨¡å‹æå–ç‰¹å¾"""
        try:
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = total_frames / fps if fps > 0 else 0

            features = {
                'basic_info': {
                    'duration': duration,
                    'fps': fps,
                    'total_frames': total_frames,
                    'resolution': f"{int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))}x{int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))}"
                },
                'detected_objects': {},  # {class_name: [timestamps]}
                'scene_changes': [],     # åœºæ™¯å˜åŒ–æ—¶é—´ç‚¹
                'object_counts': {},     # æ¯ç±»å¯¹è±¡çš„ç»Ÿè®¡
                'temporal_patterns': {}, # æ—¶åºæ¨¡å¼
                'key_moments': []        # å…³é”®æ—¶åˆ»
            }

            frame_count = 0
            prev_objects = set()

            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                if frame_count % sample_rate == 0:
                    current_time = frame_count / fps if fps > 0 else frame_count / 30

                    # YOLOæ£€æµ‹
                    results = self.model(frame)
                    current_objects = set()

                    for result in results:
                        boxes = result.boxes
                        if boxes is not None:
                            for box in boxes:
                                class_id = int(box.cls)
                                confidence = float(box.conf)
                                class_name = result.names[class_id]

                                if confidence > 0.5:
                                    current_objects.add(class_name)

                                    # è®°å½•å¯¹è±¡å‡ºç°æ—¶é—´
                                    if class_name not in features['detected_objects']:
                                        features['detected_objects'][class_name] = []
                                    features['detected_objects'][class_name].append({
                                        'timestamp': current_time,
                                        'confidence': confidence
                                    })

                    # æ£€æµ‹åœºæ™¯å˜åŒ–
                    if prev_objects and current_objects != prev_objects:
                        features['scene_changes'].append({
                            'timestamp': current_time,
                            'objects_added': list(current_objects - prev_objects),
                            'objects_removed': list(prev_objects - current_objects)
                        })

                    prev_objects = current_objects

                frame_count += 1

            cap.release()

            # ç»Ÿè®¡å¯¹è±¡è®¡æ•°
            for obj_class, detections in features['detected_objects'].items():
                features['object_counts'][obj_class] = len(detections)

            # ç”Ÿæˆæ–‡æœ¬æè¿°
            features['text_description'] = self._generate_feature_description(features)

            logger.info(f"æå–äº†{len(features['detected_objects'])}ç±»å¯¹è±¡ï¼Œ{len(features['scene_changes'])}ä¸ªåœºæ™¯å˜åŒ–")
            return features

        except Exception as e:
            logger.error(f"çœŸå®ç‰¹å¾æå–å¤±è´¥: {e}")
            return await self._extract_mock_features(video_path, sample_rate)

    async def _extract_mock_features(self, video_path: str, sample_rate: int) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿç‰¹å¾æå–"""
        return {
            'basic_info': {
                'duration': 30.0,
                'fps': 30,
                'total_frames': 900,
                'resolution': '1920x1080'
            },
            'detected_objects': {
                'person': [
                    {'timestamp': 2.0, 'confidence': 0.92},
                    {'timestamp': 15.0, 'confidence': 0.88},
                    {'timestamp': 25.0, 'confidence': 0.95}
                ],
                'laptop': [
                    {'timestamp': 5.0, 'confidence': 0.85},
                    {'timestamp': 10.0, 'confidence': 0.90}
                ],
                'phone': [
                    {'timestamp': 20.0, 'confidence': 0.78}
                ]
            },
            'scene_changes': [
                {
                    'timestamp': 10.0,
                    'objects_added': ['laptop'],
                    'objects_removed': []
                },
                {
                    'timestamp': 20.0,
                    'objects_added': ['phone'],
                    'objects_removed': ['laptop']
                }
            ],
            'object_counts': {
                'person': 3,
                'laptop': 2,
                'phone': 1
            },
            'temporal_patterns': {
                'main_subject': 'person',
                'interaction_objects': ['laptop', 'phone']
            },
            'key_moments': [2.0, 10.0, 20.0],
            'text_description': "è§†é¢‘åŒ…å«äººç‰©ä¸ç”µå­è®¾å¤‡çš„äº¤äº’åœºæ™¯ï¼Œä¸»è¦å¯¹è±¡åŒ…æ‹¬äºº(3æ¬¡å‡ºç°)ã€ç¬”è®°æœ¬ç”µè„‘(2æ¬¡å‡ºç°)ã€æ‰‹æœº(1æ¬¡å‡ºç°)ã€‚åœ¨10ç§’å’Œ20ç§’å¤„æœ‰æ˜æ˜¾çš„åœºæ™¯å˜åŒ–ã€‚"
        }

    def _generate_feature_description(self, features: Dict[str, Any]) -> str:
        """ç”Ÿæˆç‰¹å¾çš„æ–‡æœ¬æè¿°"""
        desc_parts = []

        # åŸºç¡€ä¿¡æ¯
        basic = features['basic_info']
        desc_parts.append(f"è§†é¢‘æ—¶é•¿{basic['duration']:.1f}ç§’ï¼Œåˆ†è¾¨ç‡{basic['resolution']}")

        # ä¸»è¦å¯¹è±¡
        if features['detected_objects']:
            obj_desc = []
            for obj_class, count in features['object_counts'].items():
                obj_desc.append(f"{obj_class}({count}æ¬¡å‡ºç°)")
            desc_parts.append(f"ä¸»è¦å¯¹è±¡åŒ…æ‹¬{', '.join(obj_desc)}")

        # åœºæ™¯å˜åŒ–
        if features['scene_changes']:
            change_times = [f"{change['timestamp']:.1f}ç§’" for change in features['scene_changes']]
            desc_parts.append(f"åœ¨{', '.join(change_times)}å¤„æœ‰åœºæ™¯å˜åŒ–")

        return "ã€‚".join(desc_parts) + "ã€‚"


class QwenTextAnalyzer:
    """Qwenæ–‡æœ¬åˆ†æå™¨ - åŸºäºYOLOç‰¹å¾è¿›è¡Œè½»é‡çº§ç†è§£"""

    def __init__(self, model_name: str = "Qwen/Qwen2-1.5B-Instruct", offline_mode: bool = None):
        self.model = None
        self.tokenizer = None
        self.model_name = model_name
        # é»˜è®¤ç¦ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä½¿ç”¨APIè°ƒç”¨æ¨¡å¼
        self.offline_mode = offline_mode or os.environ.get('HAS_VL_MODELS', 'false') == 'false' or True

        if not self.offline_mode and HAS_QWEN:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype="auto",
                    device_map="auto"
                )
                logger.info(f"Qwenæ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"Qwenæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.model = None
        else:
            logger.info("Qwenåˆ†æå™¨è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼")

    async def analyze_video_from_features(self, features: Dict[str, Any],
                                        analysis_type: str = "content") -> Dict[str, Any]:
        """
        åŸºäºYOLOç‰¹å¾åˆ†æè§†é¢‘å†…å®¹

        Args:
            features: YOLOæå–çš„ç‰¹å¾
            analysis_type: åˆ†æç±»å‹ (content/style/action/emotion)

        Returns:
            åˆ†æç»“æœ
        """
        if self.model:
            return await self._analyze_with_qwen(features, analysis_type)
        else:
            return await self._analyze_mock(features, analysis_type)

    async def _analyze_with_qwen(self, features: Dict[str, Any], analysis_type: str) -> Dict[str, Any]:
        """ä½¿ç”¨çœŸå®Qwenæ¨¡å‹åˆ†æ"""
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_analysis_prompt(features, analysis_type)

            # ç”Ÿæˆå›å¤
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æå¸ˆï¼ŒåŸºäºæä¾›çš„è§†é¢‘ç‰¹å¾ä¿¡æ¯è¿›è¡Œåˆ†æã€‚"},
                {"role": "user", "content": prompt}
            ]

            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

            generated_ids = self.model.generate(
                model_inputs.input_ids,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True
            )

            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]

            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

            # è§£æå“åº”
            return self._parse_qwen_response(response, analysis_type)

        except Exception as e:
            logger.error(f"Qwenåˆ†æå¤±è´¥: {e}")
            return await self._analyze_mock(features, analysis_type)

    async def _analyze_mock(self, features: Dict[str, Any], analysis_type: str) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿåˆ†æç»“æœ"""
        base_result = {
            'analysis_type': analysis_type,
            'confidence': 0.85,
            'processing_method': 'yolo_features + qwen_mock'
        }

        if analysis_type == "content":
            return {**base_result, **{
                'main_theme': 'ç§‘æŠ€äº§å“æ¼”ç¤º',
                'key_elements': ['äººç‰©', 'ç”µå­è®¾å¤‡', 'æ“ä½œæ¼”ç¤º'],
                'content_type': 'product_demo',
                'target_audience': 'tech_users',
                'description': 'è¿™æ˜¯ä¸€ä¸ªç§‘æŠ€äº§å“çš„æ¼”ç¤ºè§†é¢‘ï¼Œå±•ç¤ºäº†äººç‰©ä¸ç”µå­è®¾å¤‡çš„äº¤äº’è¿‡ç¨‹'
            }}
        elif analysis_type == "style":
            return {**base_result, **{
                'visual_style': 'modern_tech',
                'pace': 'moderate',
                'composition': 'centered',
                'color_tone': 'neutral',
                'lighting': 'professional'
            }}
        elif analysis_type == "action":
            return {**base_result, **{
                'main_actions': ['äº§å“å±•ç¤º', 'åŠŸèƒ½æ¼”ç¤º', 'æ“ä½œæŒ‡å¯¼'],
                'interaction_level': 'medium',
                'movement_pattern': 'sequential',
                'key_moments': features.get('key_moments', [])
            }}
        elif analysis_type == "emotion":
            return {**base_result, **{
                'overall_mood': 'professional',
                'emotional_arc': 'stable_positive',
                'engagement_level': 'moderate',
                'tone': 'informative'
            }}

    def _build_analysis_prompt(self, features: Dict[str, Any], analysis_type: str) -> str:
        """æ„å»ºåˆ†ææç¤ºè¯"""
        base_prompt = f"""
è¯·åŸºäºä»¥ä¸‹è§†é¢‘ç‰¹å¾ä¿¡æ¯è¿›è¡Œ{analysis_type}åˆ†æï¼š

åŸºç¡€ä¿¡æ¯ï¼š
- è§†é¢‘æ—¶é•¿ï¼š{features['basic_info']['duration']:.1f}ç§’
- åˆ†è¾¨ç‡ï¼š{features['basic_info']['resolution']}

æ£€æµ‹åˆ°çš„å¯¹è±¡ï¼š
"""

        for obj_class, detections in features['detected_objects'].items():
            base_prompt += f"- {obj_class}: {len(detections)}æ¬¡å‡ºç°\n"

        base_prompt += f"\nåœºæ™¯å˜åŒ–ï¼š{len(features['scene_changes'])}å¤„\n"
        base_prompt += f"\nç‰¹å¾æè¿°ï¼š{features.get('text_description', '')}\n"

        if analysis_type == "content":
            base_prompt += """
è¯·åˆ†æï¼š
1. è§†é¢‘çš„ä¸»è¦å†…å®¹å’Œä¸»é¢˜
2. å…³é”®å…ƒç´ å’Œé‡ç‚¹
3. å†…å®¹ç±»å‹åˆ†ç±»
4. ç›®æ ‡å—ä¼—ç¾¤ä½“
5. ç®€çŸ­æè¿°

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼ŒåŒ…å«ï¼šmain_theme, key_elements, content_type, target_audience, description
"""
        elif analysis_type == "style":
            base_prompt += """
è¯·åˆ†æï¼š
1. è§†è§‰é£æ ¼ç‰¹ç‚¹
2. èŠ‚å¥å’ŒèŠ‚æ‹
3. æ„å›¾ç‰¹ç‚¹
4. è‰²è°ƒé£æ ¼
5. å…‰çº¿æ•ˆæœ

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼ŒåŒ…å«ï¼švisual_style, pace, composition, color_tone, lighting
"""

        return base_prompt

    def _parse_qwen_response(self, response: str, analysis_type: str) -> Dict[str, Any]:
        """è§£æQwenå“åº”"""
        try:
            # å°è¯•æå–JSON
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                result['confidence'] = 0.9
                result['processing_method'] = 'yolo_features + qwen_real'
                return result
        except:
            pass

        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸºäºå“åº”æ–‡æœ¬çš„åˆ†æ
        return {
            'analysis_type': analysis_type,
            'raw_response': response,
            'confidence': 0.7,
            'processing_method': 'yolo_features + qwen_text',
            'summary': response[:200] + "..." if len(response) > 200 else response
        }


class QwenVLKeyFrameAnalyzer:
    """QwenVLå…³é”®å¸§åˆ†æå™¨ - æ·±åº¦è§†è§‰ç†è§£å•å¸§å†…å®¹"""

    def __init__(self, model_name: str = "Qwen/Qwen2-VL-2B-Instruct", offline_mode: bool = None):
        self.model = None
        self.processor = None
        self.model_name = model_name
        # é»˜è®¤ç¦ç”¨æœ¬åœ°QwenVLæ¨¡å‹ï¼Œä½¿ç”¨APIè°ƒç”¨æ¨¡å¼
        self.offline_mode = offline_mode or os.environ.get('HAS_VL_MODELS', 'false') == 'false' or True

        if not self.offline_mode and HAS_QWEN:
            try:
                # QwenVLæ¨¡å‹åŠ è½½
                self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                    model_name,
                    torch_dtype="auto",
                    device_map="auto"
                )
                self.processor = AutoProcessor.from_pretrained(model_name)
                logger.info(f"QwenVLæ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"QwenVLæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.model = None
        else:
            logger.info("QwenVLåˆ†æå™¨è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼")

    async def analyze_key_frames(self, video_path: str, key_timestamps: List[float],
                               analysis_focus: str = "detailed") -> Dict[str, Any]:
        """
        åˆ†æå…³é”®å¸§çš„è¯¦ç»†å†…å®¹

        Args:
            video_path: è§†é¢‘è·¯å¾„
            key_timestamps: å…³é”®æ—¶é—´ç‚¹åˆ—è¡¨
            analysis_focus: åˆ†æé‡ç‚¹ (detailed/product/person/scene)

        Returns:
            å…³é”®å¸§åˆ†æç»“æœ
        """
        if self.model and Path(video_path).exists():
            return await self._analyze_real_frames(video_path, key_timestamps, analysis_focus)
        else:
            return await self._analyze_mock_frames(video_path, key_timestamps, analysis_focus)

    async def _analyze_real_frames(self, video_path: str, key_timestamps: List[float],
                                 analysis_focus: str) -> Dict[str, Any]:
        """ä½¿ç”¨çœŸå®QwenVLæ¨¡å‹åˆ†æå…³é”®å¸§"""
        try:
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)

            frame_analyses = []

            for timestamp in key_timestamps:
                # å®šä½åˆ°æŒ‡å®šæ—¶é—´ç‚¹
                frame_number = int(timestamp * fps)
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)

                ret, frame = cap.read()
                if not ret:
                    continue

                # è½¬æ¢ä¸ºRGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # æ„å»ºåˆ†ææç¤º
                prompt = self._build_vl_prompt(analysis_focus, timestamp)

                # QwenVLæ¨ç†
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "image": frame_rgb},
                            {"type": "text", "text": prompt}
                        ]
                    }
                ]

                text = self.processor.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )

                image_inputs, video_inputs = process_vision_info(messages)
                inputs = self.processor(
                    text=[text],
                    images=image_inputs,
                    videos=video_inputs,
                    padding=True,
                    return_tensors="pt",
                ).to(self.model.device)

                generated_ids = self.model.generate(**inputs, max_new_tokens=512)
                generated_ids_trimmed = [
                    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                ]

                response = self.processor.batch_decode(
                    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
                )[0]

                # è§£æåˆ†æç»“æœ
                frame_analysis = self._parse_vl_response(response, timestamp, analysis_focus)
                frame_analyses.append(frame_analysis)

            cap.release()

            # ç»¼åˆåˆ†æç»“æœ
            return self._synthesize_frame_analyses(frame_analyses, analysis_focus)

        except Exception as e:
            logger.error(f"QwenVLå…³é”®å¸§åˆ†æå¤±è´¥: {e}")
            return await self._analyze_mock_frames(video_path, key_timestamps, analysis_focus)

    async def _analyze_mock_frames(self, video_path: str, key_timestamps: List[float],
                                 analysis_focus: str) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿå…³é”®å¸§åˆ†æ"""
        frame_analyses = []

        for i, timestamp in enumerate(key_timestamps):
            frame_analysis = {
                'timestamp': timestamp,
                'frame_id': f"frame_{int(timestamp)}s",
                'analysis_focus': analysis_focus,
                'confidence': 0.82,
                'processing_method': 'qwenvl_mock'
            }

            if analysis_focus == "detailed":
                frame_analysis.update({
                    'scene_description': f"ç¬¬{int(timestamp)}ç§’çš„è¯¦ç»†åœºæ™¯ï¼šä¸“ä¸šçš„äº§å“å±•ç¤ºç¯å¢ƒ",
                    'objects_detailed': [
                        {'object': 'äººç‰©', 'position': 'center', 'action': 'å±•ç¤ºäº§å“', 'confidence': 0.95},
                        {'object': 'äº§å“', 'position': 'right', 'state': 'è¢«æ“ä½œä¸­', 'confidence': 0.90}
                    ],
                    'visual_quality': {
                        'lighting': 'professional',
                        'composition': 'rule_of_thirds',
                        'focus': 'sharp',
                        'color_balance': 'neutral'
                    },
                    'content_analysis': {
                        'main_subject': 'äº§å“æ¼”ç¤º',
                        'secondary_elements': ['ç”¨æˆ·ç•Œé¢', 'æ“ä½œæ‰‹åŠ¿'],
                        'emotional_tone': 'professional_confident'
                    }
                })
            elif analysis_focus == "product":
                frame_analysis.update({
                    'product_details': {
                        'product_type': 'electronic_device',
                        'brand_visible': True,
                        'condition': 'new',
                        'usage_context': 'demonstration'
                    },
                    'product_features': ['å±å¹•æ˜¾ç¤º', 'æ“ä½œç•Œé¢', 'å¤–è§‚è®¾è®¡'],
                    'market_positioning': 'premium_consumer'
                })

            frame_analyses.append(frame_analysis)

        return self._synthesize_frame_analyses(frame_analyses, analysis_focus)

    def _build_vl_prompt(self, analysis_focus: str, timestamp: float) -> str:
        """æ„å»ºVLåˆ†ææç¤ºè¯"""
        base_prompt = f"è¯·è¯¦ç»†åˆ†æè¿™å¼ å›¾ç‰‡ï¼ˆè§†é¢‘ç¬¬{timestamp:.1f}ç§’çš„å…³é”®å¸§ï¼‰ï¼š"

        if analysis_focus == "detailed":
            return base_prompt + """
è¯·æä¾›ï¼š
1. æ•´ä½“åœºæ™¯æè¿°
2. æ‰€æœ‰å¯è§å¯¹è±¡çš„è¯¦ç»†ä¿¡æ¯ï¼ˆä½ç½®ã€çŠ¶æ€ã€åŠ¨ä½œï¼‰
3. è§†è§‰è´¨é‡è¯„ä¼°ï¼ˆå…‰çº¿ã€æ„å›¾ã€ç„¦ç‚¹ã€è‰²å½©ï¼‰
4. å†…å®¹åˆ†æï¼ˆä¸»é¢˜ã€æƒ…æ„Ÿè‰²è°ƒã€é‡è¦å…ƒç´ ï¼‰

è¯·ç”¨ç»“æ„åŒ–çš„æ–¹å¼å›ç­”ã€‚
"""
        elif analysis_focus == "product":
            return base_prompt + """
é‡ç‚¹åˆ†æå›¾ç‰‡ä¸­çš„äº§å“ï¼š
1. äº§å“ç±»å‹å’Œç‰¹å¾
2. å“ç‰Œè¯†åˆ«å’Œæ ‡è¯†
3. äº§å“çŠ¶æ€å’Œä½¿ç”¨æƒ…å†µ
4. å¸‚åœºå®šä½åˆ¤æ–­
5. äº§å“äº®ç‚¹ç‰¹è‰²

è¯·è¯¦ç»†æè¿°äº§å“ç›¸å…³ä¿¡æ¯ã€‚
"""
        elif analysis_focus == "person":
            return base_prompt + """
é‡ç‚¹åˆ†æå›¾ç‰‡ä¸­çš„äººç‰©ï¼š
1. äººç‰©æ•°é‡å’Œä½ç½®
2. åŠ¨ä½œå’Œå§¿æ€
3. è¡¨æƒ…å’Œæƒ…ç»ª
4. ç©¿ç€å’Œå½¢è±¡
5. ä¸ç¯å¢ƒçš„äº’åŠ¨

è¯·è¯¦ç»†æè¿°äººç‰©ç›¸å…³ä¿¡æ¯ã€‚
"""
        else:  # scene
            return base_prompt + """
é‡ç‚¹åˆ†æåœºæ™¯ç¯å¢ƒï¼š
1. åœºæ™¯ç±»å‹å’Œè®¾ç½®
2. ç¯å¢ƒå¸ƒç½®å’Œé“å…·
3. ç©ºé—´å¸ƒå±€å’Œæ„å›¾
4. æ°›å›´å’Œé£æ ¼
5. ä¸“ä¸šç¨‹åº¦è¯„ä¼°

è¯·è¯¦ç»†æè¿°åœºæ™¯ç›¸å…³ä¿¡æ¯ã€‚
"""

    def _parse_vl_response(self, response: str, timestamp: float, analysis_focus: str) -> Dict[str, Any]:
        """è§£æQwenVLå“åº”"""
        return {
            'timestamp': timestamp,
            'analysis_focus': analysis_focus,
            'raw_response': response,
            'confidence': 0.88,
            'processing_method': 'qwenvl_real',
            'detailed_analysis': response[:500] + "..." if len(response) > 500 else response
        }

    def _synthesize_frame_analyses(self, frame_analyses: List[Dict[str, Any]],
                                 analysis_focus: str) -> Dict[str, Any]:
        """ç»¼åˆå¤šä¸ªå…³é”®å¸§çš„åˆ†æç»“æœ"""
        return {
            'analysis_type': 'qwenvl_keyframe',
            'analysis_focus': analysis_focus,
            'total_frames_analyzed': len(frame_analyses),
            'frame_analyses': frame_analyses,
            'overall_confidence': np.mean([fa.get('confidence', 0) for fa in frame_analyses]),
            'processing_method': 'qwenvl_keyframe_synthesis',
            'synthesis': {
                'consistent_elements': self._find_consistent_elements(frame_analyses),
                'temporal_changes': self._identify_temporal_changes(frame_analyses),
                'overall_theme': self._determine_overall_theme(frame_analyses, analysis_focus)
            }
        }

    def _find_consistent_elements(self, frame_analyses: List[Dict[str, Any]]) -> List[str]:
        """æ‰¾å‡ºè·¨å¸§ä¸€è‡´çš„å…ƒç´ """
        # ç®€åŒ–å®ç°
        return ['professional_setting', 'product_focus', 'clean_composition']

    def _identify_temporal_changes(self, frame_analyses: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ—¶åºå˜åŒ–"""
        changes = []
        for i in range(1, len(frame_analyses)):
            changes.append({
                'from_timestamp': frame_analyses[i-1]['timestamp'],
                'to_timestamp': frame_analyses[i]['timestamp'],
                'change_type': 'scene_transition',
                'description': 'åœºæ™¯æˆ–ç„¦ç‚¹çš„è½¬æ¢'
            })
        return changes

    def _determine_overall_theme(self, frame_analyses: List[Dict[str, Any]], analysis_focus: str) -> str:
        """ç¡®å®šæ•´ä½“ä¸»é¢˜"""
        if analysis_focus == "product":
            return "ä¸“ä¸šäº§å“å±•ç¤ºå’Œæ¼”ç¤º"
        elif analysis_focus == "person":
            return "äººç‰©å¼•å¯¼çš„å†…å®¹å‘ˆç°"
        elif analysis_focus == "scene":
            return "ç²¾å¿ƒå¸ƒç½®çš„æ¼”ç¤ºç¯å¢ƒ"
        else:
            return "ç»¼åˆæ€§å†…å®¹å±•ç¤º"


class HybridVideoUnderstanding:
    """æ··åˆè§†é¢‘ç†è§£ç³»ç»Ÿ - æ•´åˆYOLO+Qwenå’ŒQwenVL"""

    def __init__(self, offline_mode: bool = None):
        self.yolo_extractor = YOLOFeatureExtractor(offline_mode)
        self.qwen_analyzer = QwenTextAnalyzer(offline_mode=offline_mode)
        self.qwenvl_analyzer = QwenVLKeyFrameAnalyzer(offline_mode=offline_mode)
        self.offline_mode = offline_mode or os.environ.get('HAS_VL_MODELS') == 'false'
        self.logger = logger.getChild('HybridVideoUnderstanding')

    async def understand_video(self, video_path: str,
                             understanding_level: str = "balanced") -> Dict[str, Any]:
        """
        ç»¼åˆè§†é¢‘ç†è§£

        Args:
            video_path: è§†é¢‘è·¯å¾„
            understanding_level: ç†è§£çº§åˆ«
                - "lightweight": ä»…ä½¿ç”¨YOLO+Qwen
                - "detailed": ä»…ä½¿ç”¨QwenVLå…³é”®å¸§
                - "balanced": æ··åˆä½¿ç”¨
                - "comprehensive": å…¨é¢åˆ†æ

        Returns:
            ç»¼åˆç†è§£ç»“æœ
        """
        self.logger.info(f"å¼€å§‹{understanding_level}çº§åˆ«çš„è§†é¢‘ç†è§£: {video_path}")

        result = {
            'video_path': video_path,
            'understanding_level': understanding_level,
            'timestamp': asyncio.get_event_loop().time(),
            'processing_methods': []
        }

        if understanding_level in ["lightweight", "balanced", "comprehensive"]:
            # YOLOç‰¹å¾æå– + Qwenåˆ†æ
            self.logger.info("æ‰§è¡ŒYOLOç‰¹å¾æå–...")
            features = await self.yolo_extractor.extract_video_features(video_path)
            result['yolo_features'] = features
            result['processing_methods'].append('yolo_features')

            self.logger.info("æ‰§è¡ŒQwenå†…å®¹åˆ†æ...")
            content_analysis = await self.qwen_analyzer.analyze_video_from_features(features, "content")
            style_analysis = await self.qwen_analyzer.analyze_video_from_features(features, "style")

            result['qwen_analysis'] = {
                'content': content_analysis,
                'style': style_analysis
            }
            result['processing_methods'].append('qwen_analysis')

        if understanding_level in ["detailed", "balanced", "comprehensive"]:
            # é€‰æ‹©å…³é”®å¸§è¿›è¡ŒQwenVLåˆ†æ
            if 'yolo_features' in result and result['yolo_features']['key_moments']:
                key_timestamps = result['yolo_features']['key_moments'][:3]  # æœ€å¤šåˆ†æ3ä¸ªå…³é”®å¸§
            else:
                # é»˜è®¤é€‰æ‹©å¼€å¤´ã€ä¸­é—´ã€ç»“å°¾
                duration = result.get('yolo_features', {}).get('basic_info', {}).get('duration', 30)
                key_timestamps = [duration * 0.1, duration * 0.5, duration * 0.9]

            self.logger.info(f"æ‰§è¡ŒQwenVLå…³é”®å¸§åˆ†æ: {key_timestamps}")
            detailed_analysis = await self.qwenvl_analyzer.analyze_key_frames(
                video_path, key_timestamps, "detailed"
            )
            result['qwenvl_analysis'] = detailed_analysis
            result['processing_methods'].append('qwenvl_keyframe')

        # å¦‚æœæ˜¯comprehensiveçº§åˆ«ï¼Œé¢å¤–è¿›è¡Œä¸“é—¨åˆ†æ
        if understanding_level == "comprehensive":
            self.logger.info("æ‰§è¡Œç»¼åˆä¸“é¡¹åˆ†æ...")

            # äº§å“ä¸“é¡¹åˆ†æ
            if 'yolo_features' in result:
                key_timestamps = result['yolo_features']['key_moments'][:2]
                product_analysis = await self.qwenvl_analyzer.analyze_key_frames(
                    video_path, key_timestamps, "product"
                )
                result['specialized_analysis'] = {
                    'product_focus': product_analysis
                }
                result['processing_methods'].append('specialized_analysis')

        # ç”Ÿæˆç»¼åˆç†è§£æ‘˜è¦
        result['understanding_summary'] = self._generate_understanding_summary(result)

        self.logger.info(f"è§†é¢‘ç†è§£å®Œæˆï¼Œä½¿ç”¨æ–¹æ³•: {result['processing_methods']}")
        return result

    def _generate_understanding_summary(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆç†è§£æ‘˜è¦"""
        summary = {
            'video_type': 'unknown',
            'main_content': 'unknown',
            'style_characteristics': [],
            'key_elements': [],
            'overall_quality': 'medium',
            'recommendation_score': 0.5,
            'processing_confidence': 0.0
        }

        # ä»Qwenåˆ†æä¸­æå–ä¿¡æ¯
        if 'qwen_analysis' in analysis_result:
            qwen_content = analysis_result['qwen_analysis'].get('content', {})
            qwen_style = analysis_result['qwen_analysis'].get('style', {})

            summary['video_type'] = qwen_content.get('content_type', 'unknown')
            summary['main_content'] = qwen_content.get('main_theme', 'unknown')
            summary['key_elements'] = qwen_content.get('key_elements', [])

            if qwen_style:
                summary['style_characteristics'] = [
                    qwen_style.get('visual_style', 'unknown'),
                    qwen_style.get('pace', 'unknown'),
                    qwen_style.get('lighting', 'unknown')
                ]

        # ä»QwenVLåˆ†æä¸­è¡¥å……ä¿¡æ¯
        if 'qwenvl_analysis' in analysis_result:
            qwenvl = analysis_result['qwenvl_analysis']
            summary['overall_quality'] = 'high'  # QwenVLåˆ†æè¡¨æ˜è´¨é‡è¾ƒé«˜
            summary['processing_confidence'] = qwenvl.get('overall_confidence', 0.8)

        # è®¡ç®—æ¨èåˆ†æ•°
        confidence_scores = []
        for method in analysis_result['processing_methods']:
            if 'qwen' in method:
                confidence_scores.append(0.8)
            elif 'qwenvl' in method:
                confidence_scores.append(0.9)
            elif 'yolo' in method:
                confidence_scores.append(0.7)

        if confidence_scores:
            summary['recommendation_score'] = np.mean(confidence_scores)
            summary['processing_confidence'] = np.mean(confidence_scores)

        return summary


# å…¨å±€å®ä¾‹
hybrid_video_understanding = HybridVideoUnderstanding()


# è¾…åŠ©å‡½æ•°ï¼ˆQwenVLéœ€è¦çš„ï¼‰
def process_vision_info(messages):
    """å¤„ç†è§†è§‰ä¿¡æ¯çš„è¾…åŠ©å‡½æ•°"""
    image_inputs = []
    video_inputs = []

    for message in messages:
        if isinstance(message.get("content"), list):
            for content_item in message["content"]:
                if content_item.get("type") == "image" and "image" in content_item:
                    image_inputs.append(content_item["image"])

    return image_inputs, video_inputs


async def test_hybrid_understanding():
    """æµ‹è¯•æ··åˆè§†é¢‘ç†è§£ç³»ç»Ÿ"""
    print("ğŸ§  æµ‹è¯•Qwenæ··åˆè§†é¢‘ç†è§£ç³»ç»Ÿ")

    # æµ‹è¯•YOLOç‰¹å¾æå–
    print("\n1. æµ‹è¯•YOLOç‰¹å¾æå–...")
    extractor = YOLOFeatureExtractor(offline_mode=True)
    features = await extractor.extract_video_features("/fake/test.mp4")
    print(f"   æå–ç‰¹å¾: {len(features['detected_objects'])} ç±»å¯¹è±¡")

    # æµ‹è¯•Qwenåˆ†æ
    print("\n2. æµ‹è¯•Qwenå†…å®¹åˆ†æ...")
    qwen = QwenTextAnalyzer(offline_mode=True)
    analysis = await qwen.analyze_video_from_features(features, "content")
    print(f"   å†…å®¹åˆ†æ: {analysis.get('main_theme', 'unknown')}")

    # æµ‹è¯•QwenVLå…³é”®å¸§
    print("\n3. æµ‹è¯•QwenVLå…³é”®å¸§åˆ†æ...")
    qwenvl = QwenVLKeyFrameAnalyzer(offline_mode=True)
    keyframe_analysis = await qwenvl.analyze_key_frames("/fake/test.mp4", [10.0, 20.0], "detailed")
    print(f"   å…³é”®å¸§åˆ†æ: {keyframe_analysis['total_frames_analyzed']} å¸§")

    # æµ‹è¯•ç»¼åˆç†è§£
    print("\n4. æµ‹è¯•ç»¼åˆè§†é¢‘ç†è§£...")
    hybrid = HybridVideoUnderstanding(offline_mode=True)
    result = await hybrid.understand_video("/fake/test.mp4", "balanced")
    print(f"   ç»¼åˆç†è§£: {result['understanding_summary']['video_type']}")
    print(f"   å¤„ç†æ–¹æ³•: {result['processing_methods']}")

    print("\nğŸ‰ Qwenæ··åˆè§†é¢‘ç†è§£ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
    return True


if __name__ == "__main__":
    asyncio.run(test_hybrid_understanding())