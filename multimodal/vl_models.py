#!/usr/bin/env python3
"""
VLæ¨¡å‹é›†æˆå®ç°
é›†æˆCLIPã€BLIPç­‰æ¨¡å‹ç”¨äºè§†è§‰é£æ ¼åˆ†æå’Œå†…å®¹ç†è§£
"""

import asyncio
import logging
import os
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
from pathlib import Path
import cv2
from PIL import Image
import torch
import torchvision.transforms as transforms

# å…ˆå®šä¹‰logger
logger = logging.getLogger(__name__)

# VLæ¨¡å‹ç›¸å…³å¯¼å…¥
try:
    import clip
    HAS_CLIP = True
except ImportError:
    HAS_CLIP = False
    # é™é»˜å¤„ç†ï¼Œä¸æ‰“å°è­¦å‘Š

try:
    from transformers import BlipProcessor, BlipForConditionalGeneration
    HAS_BLIP = True
except ImportError:
    HAS_BLIP = False
    # é™é»˜å¤„ç†ï¼Œä¸æ‰“å°è­¦å‘Š

# YOLOç›¸å…³å¯¼å…¥
try:
    import ultralytics
    from ultralytics import YOLO
    HAS_YOLO = True
    logger.info("âœ… YOLOæ¨¡å‹å¯ç”¨")
except ImportError:
    HAS_YOLO = False
    logger.info("â„¹ï¸ YOLOæœªå®‰è£…ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")

# Whisperç›¸å…³å¯¼å…¥
try:
    import whisper
    HAS_WHISPER = True
except ImportError:
    HAS_WHISPER = False
    # é™é»˜å¤„ç†


class CLIPStyleAnalyzer:
    """åŸºäºCLIPçš„é£æ ¼åˆ†æå™¨"""

    def __init__(self, device: str = None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        self.preprocess = None

        if HAS_CLIP:
            try:
                self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
                logger.info(f"CLIPæ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
            except Exception as e:
                logger.error(f"CLIPæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.model = None
        else:
            # é™é»˜å¤„ç†ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            pass

    async def analyze_image_style(self, image_path: str) -> Dict[str, Any]:
        """ä½¿ç”¨CLIPåˆ†æå›¾åƒé£æ ¼"""
        if not self.model or not Path(image_path).exists():
            return await self._mock_style_analysis(image_path)

        try:
            # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
            image = Image.open(image_path).convert("RGB")
            image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)

            # å®šä¹‰é£æ ¼æ ‡ç­¾
            style_labels = [
                "modern minimalist style",
                "vintage retro style",
                "corporate professional style",
                "creative artistic style",
                "natural organic style",
                "high-tech futuristic style",
                "warm cozy style",
                "clean bright style",
                "dark moody style",
                "colorful vibrant style"
            ]

            # è‰²å½©æ ‡ç­¾
            color_labels = [
                "warm colors", "cool colors", "neutral colors",
                "bright colors", "dark colors", "pastel colors",
                "monochromatic", "high contrast", "low contrast"
            ]

            # å…‰çº¿æ ‡ç­¾
            lighting_labels = [
                "natural daylight", "artificial lighting", "soft lighting",
                "hard lighting", "golden hour", "blue hour", "studio lighting"
            ]

            # æ„å»ºæ–‡æœ¬æç¤º
            text_prompts = style_labels + color_labels + lighting_labels
            text_tokens = clip.tokenize(text_prompts).to(self.device)

            with torch.no_grad():
                # è®¡ç®—å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾
                image_features = self.model.encode_image(image_tensor)
                text_features = self.model.encode_text(text_tokens)

                # è®¡ç®—ç›¸ä¼¼åº¦
                similarities = torch.cosine_similarity(image_features, text_features, dim=1)
                similarities = similarities.cpu().numpy()

            # è§£æç»“æœ
            style_scores = similarities[:len(style_labels)]
            color_scores = similarities[len(style_labels):len(style_labels)+len(color_labels)]
            lighting_scores = similarities[len(style_labels)+len(color_labels):]

            # è·å–æœ€åŒ¹é…çš„é£æ ¼
            top_style_idx = np.argmax(style_scores)
            top_color_idx = np.argmax(color_scores)
            top_lighting_idx = np.argmax(lighting_scores)

            # åˆ†æé¢œè‰²åˆ†å¸ƒ
            dominant_colors = await self._extract_dominant_colors(image_path)

            result = {
                'dominant_colors': dominant_colors,
                'lighting': lighting_labels[top_lighting_idx],
                'lighting_confidence': float(lighting_scores[top_lighting_idx]),
                'composition': 'centered',  # éœ€è¦é¢å¤–åˆ†æ
                'mood': style_labels[top_style_idx].split()[0],
                'style_tags': [
                    style_labels[top_style_idx],
                    color_labels[top_color_idx],
                    lighting_labels[top_lighting_idx]
                ],
                'confidence': float(np.mean([style_scores[top_style_idx],
                                           color_scores[top_color_idx],
                                           lighting_scores[top_lighting_idx]])),
                'detailed_scores': {
                    'style_scores': dict(zip(style_labels, style_scores.tolist())),
                    'color_scores': dict(zip(color_labels, color_scores.tolist())),
                    'lighting_scores': dict(zip(lighting_labels, lighting_scores.tolist()))
                }
            }

            logger.info(f"CLIPé£æ ¼åˆ†æå®Œæˆ: {image_path}")
            return result

        except Exception as e:
            logger.error(f"CLIPé£æ ¼åˆ†æå¤±è´¥: {e}")
            return await self._mock_style_analysis(image_path)

    async def _extract_dominant_colors(self, image_path: str, k: int = 5) -> List[str]:
        """æå–ä¸»è¦é¢œè‰²"""
        try:
            image = cv2.imread(image_path)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image = image.reshape((-1, 3))

            # ä½¿ç”¨K-meansèšç±»
            from sklearn.cluster import KMeans
            kmeans = KMeans(n_clusters=k, random_state=42)
            kmeans.fit(image)

            colors = kmeans.cluster_centers_.astype(int)
            colors_hex = [f"#{r:02x}{g:02x}{b:02x}" for r, g, b in colors]

            return colors_hex[:3]  # è¿”å›å‰3ä¸ªä¸»è¦é¢œè‰²

        except Exception as e:
            logger.error(f"é¢œè‰²æå–å¤±è´¥: {e}")
            return ['#2C3E50', '#3498DB', '#ECF0F1']  # é»˜è®¤é¢œè‰²

    async def _mock_style_analysis(self, image_path: str) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿé£æ ¼åˆ†æ"""
        return {
            'dominant_colors': ['#2C3E50', '#3498DB', '#ECF0F1'],
            'lighting': 'natural daylight',
            'lighting_confidence': 0.75,
            'composition': 'centered',
            'mood': 'professional',
            'style_tags': ['modern', 'clean', 'corporate'],
            'confidence': 0.70,
            'note': 'Using mock implementation - install CLIP for real analysis'
        }


class BLIPContentAnalyzer:
    """åŸºäºBLIPçš„å†…å®¹ç†è§£åˆ†æå™¨"""

    def __init__(self, device: str = None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.processor = None
        self.model = None

        if HAS_BLIP:
            try:
                self.processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
                self.model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
                self.model.to(self.device)
                logger.info(f"BLIPæ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
            except Exception as e:
                logger.error(f"BLIPæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.model = None
        else:
            logger.warning("BLIPæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå®ç°")

    async def analyze_content_match(self, image_path: str, description: str) -> Dict[str, Any]:
        """åˆ†æå›¾åƒå†…å®¹ä¸æè¿°çš„åŒ¹é…åº¦"""
        if not self.model or not Path(image_path).exists():
            return await self._mock_content_analysis(image_path, description)

        try:
            # ç”Ÿæˆå›¾åƒæè¿°
            image = Image.open(image_path).convert("RGB")
            inputs = self.processor(image, return_tensors="pt").to(self.device)

            with torch.no_grad():
                generated_ids = self.model.generate(**inputs, max_length=50)
                generated_caption = self.processor.decode(generated_ids[0], skip_special_tokens=True)

            # ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
            match_score = await self._calculate_text_similarity(generated_caption, description)

            # åˆ†æå…·ä½“åŒ¹é…å…ƒç´ 
            description_words = set(description.lower().split())
            caption_words = set(generated_caption.lower().split())

            relevant_objects = list(description_words.intersection(caption_words))
            missing_elements = list(description_words - caption_words)
            extra_elements = list(caption_words - description_words)

            # è®¡ç®—é€‚ç”¨æ€§
            suitability = "high" if match_score > 0.7 else "medium" if match_score > 0.4 else "low"

            result = {
                'match_score': match_score,
                'generated_caption': generated_caption,
                'relevant_objects': relevant_objects[:10],  # é™åˆ¶æ•°é‡
                'missing_elements': missing_elements[:5],
                'extra_elements': extra_elements[:5],
                'suitability': suitability,
                'confidence': 0.85
            }

            logger.info(f"BLIPå†…å®¹åˆ†æå®Œæˆ: {image_path}")
            return result

        except Exception as e:
            logger.error(f"BLIPå†…å®¹åˆ†æå¤±è´¥: {e}")
            return await self._mock_content_analysis(image_path, description)

    async def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        try:
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())

            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))

            if union == 0:
                return 0.0

            return intersection / union

        except Exception:
            return 0.5

    async def _mock_content_analysis(self, image_path: str, description: str) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿå†…å®¹åˆ†æ"""
        return {
            'match_score': 0.72,
            'generated_caption': 'a professional product image with modern lighting',
            'relevant_objects': ['product', 'background', 'lighting'],
            'missing_elements': ['specific_angle'],
            'extra_elements': ['watermark'],
            'suitability': 'high',
            'note': 'Using mock implementation - install transformers/BLIP for real analysis'
        }


class YOLOFaceDetector:
    """åŸºäºYOLOçš„äººè„¸æ£€æµ‹å™¨"""

    def __init__(self, offline_mode=None):
        self.model = None

        # æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
        if offline_mode or os.environ.get('HAS_VL_MODELS') == 'false':
            logger.info("YOLOæ£€æµ‹å™¨è¿è¡Œåœ¨ç¦»çº¿æ¨¡å¼")
            return

        if HAS_YOLO:
            try:
                # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                face_model_path = Path('yolov8n-face.pt')
                general_model_path = Path('yolov8n.pt')

                if face_model_path.exists():
                    self.model = YOLO('yolov8n-face.pt')
                    logger.info("YOLOäººè„¸æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ")
                elif general_model_path.exists() and general_model_path.stat().st_size > 0:
                    self.model = YOLO('yolov8n.pt')
                    logger.info("YOLOé€šç”¨æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ")
                else:
                    logger.info("YOLOæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨æˆ–å·²æŸåï¼Œä½¿ç”¨æ¨¡æ‹Ÿå®ç°")
                    self.model = None
            except Exception as e:
                logger.info(f"YOLOæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå®ç°: {e}")
                self.model = None
        else:
            logger.info("YOLOæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå®ç°")

    async def detect_faces_in_video(self, video_path: str) -> Dict[str, Any]:
        """æ£€æµ‹è§†é¢‘ä¸­çš„äººè„¸"""
        if not self.model or not Path(video_path).exists():
            return await self._mock_face_detection(video_path)

        try:
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = total_frames / fps if fps > 0 else 0

            face_detections = []
            current_time = 0
            frame_count = 0

            # æ¯ç§’æ£€æµ‹ä¸€å¸§ä»¥æé«˜æ•ˆç‡
            frame_skip = max(1, int(fps)) if fps > 0 else 30

            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                if frame_count % frame_skip == 0:
                    current_time = frame_count / fps if fps > 0 else frame_count / 30

                    # YOLOæ£€æµ‹
                    results = self.model(frame)

                    # æå–äººè„¸æ£€æµ‹ç»“æœ
                    for result in results:
                        boxes = result.boxes
                        if boxes is not None:
                            for box in boxes:
                                # æ£€æŸ¥æ˜¯å¦æ˜¯äººç±»ï¼ˆclass 0é€šå¸¸æ˜¯personï¼‰
                                if int(box.cls) == 0:  # person class
                                    confidence = float(box.conf)
                                    if confidence > 0.5:
                                        face_detections.append({
                                            'timestamp': current_time,
                                            'confidence': confidence,
                                            'bbox': box.xyxy[0].cpu().numpy().tolist()
                                        })

                frame_count += 1

            cap.release()

            # åˆå¹¶è¿ç»­çš„æ£€æµ‹ç»“æœä¸ºæ—¶é—´æ®µ
            face_segments = self._merge_detections_to_segments(face_detections)

            result = {
                'duration': duration,
                'fps': fps,
                'faces_detected': len(face_segments),
                'face_timestamps': face_segments,
                'total_detections': len(face_detections)
            }

            logger.info(f"YOLOäººè„¸æ£€æµ‹å®Œæˆ: {video_path}")
            return result

        except Exception as e:
            logger.error(f"YOLOäººè„¸æ£€æµ‹å¤±è´¥: {e}")
            return await self._mock_face_detection(video_path)

    def _merge_detections_to_segments(self, detections: List[Dict], gap_threshold: float = 2.0) -> List[Dict]:
        """å°†æ£€æµ‹ç»“æœåˆå¹¶ä¸ºè¿ç»­æ—¶é—´æ®µ"""
        if not detections:
            return []

        # æŒ‰æ—¶é—´æ’åº
        detections = sorted(detections, key=lambda x: x['timestamp'])

        segments = []
        current_segment = {
            'start': detections[0]['timestamp'],
            'end': detections[0]['timestamp'],
            'confidence': detections[0]['confidence']
        }

        for detection in detections[1:]:
            if detection['timestamp'] - current_segment['end'] <= gap_threshold:
                # æ‰©å±•å½“å‰æ®µ
                current_segment['end'] = detection['timestamp']
                current_segment['confidence'] = max(current_segment['confidence'], detection['confidence'])
            else:
                # å¼€å§‹æ–°æ®µ
                segments.append(current_segment)
                current_segment = {
                    'start': detection['timestamp'],
                    'end': detection['timestamp'],
                    'confidence': detection['confidence']
                }

        # æ·»åŠ æœ€åä¸€æ®µ
        segments.append(current_segment)

        return segments

    async def _mock_face_detection(self, video_path: str) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿäººè„¸æ£€æµ‹"""
        return {
            'duration': 45.0,
            'fps': 30,
            'faces_detected': 2,
            'face_timestamps': [
                {'start': 5.0, 'end': 15.0, 'confidence': 0.95},
                {'start': 20.0, 'end': 35.0, 'confidence': 0.88}
            ],
            'total_detections': 156,
            'note': 'Using mock implementation - install ultralytics for real detection'
        }


class WhisperAudioAnalyzer:
    """åŸºäºWhisperçš„è¯­éŸ³åˆ†æå™¨"""

    def __init__(self, model_size: str = "base", offline_mode=None):
        self.model = None
        self.model_size = model_size

        # æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
        if offline_mode or os.environ.get('HAS_VL_MODELS') == 'false':
            logger.info("Whisperåˆ†æå™¨è¿è¡Œåœ¨ç¦»çº¿æ¨¡å¼")
            return

        if HAS_WHISPER:
            try:
                self.model = whisper.load_model(model_size)
                logger.info(f"Whisper {model_size} æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.model = None
        else:
            logger.warning("Whisperæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå®ç°")

    async def analyze_video_audio(self, video_path: str) -> Dict[str, Any]:
        """åˆ†æè§†é¢‘éŸ³é¢‘å†…å®¹"""
        if not self.model or not Path(video_path).exists():
            return await self._mock_audio_analysis(video_path)

        try:
            # ä½¿ç”¨whisperå¤„ç†æ•´ä¸ªè§†é¢‘æ–‡ä»¶
            result = self.model.transcribe(video_path)

            # æå–è¯­éŸ³æ®µè½
            speech_segments = []
            for segment in result['segments']:
                speech_segments.append({
                    'start': segment['start'],
                    'end': segment['end'],
                    'text': segment['text'].strip(),
                    'confidence': segment.get('avg_logprob', 0.0)
                })

            # åˆ†æè¯­éŸ³ç‰¹å¾
            has_speech = len(speech_segments) > 0
            total_speech_duration = sum(seg['end'] - seg['start'] for seg in speech_segments)

            # è¯­è¨€æ£€æµ‹
            language = result.get('language', 'unknown')
            language_confidence = result.get('language_probability', 0.0)

            analysis_result = {
                'has_speech': has_speech,
                'language': language,
                'language_confidence': language_confidence,
                'speech_segments': speech_segments,
                'total_speech_duration': total_speech_duration,
                'full_text': result['text'],
                'speech_quality': self._assess_speech_quality(speech_segments)
            }

            logger.info(f"WhisperéŸ³é¢‘åˆ†æå®Œæˆ: {video_path}")
            return analysis_result

        except Exception as e:
            logger.error(f"WhisperéŸ³é¢‘åˆ†æå¤±è´¥: {e}")
            return await self._mock_audio_analysis(video_path)

    def _assess_speech_quality(self, segments: List[Dict]) -> Dict[str, Any]:
        """è¯„ä¼°è¯­éŸ³è´¨é‡"""
        if not segments:
            return {'quality': 'no_speech', 'score': 0.0}

        # åŸºäºç½®ä¿¡åº¦è¯„ä¼°
        confidences = [seg.get('confidence', 0.0) for seg in segments]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0

        # åŸºäºæ®µè½é•¿åº¦è¯„ä¼°ï¼ˆå¤ªçŸ­æˆ–å¤ªé•¿çš„æ®µè½å¯èƒ½è¡¨ç¤ºè´¨é‡é—®é¢˜ï¼‰
        segment_lengths = [seg['end'] - seg['start'] for seg in segments]
        avg_length = sum(segment_lengths) / len(segment_lengths) if segment_lengths else 0.0

        # ç»¼åˆè¯„åˆ†
        quality_score = (avg_confidence * 0.7 + min(avg_length / 5.0, 1.0) * 0.3)

        if quality_score > 0.8:
            quality = 'excellent'
        elif quality_score > 0.6:
            quality = 'good'
        elif quality_score > 0.4:
            quality = 'fair'
        else:
            quality = 'poor'

        return {
            'quality': quality,
            'score': quality_score,
            'avg_confidence': avg_confidence,
            'avg_segment_length': avg_length,
            'total_segments': len(segments)
        }

    async def _mock_audio_analysis(self, video_path: str) -> Dict[str, Any]:
        """æ¨¡æ‹ŸéŸ³é¢‘åˆ†æ"""
        return {
            'has_speech': True,
            'language': 'zh',
            'language_confidence': 0.95,
            'speech_segments': [
                {'start': 2.0, 'end': 8.0, 'text': 'æ¬¢è¿ä½¿ç”¨æˆ‘ä»¬çš„äº§å“', 'confidence': 0.92},
                {'start': 10.0, 'end': 18.0, 'text': 'è¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„è§£å†³æ–¹æ¡ˆ', 'confidence': 0.88},
                {'start': 25.0, 'end': 35.0, 'text': 'æ„Ÿè°¢æ‚¨çš„å…³æ³¨', 'confidence': 0.85}
            ],
            'total_speech_duration': 20.0,
            'full_text': 'æ¬¢è¿ä½¿ç”¨æˆ‘ä»¬çš„äº§å“ã€‚è¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚æ„Ÿè°¢æ‚¨çš„å…³æ³¨ã€‚',
            'speech_quality': {
                'quality': 'good',
                'score': 0.75,
                'avg_confidence': 0.88,
                'avg_segment_length': 6.67,
                'total_segments': 3
            },
            'note': 'Using mock implementation - install whisper for real analysis'
        }


class IntegratedVLSystem:
    """é›†æˆVLç³»ç»Ÿ - ç»Ÿä¸€æ¥å£"""

    def __init__(self, device: str = None, offline_mode: bool = None):
        # æ£€æŸ¥ç¦»çº¿æ¨¡å¼
        if offline_mode is None:
            offline_mode = os.environ.get('HAS_VL_MODELS') == 'false'

        self.offline_mode = offline_mode
        self.clip_analyzer = CLIPStyleAnalyzer(device)
        self.blip_analyzer = BLIPContentAnalyzer(device)
        self.yolo_detector = YOLOFaceDetector(offline_mode=offline_mode)
        self.whisper_analyzer = WhisperAudioAnalyzer(offline_mode=offline_mode)
        self.logger = logger.getChild('IntegratedVL')

        if offline_mode:
            self.logger.info("VLç³»ç»Ÿè¿è¡Œåœ¨ç¦»çº¿æ¨¡å¼ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå®ç°")

    async def analyze_visual_style(self, image_path: str) -> Dict[str, Any]:
        """ç»Ÿä¸€çš„è§†è§‰é£æ ¼åˆ†ææ¥å£"""
        return await self.clip_analyzer.analyze_image_style(image_path)

    async def analyze_content_match(self, image_path: str, description: str) -> Dict[str, Any]:
        """ç»Ÿä¸€çš„å†…å®¹åŒ¹é…åˆ†ææ¥å£"""
        return await self.blip_analyzer.analyze_content_match(image_path, description)

    async def compare_visual_consistency(self, image1_path: str, image2_path: str) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸¤ä¸ªå›¾åƒçš„è§†è§‰ä¸€è‡´æ€§"""
        try:
            # åˆ†åˆ«åˆ†æä¸¤ä¸ªå›¾åƒçš„é£æ ¼
            style1 = await self.analyze_visual_style(image1_path)
            style2 = await self.analyze_visual_style(image2_path)

            # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
            color_consistency = self._compare_colors(
                style1.get('dominant_colors', []),
                style2.get('dominant_colors', [])
            )

            lighting_consistency = 1.0 if style1.get('lighting') == style2.get('lighting') else 0.5

            style_consistency = self._compare_style_tags(
                style1.get('style_tags', []),
                style2.get('style_tags', [])
            )

            overall_consistency = (color_consistency * 0.4 +
                                 lighting_consistency * 0.3 +
                                 style_consistency * 0.3)

            return {
                'overall_consistency': overall_consistency,
                'color_consistency': color_consistency,
                'lighting_consistency': lighting_consistency,
                'style_consistency': style_consistency,
                'details': {
                    'similar_elements': [],
                    'different_elements': [],
                    'recommendations': []
                }
            }

        except Exception as e:
            self.logger.error(f"è§†è§‰ä¸€è‡´æ€§æ¯”è¾ƒå¤±è´¥: {e}")
            return {
                'overall_consistency': 0.5,
                'color_consistency': 0.5,
                'lighting_consistency': 0.5,
                'style_consistency': 0.5,
                'error': str(e)
            }

    async def analyze_video_content(self, video_path: str) -> Dict[str, Any]:
        """ç»Ÿä¸€çš„è§†é¢‘å†…å®¹åˆ†ææ¥å£"""
        try:
            # å¹¶è¡Œæ‰§è¡Œäººè„¸æ£€æµ‹å’ŒéŸ³é¢‘åˆ†æ
            face_task = self.yolo_detector.detect_faces_in_video(video_path)
            audio_task = self.whisper_analyzer.analyze_video_audio(video_path)

            face_result, audio_result = await asyncio.gather(face_task, audio_task)

            # åˆå¹¶ç»“æœ
            return {
                'duration': face_result.get('duration', 0),
                'fps': face_result.get('fps', 30),
                'resolution': '1920x1080',  # éœ€è¦ä»è§†é¢‘ä¸­è·å–
                'face_detection': {
                    'faces_detected': face_result.get('faces_detected', 0),
                    'face_timestamps': face_result.get('face_timestamps', [])
                },
                'audio_analysis': audio_result,
                'analysis_timestamp': asyncio.get_event_loop().time()
            }

        except Exception as e:
            self.logger.error(f"è§†é¢‘å†…å®¹åˆ†æå¤±è´¥: {e}")
            return {
                'duration': 0,
                'error': str(e)
            }

    def _compare_colors(self, colors1: List[str], colors2: List[str]) -> float:
        """æ¯”è¾ƒé¢œè‰²ä¸€è‡´æ€§"""
        if not colors1 or not colors2:
            return 0.5

        # ç®€å•çš„é¢œè‰²åŒ¹é…ç®—æ³•
        matches = 0
        for color1 in colors1:
            for color2 in colors2:
                if color1 == color2:
                    matches += 1
                    break

        return matches / max(len(colors1), len(colors2))

    def _compare_style_tags(self, tags1: List[str], tags2: List[str]) -> float:
        """æ¯”è¾ƒé£æ ¼æ ‡ç­¾ä¸€è‡´æ€§"""
        if not tags1 or not tags2:
            return 0.5

        set1 = set(tags1)
        set2 = set(tags2)
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))

        return intersection / union if union > 0 else 0.0


# å…¨å±€å®ä¾‹
vl_system = IntegratedVLSystem()


async def test_vl_system():
    """æµ‹è¯•VLç³»ç»ŸåŠŸèƒ½"""
    print("ğŸ§  æµ‹è¯•VLæ¨¡å‹é›†æˆç³»ç»Ÿ")

    # æµ‹è¯•å›¾åƒé£æ ¼åˆ†æ
    print("\n1. æµ‹è¯•å›¾åƒé£æ ¼åˆ†æ")
    if HAS_CLIP:
        print("âœ… CLIPå¯ç”¨")
    else:
        print("âš ï¸  CLIPä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå®ç°")

    # æµ‹è¯•å†…å®¹ç†è§£
    print("\n2. æµ‹è¯•å†…å®¹ç†è§£")
    if HAS_BLIP:
        print("âœ… BLIPå¯ç”¨")
    else:
        print("âš ï¸  BLIPä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå®ç°")

    # æµ‹è¯•äººè„¸æ£€æµ‹
    print("\n3. æµ‹è¯•äººè„¸æ£€æµ‹")
    if HAS_YOLO:
        print("âœ… YOLOå¯ç”¨")
    else:
        print("âš ï¸  YOLOä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå®ç°")

    # æµ‹è¯•è¯­éŸ³åˆ†æ
    print("\n4. æµ‹è¯•è¯­éŸ³åˆ†æ")
    if HAS_WHISPER:
        print("âœ… Whisperå¯ç”¨")
    else:
        print("âš ï¸  Whisperä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå®ç°")

    print("\nğŸ¯ VLç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    return True


if __name__ == "__main__":
    asyncio.run(test_vl_system())