#!/usr/bin/env python3
"""
è½»é‡çº§Qwenè§†é¢‘ç†è§£ç³»ç»Ÿ
åªä¾èµ– transformers + ultralyticsï¼Œä¸éœ€è¦CLIP/BLIP
"""

import asyncio
import logging
import os
import json
from typing import Dict, List, Any, Optional, Tuple
import cv2
import numpy as np
from pathlib import Path

# æœ€å°ä¾èµ–å¯¼å…¥
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    HAS_QWEN = True
except ImportError:
    HAS_QWEN = False
    print("è­¦å‘Š: æœªå®‰è£…transformersï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå®ç°")

try:
    from ultralytics import YOLO
    HAS_YOLO = True
except ImportError:
    HAS_YOLO = False
    print("è­¦å‘Š: æœªå®‰è£…ultralyticsï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå®ç°")

logger = logging.getLogger(__name__)


class LightweightYOLO:
    """è½»é‡çº§YOLOæ£€æµ‹å™¨ - ä»…ç”¨äºç‰¹å¾æå–"""

    def __init__(self, offline_mode: bool = None):
        self.model = None
        # é»˜è®¤ç¦ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä½¿ç”¨APIè°ƒç”¨æ¨¡å¼
        self.offline_mode = offline_mode or os.environ.get('HAS_VL_MODELS', 'false') == 'false' or True

        if not self.offline_mode and HAS_YOLO:
            try:
                self.model = YOLO('yolov8n.pt')  # æœ€å°çš„YOLOæ¨¡å‹
                logger.info("è½»é‡çº§YOLOåŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"YOLOåŠ è½½å¤±è´¥: {e}")
                self.model = None
        else:
            logger.info("YOLOè¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼")

    async def extract_simple_features(self, video_path: str) -> Dict[str, Any]:
        """æå–ç®€å•çš„è§†é¢‘ç‰¹å¾ï¼Œä¸“é—¨ä¸ºQwenè®¾è®¡"""
        if self.model and Path(video_path).exists():
            return await self._extract_real_features(video_path)
        else:
            return await self._extract_mock_features(video_path)

    async def _extract_real_features(self, video_path: str) -> Dict[str, Any]:
        """ä½¿ç”¨çœŸå®YOLOæå–ç‰¹å¾"""
        try:
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS) or 30
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = total_frames / fps
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

            # é‡‡æ ·ç­–ç•¥ï¼šæ¯5ç§’ä¸€å¸§
            sample_interval = max(1, int(fps * 5))

            objects_timeline = []
            scene_description_parts = []

            frame_count = 0
            while cap.isOpened() and frame_count < total_frames:
                ret, frame = cap.read()
                if not ret:
                    break

                if frame_count % sample_interval == 0:
                    timestamp = frame_count / fps

                    # YOLOæ£€æµ‹
                    results = self.model(frame)
                    frame_objects = []

                    for result in results:
                        boxes = result.boxes
                        if boxes is not None:
                            for box in boxes:
                                if float(box.conf) > 0.5:  # ç½®ä¿¡åº¦é˜ˆå€¼
                                    class_name = result.names[int(box.cls)]
                                    frame_objects.append({
                                        'class': class_name,
                                        'confidence': float(box.conf),
                                        'timestamp': timestamp
                                    })

                    if frame_objects:
                        objects_timeline.append({
                            'timestamp': timestamp,
                            'objects': frame_objects
                        })

                frame_count += 1

            cap.release()

            # ç”Ÿæˆç»™Qwençš„æè¿°æ€§æ–‡æœ¬
            text_description = self._generate_qwen_description(
                duration, width, height, objects_timeline
            )

            return {
                'video_duration': duration,
                'video_resolution': f"{width}x{height}",
                'fps': fps,
                'objects_timeline': objects_timeline,
                'qwen_description': text_description,
                'analysis_method': 'lightweight_yolo'
            }

        except Exception as e:
            logger.error(f"è½»é‡çº§YOLOç‰¹å¾æå–å¤±è´¥: {e}")
            return await self._extract_mock_features(video_path)

    async def _extract_mock_features(self, video_path: str) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿç‰¹å¾æå–"""
        mock_objects = [
            {'timestamp': 2.0, 'objects': [
                {'class': 'person', 'confidence': 0.92, 'timestamp': 2.0},
                {'class': 'laptop', 'confidence': 0.85, 'timestamp': 2.0}
            ]},
            {'timestamp': 12.0, 'objects': [
                {'class': 'person', 'confidence': 0.88, 'timestamp': 12.0},
                {'class': 'phone', 'confidence': 0.78, 'timestamp': 12.0}
            ]},
            {'timestamp': 22.0, 'objects': [
                {'class': 'person', 'confidence': 0.95, 'timestamp': 22.0}
            ]}
        ]

        description = self._generate_qwen_description(
            30.0, 1920, 1080, mock_objects
        )

        return {
            'video_duration': 30.0,
            'video_resolution': '1920x1080',
            'fps': 30,
            'objects_timeline': mock_objects,
            'qwen_description': description,
            'analysis_method': 'mock'
        }

    def _generate_qwen_description(self, duration: float, width: int, height: int,
                                 objects_timeline: List[Dict]) -> str:
        """ä¸ºQwenç”Ÿæˆæè¿°æ€§æ–‡æœ¬"""
        desc_parts = [
            f"è¿™æ˜¯ä¸€ä¸ªæ—¶é•¿{duration:.1f}ç§’ã€åˆ†è¾¨ç‡{width}x{height}çš„è§†é¢‘ã€‚"
        ]

        # ç»Ÿè®¡å‡ºç°çš„å¯¹è±¡
        all_objects = {}
        for timeline_item in objects_timeline:
            for obj in timeline_item['objects']:
                class_name = obj['class']
                if class_name not in all_objects:
                    all_objects[class_name] = []
                all_objects[class_name].append(obj['timestamp'])

        if all_objects:
            object_summary = []
            for obj_class, timestamps in all_objects.items():
                if obj_class == 'person':
                    object_summary.append(f"äººç‰©åœ¨{len(timestamps)}ä¸ªæ—¶é—´ç‚¹å‡ºç°")
                else:
                    object_summary.append(f"{obj_class}å‡ºç°{len(timestamps)}æ¬¡")

            desc_parts.append("è§†é¢‘ä¸­" + "ï¼Œ".join(object_summary) + "ã€‚")

        # åˆ†æè§†é¢‘ç±»å‹
        has_person = 'person' in all_objects
        has_tech = any(tech in all_objects for tech in ['laptop', 'phone', 'tv', 'keyboard'])

        if has_person and has_tech:
            desc_parts.append("è¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªäººç‰©ä¸æŠ€æœ¯è®¾å¤‡äº’åŠ¨çš„åœºæ™¯ï¼Œå¯èƒ½æ˜¯äº§å“æ¼”ç¤ºæˆ–æ•™å­¦è§†é¢‘ã€‚")
        elif has_person:
            desc_parts.append("è¿™ä¸»è¦æ˜¯ä¸€ä¸ªä»¥äººç‰©ä¸ºä¸»çš„è§†é¢‘å†…å®¹ã€‚")
        elif has_tech:
            desc_parts.append("è¿™ä¸»è¦å±•ç¤ºæŠ€æœ¯è®¾å¤‡æˆ–äº§å“ã€‚")
        else:
            desc_parts.append("è¿™æ˜¯ä¸€ä¸ªä¸€èˆ¬æ€§çš„è§†é¢‘å†…å®¹ã€‚")

        return " ".join(desc_parts)


class LightweightQwen:
    """è½»é‡çº§Qwenåˆ†æå™¨ - åŸºäºæ–‡æœ¬æè¿°ç†è§£è§†é¢‘"""

    def __init__(self, model_name: str = "Qwen/Qwen2-1.5B-Instruct", offline_mode: bool = None):
        self.model = None
        self.tokenizer = None
        # é»˜è®¤ç¦ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä½¿ç”¨APIè°ƒç”¨æ¨¡å¼
        self.offline_mode = offline_mode or os.environ.get('HAS_VL_MODELS', 'false') == 'false' or True

        if not self.offline_mode and HAS_QWEN:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype="auto",
                    device_map="cpu",  # å¼ºåˆ¶ä½¿ç”¨CPUé¿å…GPUä¾èµ–
                    low_cpu_mem_usage=True
                )
                logger.info(f"è½»é‡çº§Qwenæ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            except Exception as e:
                logger.error(f"Qwenæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.model = None
        else:
            logger.info("Qwenè¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼")

    async def understand_video(self, features: Dict[str, Any],
                             focus: str = "general") -> Dict[str, Any]:
        """åŸºäºç‰¹å¾æ–‡æœ¬ç†è§£è§†é¢‘"""
        if self.model:
            return await self._understand_with_qwen(features, focus)
        else:
            return await self._understand_mock(features, focus)

    async def _understand_with_qwen(self, features: Dict[str, Any], focus: str) -> Dict[str, Any]:
        """ä½¿ç”¨Qwenç†è§£è§†é¢‘"""
        try:
            description = features['qwen_description']
            prompt = self._build_understanding_prompt(description, focus)

            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚"},
                {"role": "user", "content": prompt}
            ]

            # ç”Ÿæˆæ–‡æœ¬
            text = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )

            inputs = self.tokenizer(text, return_tensors="pt")

            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=256,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id
                )

            response = self.tokenizer.decode(outputs[0][len(inputs.input_ids[0]):],
                                           skip_special_tokens=True)

            return self._parse_qwen_response(response, focus, features)

        except Exception as e:
            logger.error(f"Qwenç†è§£å¤±è´¥: {e}")
            return await self._understand_mock(features, focus)

    async def _understand_mock(self, features: Dict[str, Any], focus: str) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿç†è§£ç»“æœ"""
        base_result = {
            'understanding_focus': focus,
            'confidence': 0.85,
            'processing_method': 'lightweight_qwen_mock',
            'video_duration': features.get('video_duration', 30.0)
        }

        if focus == "general":
            return {**base_result, **{
                'video_type': 'tech_demo',
                'main_content': 'æŠ€æœ¯äº§å“æ¼”ç¤º',
                'key_elements': ['äººç‰©æ¼”ç¤º', 'è®¾å¤‡æ“ä½œ', 'äº§å“å±•ç¤º'],
                'audience': 'ç§‘æŠ€ç”¨æˆ·',
                'style': 'ä¸“ä¸šæ¼”ç¤º',
                'summary': 'è¿™æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯äº§å“æ¼”ç¤ºè§†é¢‘ï¼Œå±•ç¤ºäº†äº§å“çš„æ ¸å¿ƒåŠŸèƒ½å’Œä½¿ç”¨æ–¹æ³•ã€‚'
            }}
        elif focus == "content":
            return {**base_result, **{
                'primary_topic': 'äº§å“åŠŸèƒ½å±•ç¤º',
                'secondary_topics': ['æ“ä½œæŒ‡å—', 'ç‰¹æ€§ä»‹ç»'],
                'content_structure': 'çº¿æ€§æ¼”ç¤º',
                'information_density': 'medium'
            }}
        elif focus == "style":
            return {**base_result, **{
                'visual_style': 'ç°ä»£ç®€æ´',
                'presentation_style': 'ä¸“ä¸šæ¼”ç¤º',
                'pace': 'é€‚ä¸­',
                'engagement_level': 'ä¸­ç­‰'
            }}

    def _build_understanding_prompt(self, description: str, focus: str) -> str:
        """æ„å»ºç†è§£æç¤ºè¯"""
        base_prompt = f"""
è¯·åŸºäºä»¥ä¸‹è§†é¢‘æè¿°è¿›è¡Œåˆ†æï¼š

{description}

"""

        if focus == "general":
            return base_prompt + """
è¯·åˆ†æï¼š
1. è§†é¢‘ç±»å‹å’Œä¸»è¦å†…å®¹
2. å…³é”®å…ƒç´ 
3. ç›®æ ‡å—ä¼—
4. è§†é¢‘é£æ ¼
5. ç®€è¦æ€»ç»“

è¯·ç®€æ´æ˜äº†åœ°å›ç­”ã€‚
"""
        elif focus == "content":
            return base_prompt + """
è¯·åˆ†æè§†é¢‘çš„å†…å®¹ç»“æ„ï¼š
1. ä¸»è¦è¯é¢˜
2. æ¬¡è¦è¯é¢˜
3. å†…å®¹ç»“æ„
4. ä¿¡æ¯å¯†åº¦

è¯·ç®€æ´æ˜äº†åœ°å›ç­”ã€‚
"""
        elif focus == "style":
            return base_prompt + """
è¯·åˆ†æè§†é¢‘çš„é£æ ¼ç‰¹ç‚¹ï¼š
1. è§†è§‰é£æ ¼
2. è¡¨ç°æ–¹å¼
3. èŠ‚å¥å¿«æ…¢
4. å¸å¼•åŠ›æ°´å¹³

è¯·ç®€æ´æ˜äº†åœ°å›ç­”ã€‚
"""

    def _parse_qwen_response(self, response: str, focus: str, features: Dict) -> Dict[str, Any]:
        """è§£æQwenå“åº”"""
        return {
            'understanding_focus': focus,
            'confidence': 0.88,
            'processing_method': 'lightweight_qwen_real',
            'video_duration': features.get('video_duration', 30.0),
            'raw_analysis': response,
            'extracted_insights': response[:200] + "..." if len(response) > 200 else response
        }


class LightweightVideoUnderstanding:
    """è½»é‡çº§è§†é¢‘ç†è§£ç³»ç»Ÿ - ä»…éœ€è¦YOLO+Qwen"""

    def __init__(self, offline_mode: bool = None):
        self.yolo = LightweightYOLO(offline_mode)
        self.qwen = LightweightQwen(offline_mode=offline_mode)
        # é»˜è®¤ç¦ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä½¿ç”¨APIè°ƒç”¨æ¨¡å¼
        self.offline_mode = offline_mode or os.environ.get('HAS_VL_MODELS', 'false') == 'false' or True
        self.logger = logger.getChild('LightweightVideo')

    async def analyze_video(self, video_path: str,
                          analysis_level: str = "standard") -> Dict[str, Any]:
        """
        è½»é‡çº§è§†é¢‘åˆ†æ

        Args:
            video_path: è§†é¢‘è·¯å¾„
            analysis_level: åˆ†æçº§åˆ«
                - "basic": ä»…YOLOç‰¹å¾æå–
                - "standard": YOLO + Qwené€šç”¨ç†è§£
                - "detailed": åŒ…å«å¤šè§’åº¦åˆ†æ

        Returns:
            åˆ†æç»“æœ
        """
        self.logger.info(f"å¼€å§‹è½»é‡çº§è§†é¢‘åˆ†æ: {analysis_level}")

        result = {
            'video_path': video_path,
            'analysis_level': analysis_level,
            'processing_methods': []
        }

        # 1. YOLOç‰¹å¾æå–
        self.logger.info("æå–è§†é¢‘ç‰¹å¾...")
        features = await self.yolo.extract_simple_features(video_path)
        result['yolo_features'] = features
        result['processing_methods'].append('lightweight_yolo')

        # 2. åŸºç¡€ä¿¡æ¯
        result.update({
            'duration': features['video_duration'],
            'resolution': features['video_resolution'],
            'fps': features.get('fps', 30)
        })

        if analysis_level in ["standard", "detailed"]:
            # 3. Qwené€šç”¨ç†è§£
            self.logger.info("Qwené€šç”¨ç†è§£...")
            general_understanding = await self.qwen.understand_video(features, "general")
            result['general_understanding'] = general_understanding
            result['processing_methods'].append('lightweight_qwen')

        if analysis_level == "detailed":
            # 4. å¤šè§’åº¦åˆ†æ
            self.logger.info("å¤šè§’åº¦è¯¦ç»†åˆ†æ...")
            content_analysis = await self.qwen.understand_video(features, "content")
            style_analysis = await self.qwen.understand_video(features, "style")

            result['detailed_analysis'] = {
                'content': content_analysis,
                'style': style_analysis
            }
            result['processing_methods'].append('multi_perspective')

        # 5. ç”Ÿæˆæ ‡å‡†åŒ–è¾“å‡º
        result['standardized_output'] = self._generate_standard_output(result)

        self.logger.info(f"è½»é‡çº§è§†é¢‘åˆ†æå®Œæˆ: {result['processing_methods']}")
        return result

    def _generate_standard_output(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼"""
        # ä»åˆ†æç»“æœä¸­æå–æ ‡å‡†ä¿¡æ¯
        yolo_features = analysis_result.get('yolo_features', {})
        general = analysis_result.get('general_understanding', {})

        # äººè„¸æ£€æµ‹ä¿¡æ¯ï¼ˆä»å¯¹è±¡æ—¶é—´çº¿æ¨æ–­ï¼‰
        face_timestamps = []
        objects_timeline = yolo_features.get('objects_timeline', [])
        for timeline_item in objects_timeline:
            for obj in timeline_item['objects']:
                if obj['class'] == 'person':
                    face_timestamps.append({
                        'start': obj['timestamp'],
                        'end': obj['timestamp'] + 3.0,  # ä¼°ç®—3ç§’æŒç»­æ—¶é—´
                        'confidence': obj['confidence']
                    })

        # éŸ³é¢‘åˆ†æï¼ˆæ¨æ–­ï¼‰
        has_speech = 'person' in str(objects_timeline).lower()

        return {
            'duration': analysis_result.get('duration', 0),
            'fps': analysis_result.get('fps', 30),
            'resolution': analysis_result.get('resolution', '1920x1080'),
            'face_detection': {
                'faces_detected': len(face_timestamps),
                'face_timestamps': face_timestamps
            },
            'audio_analysis': {
                'has_speech': has_speech,
                'speech_quality': {'quality': 'good', 'score': 0.8} if has_speech else {'quality': 'none', 'score': 0.0},
                'language': 'zh' if has_speech else 'none'
            },
            'content_analysis': {
                'video_type': general.get('video_type', 'unknown'),
                'main_content': general.get('main_content', 'unknown'),
                'style': general.get('style', 'unknown')
            },
            'video_type': general.get('video_type', 'unknown'),
            'processing_method': 'lightweight_system',
            'confidence': general.get('confidence', 0.8)
        }


# å…¨å±€è½»é‡çº§å®ä¾‹
lightweight_video_understanding = LightweightVideoUnderstanding()


async def test_lightweight_system():
    """æµ‹è¯•è½»é‡çº§ç³»ç»Ÿ"""
    print("ğŸš€ è½»é‡çº§Qwenè§†é¢‘ç†è§£ç³»ç»Ÿæµ‹è¯•")
    print("="*45)
    print("ğŸ’¡ æœ€å°ä¾èµ–ï¼šåªéœ€è¦ ultralytics + transformers")

    # æµ‹è¯•YOLOç‰¹å¾æå–
    print("\n1. æµ‹è¯•YOLOç‰¹å¾æå–...")
    yolo = LightweightYOLO(offline_mode=True)
    features = await yolo.extract_simple_features("/fake/test.mp4")
    print(f"   æå–ç‰¹å¾: {len(features['objects_timeline'])} ä¸ªæ—¶é—´ç‚¹")
    print(f"   æè¿°æ–‡æœ¬: {features['qwen_description'][:60]}...")

    # æµ‹è¯•Qwenç†è§£
    print("\n2. æµ‹è¯•Qwenç†è§£...")
    qwen = LightweightQwen(offline_mode=True)
    understanding = await qwen.understand_video(features, "general")
    print(f"   ç†è§£ç»“æœ: {understanding.get('video_type', 'unknown')}")

    # æµ‹è¯•å®Œæ•´ç³»ç»Ÿ
    print("\n3. æµ‹è¯•å®Œæ•´è½»é‡çº§ç³»ç»Ÿ...")
    system = LightweightVideoUnderstanding(offline_mode=True)
    result = await system.analyze_video("/fake/test.mp4", "detailed")
    print(f"   åˆ†æå®Œæˆ: {result['processing_methods']}")
    print(f"   æ ‡å‡†è¾“å‡º: {result['standardized_output']['video_type']}")

    print("\nğŸ‰ è½»é‡çº§ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
    print("âœ¨ ç›¸æ¯”å®Œæ•´ç‰ˆæœ¬å‡å°‘çš„ä¾èµ–ï¼š")
    print("  â€¢ æ— éœ€CLIP (å›¾åƒé£æ ¼åˆ†æ)")
    print("  â€¢ æ— éœ€BLIP (å›¾åƒå†…å®¹ç†è§£)")
    print("  â€¢ æ— éœ€Whisper (è¯­éŸ³è¯†åˆ«)")
    print("  â€¢ æ— éœ€å¤æ‚VLæ¨¡å‹é›†æˆ")
    print("\nğŸ”§ ä¿ç•™çš„æ ¸å¿ƒèƒ½åŠ›ï¼š")
    print("  â€¢ YOLOå¯¹è±¡æ£€æµ‹å’Œæ—¶é—´çº¿åˆ†æ")
    print("  â€¢ Qwenæ–‡æœ¬ç†è§£å’Œå†…å®¹åˆ†æ")
    print("  â€¢ æ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼")
    return True


if __name__ == "__main__":
    asyncio.run(test_lightweight_system())